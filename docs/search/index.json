[{"content":"前言 人眼可见色域在色度图中表现为彩色的马蹄形，单色光（monochromatic light）的颜色对应于马蹄的弧形边界。本文想将单色光的颜色按波长线性增大的顺序一字排开，用类似彩虹渐变图的形式展示单色光光谱。用 Python 的 Matplotlib 包来实现的话，很快就能决定画图思路：\n 读取 XYZ 颜色匹配函数（CMF）作为 XYZ 三刺激值。 XYZ 变换为 sRGB，接着做 gamma 校正。 用 RGB 数组构造 ListedColormap 对象，用 plt.colorbar 画出。  RGB 要求范围在 $[0, 1]$，但 CMF 直接计算出的 RGB 既有负数分量，也有大于 1 的分量，所以必须采用一种方法处理范围外的分量。最后的画图效果会因处理方法的不同产生很大差别，例如下图的三条光谱：\n就采取了不同的处理方式，因此在发色、颜色过渡，和亮度表现上都大有不同。本文将尝试实现不同的效果并加以分析。完整代码和相关数据见 我的 Github 仓库。\n理论知识 本节将依次介绍 CIE RGB、XYZ 和 sRGB，以及画图时会用到的一些结论。\nCIE RGB CIE RGB 基于 700 nm 的红光、546.1 nm 的绿光，和 435.8 nm 的蓝光，CMF 指 $\\bar{r}(\\lambda)$、$\\bar{g}(\\lambda)$ 和 $\\bar{b}(\\lambda)$ 三条函数曲线，满足方程\n$$ V(\\lambda) = L_R \\bar{r}(\\lambda) + L_G \\bar{g}(\\lambda) + L_B \\bar{b}(\\lambda) $$\n其中 $V(\\lambda)$ 是光效函数（luminous efficiency function），表示相对于 555 nm 单色绿光，人眼对于波长为 $\\lambda$ 的单色光的敏感度；常数 $L_R = 1$，$L_G = 4.5907$，$L_B = 0.0601$。该方程的物理意义是，颜色匹配实验中为了匹配单位辐亮度（radiance）的单色光 $\\lambda$，需要辐亮度为 $L_R \\bar{r}(\\lambda) / V(\\lambda)$ 的红光、$L_G \\bar{g}(\\lambda) / V(\\lambda)$ 的绿光，和 $L_B \\bar{b}(\\lambda) / V(\\lambda)$ 的蓝光。\n对功率谱（power spectrum）为 $P(\\lambda)$ 的任意光，定义其三刺激值（tristimulus）为\n$$ \\begin{gather*} R = \\int \\bar{r}(\\lambda) P(\\lambda) d\\lambda \\cr G = \\int \\bar{g}(\\lambda) P(\\lambda) d\\lambda \\cr B = \\int \\bar{b}(\\lambda) P(\\lambda) d\\lambda \\end{gather*} $$\n该光束的颜色就由向量 $(R, G, B)$ 描述。因为光束的辐亮度和光源的功率成正比，所以三刺激值可以理解为，匹配目标光所需基色光的数量。接着介绍三条重要的性质：\n CMF 可以视作功率为 1 W 的单色光的三刺激值。 三刺激值之间的比例决定颜色的色度（chromaticity）。 $L_R R + L_G G + L_B B$ 线性正比于颜色的辐亮度和视亮度（luminance）。  由性质 2 和 3 可以推论，$(kR, kG, kB)$ 意味着维持色度不变，亮度变为 $k$ 倍。\nCMF 在有些波段存在负值，例如 440 到 550 nm 间的 $\\bar{r}(\\lambda)$，说明有些单色光无法用 CIE RGB 的三基色光混合出来，但如果先在目标光上面叠加红光，那么就能用绿光和蓝光混合出目标光，这就相当于是混合了负量的红光。同理，有些非单色光会计算出负的三刺激值。这两个事实意味着现实世界有很多颜色无法直接通过混合三基色得到。\nCIE XYZ 国际照明委员会（CIE）挑选了三个不存在的假想色（imaginay colors）作为色彩空间的新基向量，对 CIE RGB 空间做线性变换得到了 CIE XYZ 空间，XYZ 空间的 CMF 是 $\\bar{x}(\\lambda)$、$\\bar{y}(\\lambda)$ 和 $\\bar{z}(\\lambda)$。同样定义三刺激值\n$$ \\begin{gather*} X = \\int \\bar{x}(\\lambda) P(\\lambda) d\\lambda \\cr Y = \\int \\bar{y}(\\lambda) P(\\lambda) d\\lambda \\cr Z = \\int \\bar{z}(\\lambda) P(\\lambda) d\\lambda \\end{gather*} $$\nXYZ 空间的主要性质是：\n CMF 全为正值，人眼可见颜色的三刺激值都是正数。 三刺激值之间的比例决定颜色的色度。 $Y = L_R R + L_G G + L_B B$  第三条意味着 $Y$ 能直接指示亮度，但若想维持色度不变修改亮度，还是需要同时缩放三刺激值。\nXYZ 空间主要用于颜色的理论表示，以及作为色彩空间变换的中间量。我们平时用到的和能下载到的都是 XYZ CMF。\nsRGB 显示器显示颜色要用到 sRGB，以 CRT 显示器的红绿蓝磷光体（phosphor）为色彩空间的新基向量，对 CIE XYZ 空间做线性变换得到了 sRGB 空间。类似 CIE RGB 空间，sRGB 也用 RGB 值描述颜色。最终显示前还需要做 gamma 校正，详细计算公式可见 搞颜色系列：绘制 CIE 1931 色度图。接下来的讨论里提到 RGB 的地方都是指 sRGB，并且会忽略 gamma 校正环节。\nsRGB 空间的主要性质是：\n 单色光的 RGB 都存在负数分量。 RGB 值之间的比例决定颜色的色度。 $Y = 0.2126 R + 0.7152 G + 0.072 B$ 显示时要求 $R, G, B \\in [0, 1]$。  性质 1 是因为色度图上单色光对应的马蹄形边界全在 sRGB 的三角形色域外，sRGB 的三基色光不能直接混合得到单色光。性质 2 和 3 直接源于 CIE RGB，同样能推论 $(kR, kG, kB)$ 表示亮度变为 $k$ 倍。性质 4 需要详细解释一下：显示器的像素不能产生负量的基色光，所以不允许分量小于 0；显示器的亮度可以通过面板上的按钮从最低档调到最高档，$(R, G, B)$ 表示的颜色在不同的亮度档位下呈现不同的亮度。所以把 RGB 看作绝对量是没有意义的，将其视为 $[0, 1]$ 之间的相对量更加便利。\n实际计算时如果碰到 RGB 分量是负数的情况，可以直接将负值修改为 0；碰到大于 1 的情况，考虑到相对量的概念，可以根据需求对 RGB 整体做缩放，只要最大分量小于 1 就行。\n本节关于亮度的性质主要基于个人理解，在翻阅相关教材时没看到有明确这么表述的，如果有误还请读者指出。但不管对不对，后面的画图环节马上就会用到。\n画图 单色光的 RGB 需要用到的数据是伦敦大学学院 CVRL 实验室官网 提供的 CIE 1931 XYZ CMF，范围从 360 到 830 nm，分辨率为 1nm。直接用 Pandas 读取：\nimport pandas as pd cmf = pd.read_csv('./data/cie_1931_2deg_xyz_cmf.csv', index_col=0)  CMF 可以看作功率为 1 W 的单色光的 XYZ 值，根据 XYZ 到 sRGB 的变换公式\n$$ \\begin{bmatrix} R \\cr G \\cr B \\end{bmatrix} = \\begin{bmatrix} +3.2406 \u0026amp; -1.5372 \u0026amp; -0.4986 \\cr -0.9689 \u0026amp; +1.8758 \u0026amp; +0.0415 \\cr +0.0557 \u0026amp; -0.2040 \u0026amp; +1.0570 \\end{bmatrix} \\begin{bmatrix} X \\cr Y \\cr Z \\end{bmatrix} $$\nimport numpy as np XYZ = cmf.to_numpy() Y = XYZ[:, 1] M = np.array([ [+3.2406, -1.5372, -0.4986], [-0.9689, +1.8758, +0.0415], [+0.0557, -0.2040, +1.0570] ]) RGB = np.tensordot(XYZ, M, (-1, 1))  CMF 的 RGB 和 $Y$ 曲线如下图所示：\n如上一节所述，RGB 全波段都存在负数分量，有些波段的分量大于 1。$Y$ 曲线实际上就是 $V(\\lambda)$，最高点是 555 nm 绿光处，亮度向短波和长波端递减至 0。画光谱时必须将分量处理到 $[0, 1]$ 范围内，下面测试不同处理方法的效果。\n方法一：clip 最朴素的做法是用 np.clip 函数将负数修改为 0，大于 1 的数修改为 1：\nRGB = RGB.clip(0, 1)  光谱颜色用 plt.colorbar 画出：\nimport matplotlib.pyplot as plt from matplotlib.colors import Normalize, ListedColormap from matplotlib.cm import ScalarMappable from matplotlib.ticker import MultipleLocator fig, ax = plt.subplots(figsize=(8, 1)) mappable = ScalarMappable( norm=Normalize(*cmf.index[[0, -1]]), cmap=ListedColormap(gamma_encoding(RGB)) ) cbar = fig.colorbar(mappable, cax=ax, orientation='horizontal') cbar.set_ticks(MultipleLocator(50))  为了便于分析，额外画出处理后的 RGB 和反算出来的 $Y$，以及色度图上处理前的单色光颜色，处理后会被映射到什么颜色上：\n先看左下角的光谱：\n 410 - 455 nm 呈红紫色（purple），比右边的蓝色还亮。 蓝色、绿色和红色区域内缺乏过渡，以 510 - 560 nm 为例，看起来像是同一种绿色绿了一片。  这些问题都可以用左上的曲线图解释：\n 410 - 455 nm 波段的紫色本来应该由蓝色、红色，和负量的绿色混合得到，现在 $G = 0$，所以呈红紫色；由 $Y = 0.2126 R + 0.7152 G + 0.072 B$，$G = 0$ 相当于增大 $Y$，所以 $Y$ 曲线在这一段凸起，比右边的蓝色更亮。 510 - 560 nm 波段负数 $R$ 和 $B$ 变成 0，本来大于 1 且有变化的 $G$ 全变成 1，所以这一段全是 $(0, 1, 0)$ 的绿色。同理 600 - 650 nm 全是 $(1, 0, 0)$ 的红色。  右边的映射图也能给出形象的解释：边界上 505 - 550 nm 的颜色全被映射到了 sRGB 三角的 $G$ 顶点上，同理 610 - 800 nm 的颜色全被映射到了 $R$ 顶点上。\n方法二：压缩高度 第二个方法是先用 clip 去除负数分量，再同时压缩三条 RGB 曲线直到最高点恰好为 1：\nRGB = RGB.clip(0, None) RGB /= RGB.max()  其中 RGB.max() 对应于 $R(605 ; \\rm{nm}) = 2.517$。效果如下图：\n相比方法一：\n 因为 RGB 整体除以 2.517，所以 $Y$ 曲线的高度下降，导致光谱亮度仅有方法一的一半，黄色因为太暗显得发棕。 410 - 455 nm 的紫色亮度依旧比周围高，但没有方法一那么明显了。 蓝色、绿色和红色部分现在有了平滑的过渡。 色度图上短波和长波端的颜色映射相比方法一稍有区别。  方法三：调整亮度 紫色偏亮的问题可以通过调整亮度解决：\n 设 CMF 的亮度为 $Y_1$。 CMF 变换为 RGB 后用 clip 去除负数分量，再变换回 XYZ 值，得到亮度 $Y_2$。 RGB 乘以 $Y_1 / Y_2$。 RGB 曲线压缩至最高高度为 1。  最后得到的 $Y$ 曲线的形状和 $V(\\lambda)$ 相同，但高度有压缩。代码为：\nY1 = XYZ[:, 1] RGB = RGB.clip(0, None) Y2 = sRGB_to_XYZ(RGB)[:, 1] RGB *= Y1 / Y2 RGB /= RGB.max()  观感和方法二非常接近，但紫色不再偏亮。\n方法四：沿连线朝白色移动 色度图上让单色光的颜色沿直线向 sRGB 的白点移动，RGB 的负数分量会逐渐增大，到达 sRGB 的三角形色域边界时恰好为 0，取交点处的颜色作为单色光颜色的近似。相比于 clip 方法，该方法的色相（hue）与原单色光更接近，但饱和度（saturation）会更低。搞颜色系列：绘制 CIE 1931 色度图 中已经论述过，如果一个颜色的最小分量为负数，那么让每个分量都减去这个负数即可。明确一下方法四的流程：\n CMF 变换得到 RGB。 每个颜色的 RGB 减去最小的负数分量。 RGB 乘以 $Y_1 / Y_2$ 调整亮度。 RGB 曲线压缩至最高高度为 1。  Y1 = XYZ[:, 1] RGB -= RGB.min(axis=1, keepdims=True).clip(None, 0) Y2 = sRGB_to_XYZ(RGB)[:, 1] RGB *= Y1 / Y2 RGB /= RGB.max()  前几种方法里 510 - 540 nm 的绿色都映射到色度图上的 $G$ 顶点附近，而方法四里这一波段的绿色都映射到了 $GB$ 直线上，表现出蓝绿混合的青色（cyan），只不过因为饱和度低显得不是很纯净。另外曲线图里 $B$ 变成了很搞笑的形状。\n颜色增亮 方法三和四都比直接 clip 的方法一看起来更自然，无奈因为 RGB /= RGB.max() 操作亮度减半，看起来像是蒙了一层灰脏兮兮的。所以最后决定整体放大 RGB 来增亮，这里以方法四为例，参考 Rendering Spectra 选择 1.8 的倍数：\nRGB *= 1.8 RGB = RGB.clip(0, 1)  这下看起来靓丽多了。但是 $R$ 和 $G$ 超过 1 的部分需要做 clip，所以 610 - 630 nm 的红色区域又有点红成一片的效果，不过比方法一还是轻微许多，可以接受。\n当然除了整体增亮以外，还有一种简单粗暴的方式，那就是调高屏幕亮度……\n结语 本文开头的三条光谱，分别对应于方法一（clip）、方法三（调整亮度）和方法四（沿连线朝白色移动，再增亮 1.8 倍）。\nPython Colour 包 的 plot_visible_spectrum 函数能直接画出单色光光谱，默认效果非常接近本文的方法二，整体略暗，紫色发亮。所以本文有助于解释为什么调包画出来是那样一种效果，以及如何自己实现其它效果。另外网上直接搜索 \u0026ldquo;visible light spectrum\u0026rdquo; 的图片，会发现大部分图片里光谱里蓝绿之间的青色非常明显，蓝色段也很宽。我现在还没想到这个效果是怎么做到的，如果有读者了解还请指教。\n参考资料 Color Vision and Colorimetry: Theory and Applications, Second Edition\nConvert light frequency to RGB?\nColour Rendering of Spectra\nRendering Spectra\n光谱渲染的几个例子\n","date":"2023-09-14","permalink":"https://zhajiman.github.io/post/monochromatic_light/","tags":["色彩","matplotlib"],"title":"搞颜色系列：单色光光谱"},{"content":"前言 1920 年代末 Wright 和 Guild 的颜色匹配实验发展出了用红绿蓝三基色（primaries）定量表示所有人眼可见颜色的 CIE RGB 色彩空间，1931 年国际照明委员会（CIE）通过对 CIE RGB 色彩空间做线性变换得到了 CIE XYZ 色彩空间。XYZ 空间里的人眼可见色域（gamut of human vision）是一块从原点出发，向无限远处不断延伸的立体区域。将这块区域投影到 $X + Y + Z = 1$ 的平面上，就能画出方便展示的 CIE 1931 色度图（chromaticity diagram）（图自 维基）：\n图中彩色马蹄形（horseshoe）的边界对应于单色光的颜色，并标出了波长的刻度；马蹄形内部则是单色光混合产生的颜色。解释一下色度图的横纵坐标：XYZ 空间可以理解为选取了三个假想色（imaginary colors）作为色彩空间里的基向量，使人眼可见色域恰好落入 XYZ 空间的第一卦限。混合颜色 $\\boldsymbol{C}$ 需要 $(X, Y, Z)$ 份的假想色，将这些份数用总和归一化为比值，前两个比值就是色度图的坐标。\n这张图在色彩科学教程中经常出现，不过日常里见得最多的场合估计还是电脑显示器的广告：显示器的每个像素由 RGB 子像素组成，依据三基色理论可以混合出任意颜色。但由于现实世界的功率不能为负数，所以三基色只能在色度图上圈出一个三角形的区域，对应于显示器所能产生的所有色度的颜色。广告里通常会给色度图叠上 sRGB 和 NTSC 的色域三角形，然后强调显示器能做到 100% 覆盖 sRGB 色域。\n我想动手画一张色度图试试，通过实践加深对色彩的理解，本文的目的便是总结相关经验。另外我在网上搜索时发现除了 一篇用 Qt C++ 的博文，其它教程都是直接调用 python 的 Colour 包，或 Matlab 和 Mathematica 的内置函数来画的。所以本文也想填补 Python 从零实现的空白。\n本文用到的 Python 包是 NumPy、Pandas 和 Matplotlib。完整代码和用到的数据可见 我的 Github 仓库。\n画图思路  在 xy 平面上用 np.linspace 和 np.meshgrid 构造网格。 计算每个网格点的坐标 $(x, y)$ 对应的 sRGB。 用 imshow 将网格当作彩色图片画出来。 读取 XYZ 颜色匹配函数的色度坐标，构造马蹄形的 Polygon 去裁剪 imshow 的结果。 在马蹄图边缘添加波长刻度。  思路不复杂，但坑却比想象中多，后面将会一一道来。\n准备数据 在伦敦大学学院的 CVRL 实验室官网 下载：\n CIE 1931 2-deg, XYZ CMFs CIE 1931 2-deg xyz chromaticity coordinates  第一样是 CIE 1931 XYZ 颜色匹配函数（Color Matching Function, CMF），第二样是 CMF 的色度坐标，范围从 360 到 830 nm，分辨率为 1nm。\n构造网格 人眼可见色域落在 xy 平面上的 $x,y \\in [0, 1]$ 范围内，所以网格只需要在 $[0, 1]$ 之间取：\nimport numpy as np N = 256 x = np.linspace(0, 1, N) y = np.linspace(0, 1, N).clip(1e-3, 1) x, y = np.meshgrid(x, y)  得到形如 (N, N) 的 xy 网格。如果 y 里含有 0，那么后续计算 Y / y 时将会出现除零警告，所以这里设置 y 的下限为 1e-3。\n计算 sRGB xyz 转换 XYZ $(x, y)$ 坐标只表示颜色的色度，还要补上亮度（luminance） $Y$，将其转换为三刺激值（tristimulus） $(X, Y, Z)$ 后才能进一步变换为 RGB。\n色度坐标的定义为\n$$ \\begin{align*} x \u0026amp;= \\frac{X}{X + Y + Z} \\cr y \u0026amp;= \\frac{Y}{X + Y + Z} \\cr z \u0026amp;= \\frac{Z}{X + Y + Z} \\end{align*} $$\n假设已知亮度 $Y$，那么由定义可以推出 $X$ 和 $Z$\n$$ \\begin{align*} X \u0026amp;= x \\frac{Y}{y} \\cr Z \u0026amp;= (1 - x - y) \\frac{Y}{y} \\end{align*} $$\n色度图里颜色的亮度可以随意指定，这里不妨设 $Y = 1$。代码如下：\nY = np.ones_like(y) Y_y = Y / y X = x * Y_y Z = (1 - x - y) * Y_y XYZ = np.dstack((X, Y, Z))  XYZ 转 sRGB sRGB 是微软和惠普于 1996 年联合开发的用于显示器、打印机和互联网的色彩空间标准，s 意指 standard。PC 和互联网默认以 sRGB 标准解读图片存储的 RGB 数组，Matplotlib 也不例外。所以为了用 Matplotlib 绘制色度图，需要将 XYZ 坐标变换为 sRGB 坐标。sRGB 空间由 XYZ 空间线性变换而来，首先是在 XYZ 空间里选取新的红绿蓝三基色和白点\n$$ \\begin{align*} (x_r, y_r, z_r) \u0026amp;= (0.64, 0.33, 0.03) \\cr (x_g, y_g, z_g) \u0026amp;= (0.30, 0.60, 0.10) \\cr (x_b, y_b, z_b) \u0026amp;= (0.15, 0.06, 0.79) \\cr (X_w, Y_w, Z_w) \u0026amp;= (0.95046, 1.0, 1.08906) \\end{align*} $$\n已知三基色的色度坐标，相当于知道了基向量的方向；已知白点的三刺激值，就可以确定基向量的长度，让 D65 白点在 RGB 空间里对应于 $(1, 1, 1)$。由此可以求得 XYZ 坐标到 sRGB 坐标的线性变换矩阵。具体求解过程可见 Computing RGB-XYZ conversion matrix，这里直接引用 维基 的结果\n$$ \\begin{bmatrix} R \\cr G \\cr B \\end{bmatrix} = \\begin{bmatrix} +3.2406 \u0026amp; -1.5372 \u0026amp; -0.4986 \\cr -0.9689 \u0026amp; +1.8758 \u0026amp; +0.0415 \\cr +0.0557 \u0026amp; -0.2040 \u0026amp; +1.0570 \\end{bmatrix} \\begin{bmatrix} X \\cr Y \\cr Z \\end{bmatrix} $$\n将变换矩阵记作 $\\boldsymbol{M}$。前面得到的 XYZ 数组形如 (N, N, 3)，M 形如 (3, 3)，我们希望得到形如 (N, N, 3) 的 RGB 数组。按 NumPy 的矩阵乘法，我首先写出了：\nRGB = M @ XYZ  这里 @ 运算符等价于 np.matmul。然而 NumPy 对多维数组矩阵乘法的处理是，让它最后两维都参与计算，所以会给出错误的结果。其实这种情况下更合适的算法是张量缩并（tensor contraction），设 XYZ 的元素为 $X_{ijk}$，RGB 的元素为 $C_{ijk}$，M 的元素为 $M_{ij}$，缩并公式为\n$$ C_{ijk} = \\sum_{l=1}^{3} X_{ijl} M_{kl} $$\n这样计算出的 RGB 就是正确的。代码为：\nM = np.array([ [+3.2406, -1.5372, -0.4986], [-0.9689, +1.8758, +0.0415], [+0.0557, -0.2040, +1.0570] ]) RGB = np.tensordot(XYZ, M, (-1, 1))  别问什么是张量，问就是我也不会，这个矩阵乘法技巧是 stack overflow 上抄的。\nsRGB 的定义要求 RGB 值在 $[0, 1]$ 范围内，即 RGB 空间里第一卦限内单位立方体的空间，超出范围的值我们可以用 np.clip 直接削头去尾：\nRGB = RGB.clip(0, 1)  Gamma 校正 用 $\\boldsymbol{M}$ 乘出来的 sRGB 还只是线性 sRGB，需要经过 gamma 校正后才能得到最后的结果。CRT 显示器的显示强度跟 RGB 像素值之间呈非线性关系\n$$ I = A D^{\\gamma} $$\n其中 $I$ 是显示亮度，$A$ 是最大亮度，$D$ 是 $[0, 1]$ 之间的 RGB 像素值，一般有 $\\gamma = 2.2$。下面画出函数 $y = x^{2.2}$ 的曲线：\n本来像素值跟颜色的亮度成线性关系，低像素值对应低亮度，但经过 CRT 显示器的非线性映射后，现在低像素值对应的亮度更低，并且在很大一段范围内都只能输出低亮度，只有当像素值接近于 1 时亮度才会陡然提升，这就会使图像显得更暗。为了修正这一问题，可以提前让 RGB 变为原来的 1/2.2 次方，这样经过 CRT 显示器的映射后就能变回原来的 RGB，显示正确的色彩。后来淘汰了 CRT 的液晶显示器并没有 $y = x^{2.2}$ 的非线性特征，但仍会通过电路模拟出这个效果，以正常显示 sRGB 标准的图像。上图的绿线画出了 $y = x^{1/2.2}$ 的曲线（几乎被蓝线挡住了），可以看到形状往上鼓，正好能抵消 $y = x^{2.2}$ 的下凹形状。\n对线性的 sRGB 做 $y = x^{1/2.2}$ 映射的操作叫做 gamma 编码（encoding），反过来映射的操作就叫做 gamma 解码（decoding），gamma 校正就是指编码解码的这一过程。网上有文章认为 gamma 校正的深层原因是为了契合人眼对亮度的非线性感知，或者是先做 gamma 编码再做 8 bit 量化能保留更多暗部信息。相关讨论详见 色彩校正中的 gamma 值是什么？，这里不再深究。\nsRGB 标准考虑到低亮度时的显示和量化误差等因素，设计出了跟 $y = x^{1/2.2}$ 非常接近，但存在细微区别的 gamma 编码方案\n$$ C' = \\begin{cases} 12.92 C \\quad \u0026amp; C \\le 0.0031308 \\cr 1.055 C^{1/2.4} - 0.055 \\quad \u0026amp; C \\gt 0.0031308 \\end{cases} $$\n其中 $C$ 指代 $R$、$G$ 或 $B$。这一映射的曲线在上面的图中已用蓝线画出，与 $y = x^{1/2.2}$ 的曲线几乎重合。注意 $C \\in [0, 1]$ 经过映射后仍有 $C' \\in [0, 1]$。\n经过 gamma 编码后的 RGB 值才是最终的 sRGB，代码如下：\n# 使用np.where会遇到非正数求幂的问题. mask = RGB \u0026gt; 0.0031308 RGB[~mask] *= 12.92 RGB[mask] = 1.055 * RGB[mask]**(1 / 2.4) - 0.055  色度图填色 绘制马蹄形轨迹 我将 CVRL 上下载的色度坐标数据更名为 cie_1931_2deg_xyz_cc.csv，用 Pandas 读取后构造 Matplotlib 的 Polygon 对象再添加到 Axes 上：\nimport pandas as pd import matplotlib.pyplot as plt from matplotlib.patches import Polygon fig, ax = plt.subplots() ax.set_aspect('equal') ax.set_xlim(0, 0.8) ax.set_ylim(0, 0.9) cc = pd.read_csv('./data/cie_1931_2deg_xyz_cc.csv', index_col=0) patch = Polygon( xy=cc[['x', 'y']], transform=ax.transData, ec='k', fc='none', lw=1 ) ax.add_patch(patch)  imshow 填色 imshow 能把形如 (M, N, 3) 的 RGB 数组当作彩色图片添加到 Axes 的指定位置上，imshow 的 clip_path 参数还能用 Polygon 的轮廓裁剪彩色图片，只保留落入 Polygon 内部的填色：\nax.imshow( RGB, origin='lower', extent=[0, 1, 0, 1], interpolation='bilinear', clip_path=patch )  origin 和 extent 参数的用法请见 origin and extent in imshow。\n亮度设置 前面的代码已经够画一张有模有样的色度图了，不过最大的坑也随之而来。下面展示 $Y = 1$ 和 $Y = 0.3$ 的结果：\n跟维基的效果很接近了，但却存在很大的违和感。首先 $Y = 1$ 时马蹄中心区域过白，令 $k = Y/y$，网格点的三刺激值为 $(kx, Y, k(1 - x - y))$，那么 $k$ 相当于 $x$ 和 $z$ 的放大因子。$Y = 1$ 时 $k$ 偏大，使马蹄中心的 RGB 值接近甚至超过 $(1, 1, 1)$，导致白了一片。\n那么调低亮度使 $Y = 0.3$，马蹄中间不发白了，但和 $Y = 1$ 时一样，在紫红线上方仍有一大条紫色。考虑 XYZ 和 sRGB 的换算关系\n$$ \\begin{bmatrix} R \\cr G \\cr B \\end{bmatrix} = \\begin{bmatrix} +3.2406 \u0026amp; -1.5372 \u0026amp; -0.4986 \\cr -0.9689 \u0026amp; +1.8758 \u0026amp; +0.0415 \\cr +0.0557 \u0026amp; -0.2040 \u0026amp; +1.0570 \\end{bmatrix} \\begin{bmatrix} X \\cr Y \\cr Z \\end{bmatrix} $$\n紫红线附近 $k$ 大 $x$ 大，量级上有 $X \u0026gt; Z \u0026gt; Y$。观察变换矩阵 $\\boldsymbol{M}$ 每一行的权重，可见第一行由正的 $X$ 主导，第二行由负的 $X$ 主导，第三行由正的 $Z$ 主导。因此紫红线附近的网格点 RGB 值大致满足 $R, B \u0026gt; 1$，$G \u0026lt; 0$，clip 后得 $(1, 0, 1)$，正好对应紫色。另外 clip 操作会通过削去高值降低 RGB 的量级，从而减小色度图部分区域的最终亮度。\n进一步减小 $Y$ 能够隐去右下角的紫条，但也会使上方的颜色过于黯淡。因此考虑另外两种亮度设置：\n $Y$ 随 $y$ 从 0 到 1 线性增大。 任取正数 $Y$，先 clip 掉线性 sRGB 的负数分量，再除以 $C_{max} = \\max \\lbrace R, G, B \\rbrace$，即用最大分量做归一化。  两种设置的效果如下图：\n设置 1 的效果很不错，颜色过渡自然，红绿蓝紫青粉都有了；设置 2 的效果比 1 更明亮，但中间浮现出“Y”形亮纹。设置 2 里亮度任取，算出来的 RGB 无论是高是低，都会因为除以 $C_{max}$，使分量最高只有 1，这样一来就能在不需要用 clip 削头的情况下尽量提高每个网格点的亮度。至于亮纹，很容易推导出归一化后的亮度 $Y' = Y/C_m$，直觉告诉我 $R = G$、$R = B$、和 $G = B$ 时 $C_{max}$ 能取较小的值，将对应的三条直线投影到 $X + Y + Z = 1$ 平面上算出的直线确实跟“Y”形亮纹的位置重合，但你要问怎么证明我只能说不会。\n设置 1 的代码为：\nY = np.linspace(0, 1, y.shape[0]) Y = np.broadcast_to(Y[:, np.newaxis], y.shape) \u0026lt;计算sRGB和clip的代码\u0026gt; \u0026lt;gamma encoding的代码\u0026gt;  设置 2 的代码为：\nY = np.ones_like(x) \u0026lt;xyz转换XYZ的代码\u0026gt; RGB = XYZ_to_sRGB(XYZ) RGB = RGB.clip(0, None) RGB /= RGB.max(axis=2, keepdims=True) \u0026lt;gamma encoding的代码\u0026gt;  设置 2 的亮度表现跟维基图很接近了，但左上角部分发绿，而维基图则发青（cyan）。\n色域出界处理 容易证明 XYZ 空间里两个颜色线性混合而成的颜色在色度图上正好落在两点间的连线上，所以 sRGB 三基色混合而成的颜色在色度图上对应于三基色围成的三角形区域，此即 sRGB 的色域。如果硬要用三基色混合出色域外的颜色，就会出现组合系数有负值的情况，RGB 表现为存在负数分量。前面我们的处理是简单粗暴地将负数分量修改为零（clip），这里介绍另一种处理方式：色度图上让色域外的颜色沿直线向白点移动，直至遇到三角形边界，用边界上的交点近似表示界外色。优点是能尽量维持界外色的色相（hue）。 设 $\\boldsymbol{C_1}$ 为色域外的颜色，$\\boldsymbol{C_2}$ 为 D65 白点\n$$ \\begin{align*} \\boldsymbol{C_1} \u0026amp;= (R, G, B) \\cr \\boldsymbol{C_2} \u0026amp;= (1, 1, 1) \\end{align*} $$\n色域外要求 RGB 最小分量为负数\n$$ C_{min} = \\min \\lbrace R, G, B \\rbrace \u0026lt; 0 $$\n色度图上让 $\\boldsymbol{C_1}$ 向 $\\boldsymbol{C_2}$ 移动相当于线性混合这两个颜色。不妨令混合色 $\\boldsymbol{C_3} = \\boldsymbol{C_1} + k\\boldsymbol{C_2}$，比例系数 $k$ 使 $\\boldsymbol{C_3}$ 的最小分量恰好为零，即 $\\boldsymbol{C_3}$ 恰好落在三角形边界上。显然 $k = -C_{min}$，于是\n$$ \\begin{align*} \\boldsymbol{C_3} \u0026amp;= \\boldsymbol{C_1} + k\\boldsymbol{C_2} \\cr \u0026amp;= \\boldsymbol{C_1} - C_{min} \\boldsymbol{C_2} \\cr \u0026amp;= (R - C_{min}, G - C_{min}, B - C_{min}) \\cr \\end{align*} $$\n$\\boldsymbol{C_3}$ 的最小分量为 $C_{min} - C_{min} = 0$。一句话概括该方法：如果一个颜色的最小分量为负数，那么让每个分量都减去这个负数。如此处理后分量可能超过 1，可以再应用上一节的设置 2，除以新的 $C_{max}$ 做归一化。跟 clip 处理的对比如下图：\n区别是左半区域显得更青，色彩跟维基的图几乎一致了。所以本文最后决定通过混合白色处理出界色，用最大分量做归一化设置亮度。如果读者不喜欢这种发青的效果，可以自行尝试不同的设置。\n代码如下：\nY = np.ones_like(x) \u0026lt;xyz转换XYZ的代码\u0026gt; RGB = XYZ_to_sRGB(XYZ) RGB -= RGB.min(axis=2, keepdims=True).clip(None, 0) RGB /= RGB.max(axis=2, keepdims=True) \u0026lt;gamma encoding的代码\u0026gt;  波长刻度 马蹄形边界一圈（不含紫红线）对应于单色光的颜色，维基的图中给边界标注了对应波长的刻度，可以看到从紫色到红色，波长从 460 nm 升至 620 nm。Matplotlib 只支持给 Axes 的四周标刻度，像这种曲线上的刻度我们只能自己用 plot 方法来画。以边界曲线上一点 $(x_i, y_i)$ 为例，刻度长度自定，如果已知该点的法向方向，就能根据刻度长度 $L$ 和方向确定刻度终点 $(x'_i, y'_i)$。求离散曲线的法向方向，网上有说直接用差分近似导数的，有说用三点共圆的，还有三次样条插值后再求导的。我参考的是论文 色度図の着色 里的做法，核心思路是对于相邻的三点 $P_1$、$P_2$ 和 $P_3$，以连线 $P_1 P_3$ 的垂线方向作为 $P_2$ 点的法向：\n这个算法实现起来比较简单，并且可以向量化。计算公式为\n$$ \\begin{gather*} \\Delta x_i = x_{i + 1} - x_{i - 1} \\cr \\Delta y_i = y_{i + 1} - y_{i - 1} \\cr \\Delta l_i = \\sqrt{\\Delta x_i^2 + \\Delta y_i^2} \\cr \\cos \\theta_i = -\\Delta y_i / \\Delta l_i \\cr \\sin \\theta_i = \\Delta x_i / \\Delta l_i \\cr x'_i = x_i + L \\cos \\theta_i \\cr y'_i = y_i + L \\sin \\theta_i \\end{gather*} $$\n其中 $\\theta_i$ 是法向跟 x 轴的夹角。代码为：\nx, y = cc['x'], cc['y'] xy = np.column_stack((x, y)) dxdy = np.zeros_like(xy) dxdy[0] = xy[1] - xy[0] dxdy[-1] = xy[-1] - xy[-2] dxdy[1:-1] = xy[2:] - xy[:-2] dx, dy = dxdy[:, 0], dxdy[:, 1] dl = np.hypot(dx, dy) dl[dl \u0026lt;= 0] = np.nan cos = -dy / dl sin = dx / dl cs = pd.DataFrame( data=np.column_stack((cos, sin)), index=cc.index, columns=['cos', 'sin'] ) cs.loc[(cs.index \u0026lt; 430) | (cs.index \u0026gt; 660)] = np.nan cs = cs.ffill().bfill() cos, sin = cs['cos'], cs['sin'] tick_len = 0.03 tick_df = pd.DataFrame({ 'x0': x, 'y0': y, 'x1': x + tick_len * cos, 'y1': y + tick_len * sin, }) ticks = [380, *range(460, 601, 10), 620, 700] tick_df = tick_df.loc[ticks] for row in tick_df.itertuples(): ax.plot( [row.x0, row.x1], [row.y0, row.y1], c='k', lw=0.6 )  中间用到了 DataFrame 能用标签进行索引的特性。这里还有一个隐藏的坑：波长很小时 CMF 的色度坐标不是很精准，放大后会看到曲线歪歪扭扭，因此算出的法向一会儿朝左一会儿朝下，并不稳定；波长很大时 $\\Delta x_i$ 和 $\\Delta y_i$ 全为零，无法计算夹角。这里将波长小于 430 nm 部分的 $\\Delta x_i$ 和 $\\Delta y_i$ 都修改成 430 nm 处的值，波长大于 660 nm 的部分同理。具体到代码是通过 Pandas 的 ffill 和 bfill 实现的。\n给刻度加标签的操作类似，无非是将刻度长度拉长，计算出距离更远的 $(x'_i, y'_i)$，然后用 text 方法加字。\n最终效果 还额外画上了表示 sRGB 色域的三角形和 D65 白点。\n最后再顺手实现一下基于 CIE RGB 空间的 rg 色度图。同样是在 rg 空间里拉出网格，将 rg 按比例放大为 RGB，计算 CIE RGB -\u0026gt; XYZ -\u0026gt; sRGB，其中 CIE RGB 到 XYZ 的变换公式为\n$$ \\begin{bmatrix} X \\cr Y \\cr Z \\end{bmatrix} = \\begin{bmatrix} 2.76888 \u0026amp; 1.75175 \u0026amp; 1.13016 \\cr 1.00000 \u0026amp; 4.59070 \u0026amp; 0.06010 \\cr 0.00000 \u0026amp; 0.05651 \u0026amp; 5.59427 \\end{bmatrix} \\begin{bmatrix} R \\cr G \\cr B \\end{bmatrix} $$\n这个公式还会用于将 XYZ CMF 变换为 RGB CMF，再归一化为色度坐标，用来画单色光轨迹和裁剪 imshow。最后额外画上 XYZ 空间三基色投影在 rg 平面上的三角形：\n结语 至此已经成功复现了 CIE 1931 色度图，该过程充分考验了我们对 CIE RGB、XYZ 和 sRGB 空间的理解，也引入了一些 NumPy 和 Matplotlib 的使用技巧。不过本文的理解和实现也不一定正确，工作中还是更推荐调用 Colour 包的 plot_chromaticity_diagram_CIE1931 的函数，既方便又可靠。读者有问题还请多多指出。\n参考资料 Color Vision and Colorimetry: Theory and Applications, Second Edition\nWikipedia: CIE 1931 color space\nWikipedia: sRGB\nProposal for a Standard Default Color Space for the Internet—sRGB\nColor space conversion (2) RGB-XYZ conversion\nRendering Spectra\n色度図の着色\n如何绘制CIE1931xy色度图\n","date":"2023-09-03","permalink":"https://zhajiman.github.io/post/chromaticity_diagram/","tags":["色彩","matplotlib"],"title":"搞颜色系列：绘制 CIE 1931 色度图"},{"content":" 罗切斯特大学朱禺皓的 博客文章，基于颜色匹配实验的原始论文跟后人的调查，先从单位系统和色度系数讲起，再引入颜色匹配函数的概念和计算方法，并直接指出颜色匹配函数就是匹配单位功率单色光的亮度时，红绿蓝三基色的亮度经亮度系数缩放后的值。本文讲解的顺序跟一般教科书相反，显得更加自然和易于理解。专业术语的翻译可能有误，还请读者指正。\n 1. 引言 在研究 CIE 1931 RGB 颜色匹配函数（Color Matching Function, CMFs）时，一个常见且合理的疑问是：为什么 CMF 在三基色（primaries）处的值不是 1？例如当 𝛌 = 700 nm 时，即 CIE 1931 RGB 空间中表示红色的基色，R 值为 0.0041（当然 G 和 B 值为 0）。这个 0.0041 表示什么？匹配一个单位的 𝛌 = 700 nm 的红光的颜色正好需要一个单位的 𝛌 = 700 nm 的红光，那对应的 R 值不应该为 1 吗？同样，在 𝛌 = 546.1 nm 和 𝛌 = 435.8 nm 处，即 CIE 1931 标准的绿色基色和蓝色基色，G 和 B 值分别约为 0.215 和 0.2911，显然它们也不为 1。\n流行的图形学教科书里都没有解释这一“矛盾”，通常 CMF 的数值只是被称为“权重（weights）”或“数量（amounts）”。比如在第四版的《Real-Time Rendering》中，CMF 被定义为：\n The functions relating each set of matching weights to the test patch wavelengths\u0026hellip;\n 第四版的《Fundamentals of Computer Graphics》 里说：\n The amount of each (light) required to match a given wavelength λ is encoded in color matching functions…\n 这些解释还是有点模棱两可。如果 𝛌 = 500 nm 时 RGB 值为 (-0.9494, 1.1727, 0.7767)，那么合理的解读是我们需要 -0.9494 个单位的红光，1.1727 个单位的绿光（𝛌 = 546.1 nm），和 0.7767 个单位的蓝光（𝛌 = 435.8 nm），去匹配一个单位的 𝛌 = 500 nm 的单色光的颜色。按这个逻辑，𝛌 = 700 nm 处 CMF 的 R 值就应该是 1。\nCIE 1931 的 维基页面 里有这样的注解（不知道谁写的）：\n Note that rather than specify the brightness of each primary, the curves are normalized to have constant area beneath them.\n 以及：\n The resulting normalized color matching functions are then scaled in the r : g : b ratio of 1 : 4.5907 : 0.0601 for source luminance and 72.0962 : 1.3791 : 1 for source radiance to reproduce the true color matching functions.\n 这些注解非常有用，它们表明 CMF 的数值经历过某种缩放（scaling）和归一化（normalization）操作，因此不能被直接解读。尽管如此，仍存在三个关键问题：\n CMF 的数值到底表示什么？ CMF 为什么要被归一化？ CMF 是如何被归一化的？  要回答这些问题，就必须回到最初的颜色匹配实验（距今快一个世纪了），理解原始实验数据如何获取，后续又是如何处理成今天常用的 1931 RGB CMF 的。\n这件事困扰了我好些日子，因为原始实验数据相关的信息很不好找。后来我偶然发现了 Arthur Broadbent 的两篇论文2 [Broadbent 2004a, Broadbent 2004b]，他详细记录了最初的颜色匹配实验数据在用于构建 CIE 1931 RGB CMF 前所经历的奇妙的变换过程，并在原始数据缺失的地方做了逆向工程。了解这一变换过程不仅能提供历史背景，更重要的是能让我们充分理解 CMF 究竟是什么。\n2. 历史 20 世纪 20 年代末，W. D. Wright 和 J. Guild 各做过一次颜色匹配实验。两次实验都是独立进行的，但结果却惊人的一致。Guild 合并了两个实验的数据，并做了一系列数据增强操作（插值和平滑等），由此产生的色度数据构成了 CIE 1931 RGB 空间和 RGB CMF。\n图 1：对最初的 Wright-Guild 颜色匹配实验数据做变换，得出 CIE 1931 RGB CMF 的步骤。\n图 1 展示了 Wright 和 Guild 的原始测量数据经变换最终用于构造 1931 CIE RGB CMF 的全过程。之后 RGB 空间数据发展出了 XYZ 空间，论文 [Fairman 1997] 漂亮地总结了 RGB 变换到 XYZ 的详细推导过程。另一篇文章 [Service 2016] 则总结了如何从颜色匹配实验得出 RGB CMF，以及 RGB 空间如何变换到 XYZ 空间，虽然这篇没那么详细，但也包含很多值得一读的见解。\n3. 如何量化颜色匹配实验中的光量？ 最基本的问题是，一个单位的光到底有多少：是一瓦特、一流明，还是一定量的光子？我们当然可以随意指定一个单位的红光等于含 1 W 能量的红光，或者一个单位的绿光等于含 2 万亿个光子的绿光，又或者一个单位的蓝光等于 43 流明的蓝光。这些单位都是合理的，但对我们来说缺乏有意义的信息。单位的定义应该符合我们的直觉，满足直觉上的要求。没错，单位这一概念非常主观。我们将通过 Wright 和 Guild 实验中的主要步骤来更好地理解这一点。\n3.1 Wright 的实验 Wright 在 1928 年的论文 [Wright 1928] 里解释了如何确定单位：\n To determine the units of the primaries it is customary to match white by a mixture of the three primaries of the colorimeter in use and to correct the readings as given on the instrument scale by suitable factors, so that for this match all three shall be equal.\n 关键在于 \u0026ldquo;for this match all three shall be equal\u0026rdquo;，即白色应该由等量的三基色产生。这并非客观事实，而只是我们希望拥有的符合直觉的性质：等量的红色、绿色和蓝色产生白色\n然后他举例说明：\n Suppose that the intensities of the primaries in the colorimeter are controlled by variation of sector openings and that these angular apertures for the white match are 30\u0026quot; for red, 15\u0026quot; for green, and 60\u0026quot; for blue. To correct these readings so that they are in equal proportions, a factor of 2 must be applied to the red and 4 to the green and these factors must be used in any subsequent colour match. Thus, if for a new colour the instrument readings are 40° for red, 10\u0026quot; for green, and 20' for blue, the proportions, in the units agreed upon, are\nred 80, green 40, blue 20\ngiving the trichromatic coefficients as\nred 80/140 = .571, green 40/140 = .286, blue 20/140 = .143\nfrom which we have the unit equation for the colour as\nc = .571R + .286G + .143B\n 在这个例子中，光的实际功率由光圈（aperture）控制。重要的是，光圈的读数（角度）与放出的光线的实际功率成线性关系，因为放出的光子数量应该线性正比于光圈打开的面积，而这一面积又线性正比于角度3。\n为了匹配白光，光圈设置成红光 30°，绿光 15°，蓝光 60°。然后我们决定认为此时红绿蓝是等量的。也就是说，光圈读数为 30° 时放出的红光的数量算一个单位的红光；光圈读数为 15° 时放出的绿光的数量算一个单位的绿光；光圈读数为 60° 时放出的蓝光的数量算一个单位的蓝光。\n定义完基色光的单位，我们就能确定匹配任意测试光需要多少数量（即几个单位）的基色光了。在 Wright 的例子里，新颜色 C 的光圈读数为红光 40°，绿光 10°，蓝光 20°，所以我们需要 40/30 个单位的红光，10/15 个单位的绿光，和 20/60 个单位的蓝光来匹配颜色 C4。数量换算为比例即 4 : 2 : 1。\n需要注意，此时我们还不知道 C 里有多少光，只知道 40/30 个单位的红光，10/15 个单位的绿光，和 20/60 个单位的蓝光对应一定量的 C 光。\n为了量化 C 的数量，Wright 进一步定义：当红绿蓝组分的数量被归一化为加起来等于单位 (1)，即 0.571 : 0.286 : 0.143 时，相匹配的 C 光有一个单位的数量。此时红光、绿光和蓝光的相对数量被称为三色系数（trichromatic coefficients），本质上跟现代术语里的色度系数（chromaticity coefficients）是同一个东西。\nC = 0.571R + 0.286G + 0.143B  我们有两种方式解读这个方程。首先，一个单位的 C 被定义为 0.571 个单位的红光，0.286 个单位的绿光，和 0.143 个单位的蓝光的混合。其次，一个单位的 C 跟 0.571 个单位的红光，0.286 个单位的绿光，和 0.143 个单位的蓝光的混合光颜色相同。请注意，一个单位红绿蓝所含的光量，对应于真实仪器的读数，而一个单位 C 所含的光量虽然是一种物理属性，但却是从红绿蓝的数量里推导出来的纯概念。\n在 Wright 的例子里，通过混合 40/30 个单位的红光，10/15 个单位的绿光，和 20/60 个单位的蓝光，我们得到了 40/30/0.571 = 2.34 个单位的 C。如果我们改变 C 的数量，红绿蓝的数量也会等比例变化，但色度系数保持不变。例如，如果我们想要 2 个单位的 C，应该混合 1.142 个单位的红光，0.572 个单位的绿光，和 0.286 个单位的蓝光，但数量的比例仍为 4 : 2 : 1。\n值得注意的是 Wright 在校准基色的单位时并没有指定用于匹配的白光的数量。如果我们将白光的功率加倍，红绿蓝三基色的读数也会加倍成 60°、30° 和 120°。那我们能将 60° 光圈放出的红光的数量定义为一个单位的红光吗？当然可以！同样地，一个单位的绿光（蓝光）就是 30°（120°）光圈放出的绿光（蓝光）的数量。基色光每个单位所含的绝对光量被改变，但它们之间的比例保持不变。因此色度系数也会保持不变。\n“单位”的概念涉及光的绝对数量，因为它与光圈的读数直接相关，而光圈读数又控制光的功率。但对于计算色度系数而言，只要单位定义成等量的基色产生白色，那么一个单位所含的光的绝对数量并不重要。因此，无论一个单位的红光是 6 瓦特还是 3 瓦特，都不会改变匹配任意量光线时的色度系数。\n总之，计算任意测试光的色度系数的流程为：\n 混合三基色去匹配参考白光，由此定义单位，使得等量的基色能匹配某数量的白光。其中被匹配的白光的绝对数量并不重要。 用测试光匹配三基色，记录它们的仪器读数（即光圈孔径角）。同样，测试光的绝对数量不重要。 根据第一步中的单位定义，将每种基色的仪器读数转换为该基色的量值。 归一化上面的单位数量，使它们的和为 1。这三个归一化后的值即测试光的色度系数。  Wright 以 10 nm 的间隔，对 400 nm 到 700 nm 之间的所有光谱光（spectral lights）做了步骤 2 到 4，得到了这些光的色度系数。他使用的基色光都是单色光：𝛌 = 650 nm 的红光，𝛌 = 530 nm 的绿光，和 𝛌 = 460 nm 的蓝光。参考白光则是英国国家物理实验室（National Physical Laboratory, NPL）的标准白光（具体的功率谱分布见 [Guild 1931] 表 I）。实验中匹配光谱光用到了 10 位观察者，匹配参考白光则用到了 30 位观察者。\n图 2：Wright 初始实验得到的光谱光的色度系数。图自 [Wright 1928]。\n摘自 [Wright 1928] 的图 2 展示了所有光谱光的色度系数，其中每条曲线对应一位观察者。\n3.2 有益的思想实验：使用反直觉的单位系统 下面做一个有益的思想实验，检查你对概念的理解，并且后续构建 CMF 时也会派上用场：假设我们的直觉变成 1 : 2 : 3 的基色产生白色，而非 1 : 1 : 1。这个比例无所谓对错，但会改变一个单位的光含有的绝对数量，同时色度系数也会跟着改变。\n我们可以进行如下的推理。假设在旧的单位系统里，一定量的白光由各一个单位的基色构成；而现在同样数量的白光在新的单位系统里，将由 k 个单位的红光，2k 个单位的绿光，和 3k 个单位的蓝光构成。其中 k 是一个缩放因子（scaling factor），具体值不清楚，因为我们只需要知道产生白光的基色的比例，而不需要知道确切的数量。因此得到下面的方程：\nR + G + B = k R' + 2k G' + 3k B'  其中 R、G、B 代表旧单位系统里一个单位的红光、绿光、蓝光；而 R'、G'、B' 代表新单位系统里一个单位的红光、绿光、蓝光。方程里用两种不同的方式表示了相同数量的白光，这意味着：\n 旧系统里一个单位的红光等价于新系统里 k 个单位的红光 旧系统里一个单位的绿光等价于新系统里 2k 个单位的红光 旧系统里一个单位的蓝光等价于新系统里 3k 个单位的红光  用于测试的 C 光在旧系统里表示为：\nC = 0.571 R + 0.286 G + 0.143 B  它在新系统里表示为：\nC = 0.571k R' + 0.286(2k) G' + 0.143(3k) B'  对系数做归一化使它们的和为 1，由此得到一个单位的 C 在新单位系统里的定义：\nC = 0.571k/(0.571k + 0.286(2k) + 0.143(3k)) R' + 0.286(2k)/(0.571k + 0.286(2k) + 0.143(3k)) G' + 0.143(3k)/(0.571k + 0.286(2k) + 0.143(3k)) B'  消去 k 得到：\nC = 0.571/(0.571 + 0.286×2 + 0.143×3) R' + 0.286×2/(0.571 + 0.286×2 + 0.143×3) G' + 0.143×3/(0.571 + 0.286×2 + 0.143×3) B' = 0.363 R' + 0.364 G' + 0.273 B'  (0.363, 0.364, 0.273) 便是 C 在新单位系统里的色度系数。\n可以观察到两个关键的点：首先，仅仅因为我们改变了单位的定义方式，新系数的值就不同于原始系数 (0.571、0.286、0.143) 了。有三个重要因素决定了色度系数：(1) 基色的选择；(2) 参考白光的选择；(3) 我们用来定义单位的“直觉”。\n其次，新的色度系数可以从原始色度系数中推导出来，不依赖于 k 的具体值。\n一个很实际的问题是，Wright 的初始数据基于他自己挑选的基色（𝛌 = 650 nm 的红光，𝛌 = 530 nm 的绿光，𝛌 = 460 nm 的蓝光）。然而当时 NPL 使用的标准基色与此不同，为 𝛌 = 700 nm 的红光，𝛌 = 546.1 nm 的绿光，𝛌 = 435.8 nm 的蓝光。因此为了与他人的数据进行比较和匹配，他需要用某种方式将自己的数据变换成基于 NPL 标准基色的数据。当然他可以改用 NPL 标准基色重做所有实验，但这也许只是浪费时间。聪明的 Wright 提出了一种巧妙的方法，在原始数据的基础上“合成（synthesizing）”结果，让实验看起来就像是基于 NPL 标准基色做的一样。[Wright 1929] 里描述了这一方法，[Broadbent 2004] 对此也有很好的总结。\n3.3 Guild 的实验 与此同时 Guild 用 7 位观察者做了类似的实验（实际上比 Wright 还早一年），基于他自己挑选的三基色光，以 NPL 标准白光作为参考白光。不出所料，定义单位时也要求等量的基色组合起来可以匹配 NPL 标准白光。\nGuild 并没有记录基色具体的光谱，只是简单提到，基色是通过“让乳白灯泡的白炽灯发出的光线穿过红绿蓝明胶滤光片”得到的。他在该实验中以 5 nm 的间隔得到了 380 nm 到 700 nm 间所有光谱光的色度系数。\n图 3：Guild 初始实验中光谱光的色度系数。图自 [Guild 1930]。\n图 3 取自 [Guild 1930]，展示了测量出的光谱色度系数，其中每条曲线对应一位观察者。你可能已经注意到了，虽然图 2 和图 3 都绘制了色度系数，但它们略有不同。原因在于使用的基色、参考白光和单位系统不同。图 2 对应图 1 中的“Wright 1”步骤，而图 3 对应图 1 中的“Guild 1”步骤。两张图之间的对比清楚显示了基色、参考白光和单位系统的重要性。\n与 Wright 类似，Guild 随后“合成”了他的数据，就好像实验是基于 NPL 基色做的一样，同时仍以 NPL 标准白光作为参考白光。\n图 4：Wright 和 Guild 得到的色度系数的对比。图自 [Broadbent 2004b]。\n此时 Guild 和 Wright 的数据都处于完全相同的系统里（基色 + 参考白光 + 单位系统），所以可以进行比较。当 Guild 合并两组数据时，它们非常吻合。摘自 [Broadbent 2004b] 的图 4 比较了这两组数据，其中点表示 Wright 的数据，线表示 Guild 的数据。三种颜色对应三基色的光谱色度系数。\n考虑到两组数据是独立测出的，使用完全不同的基色、观察者和实验设置，图中近乎完美的匹配令人称奇。此外 Guild 用来比较的两组数据甚至不是原始实验数据，而是经合成/变换后的数据。这种几乎完美的一致性使人们有信心为色彩空间定义出一套标准。\nGuild 又陆续对数据做了增强处理（平均、曲线拟合、平滑等），以使他的数据与 Wright 的数据相统一，由此得出了最终的光谱色度系数。我们通常用 r(𝛌)、g(𝛌) 和 b(𝛌) 表示波长为 𝛌 的光谱光的色度系数。\n4. 亮度系数：改变参考白光 在利用数据导出 CMF 前还有最后一步。CIE 1931 标准里的参考白光不是 NPL 标准白光，而是一种假想的等能量白光（EEW），在所有波长上具有相同的能量或功率。Guild 利用手中的数据合成了基于 EEW 的光谱色度系数，就好像他们真的用 EEW 作为参考白光做了实验一样。\n为了理解 Guild 的变换操作，我们需要知道一个关键的新概念：亮度系数（luminance coeffcient）。Guild 和 Wright 在他们的论文里称其为相对亮度（relative luminance）或亮度因子（luminosity factor）。接下来介绍变换的原理。\n问题设置如下。已知波长为 𝛌 的光谱光基于 NPL 标准基色，以 NPL 标准白光为参考白光时的色度系数，需要计算以 EEW 为参考白光时该光新的色度系数（仍使用 NPL 标准基色）。\nGuild 的方法是弄清楚旧系统中需要多少个单位的基色才能匹配 EEW。如果我们能计算出这些数值，就能将问题转化为之前的思想实验。为了得到基色的比例，我们将逐个波长地进行计算。基本思路是，对构成 EEW 的每个波长的光，明确旧系统中需要多少单位的基色来匹配该光的亮度（luminance）。然后将它们相加以得到基色总的量值，再标准化为 1 得出实际比值。\n为了不失一般性，假设 EEW 光谱上每个波长的功率都为 P。EEW 在 𝛌 波长处的单色组分用 NPL 标准基色在旧单位系统（即参考白光为 NPL 标准白光）中表示为：\nW(𝛌) = M(𝛌)(r(𝛌)R + g(𝛌)G + b(𝛌)B)  其中 R、G、B 分别是旧系统中一个单位的红光、绿光、蓝光，r(𝛌)、g(𝛌)、b(𝛌) 是旧系统中波长 𝛌 的色度系数，而 M(𝛌) 是未知的缩放因子，使得 M(𝛌)r(𝛌)、M(𝛌)g(𝛌)、M(𝛌)b(𝛌) 分别表示匹配 𝛌 处的 EEW 所需的红绿蓝的绝对数量（以单位计）。注意 M(𝛌) 会随 𝛌 的变化而变化。\n在 EEW 的全光谱上对 𝛌 进行积分，得到匹配 EEW 所需的红绿蓝的总量（以单位计）：\nr = ∑M(𝛌)r(𝛌) g = ∑M(𝛌)g(𝛌) b = ∑M(𝛌)b(𝛌)  因此 EEW 可以表示为\nEEW = rG + gG + bB  那怎么得到 M(𝛌) 呢？关键在于利用亮度的不变性：EEW 在任意波长 𝛌 处的亮度应该等于匹配该波长的基色光的总亮度。毕竟这也是我们生成任意复杂光谱对应颜色的方法。\n计算 EEW 在特定波长 𝛌 处的亮度很简单：等于 P * V(𝛌)，其中 V(𝛌) 是亮度效率函数（luminance efficiency function），Guild 也称其为日视功效函数（photopic efficacy function），由 CIE 在 1924 年测量得到。这里我们忽略常数因子（683 lm/W）。\n但波长 𝛌 处三基色的亮度怎么计算呢？已知波长 𝛌 所需三基色的绝对数量（即 M(𝛌)r(𝛌)、M(𝛌)g(𝛌) 和 M(𝛌)b(𝛌) ），但怎么把光的数量和亮度联系起来？色度系数只是比例，不含亮度相关的信息，只表示为了创造任意数量的单色光 𝛌 的颜色，三基色量值的比例应该为 r(𝛌) : g(𝛌) : b(𝛌)。无论单色光的数量是多少，比值都保持不变。想匹配数量更多的 𝛌 光就需要更多基色光，反之亦然。显然我们还缺少一些蕴含亮度信息的东西。\n为此需要引入亮度系数的概念。在特定系统中，光的亮度系数定义为一个单位的光的亮度。通常还要对三基色的亮度系数进行缩放，使红光的系数为 1。在 Guild 的实验里，他测量并计算出了 NPL 标准基色的亮度系数，Lʳ、Lᵍ 和 Lᵇ，如下所示5：\nLʳ = 1.0000 Lᵍ = 4.4036 Lᵇ = 0.0471  𝛌 处红绿蓝的总亮度即：\nM(𝛌)r(𝛌)LʳN + M(𝛌)g(𝛌)LᵍN + M(𝛌)b(𝛌)LᵇN  其中 N 是未知的缩放因子。于是有：\nV(𝛌)P = M(𝛌)r(𝛌)LʳN + M(𝛌)g(𝛌)LᵍN + M(𝛌)b(𝛌)LᵇN  因此可以计算 M(𝛌)：\nM(𝛌) = V(𝛌)P / N(r(𝛌)Lʳ + g(𝛌)Lᵍ + b(𝛌)Lᵇ)  其中 V(𝛌)、r(𝛌)、g(𝛌)、b(𝛌)、Lʳ、Lᵍ、Lᵇ 的值都是已知的，P/N 是未知的缩放因子。不妨令：\nU(𝛌) = V(𝛌) / (r(𝛌)Lʳ + g(𝛌)Lᵍ + b(𝛌)Lᵇ)  以及 P/N = C，得到：\nM(𝛌) = U(𝛌)C  于是，\nr = ∑M(𝛌)r(𝛌) = C∑U(𝛌)r(𝛌), g = ∑M(𝛌)g(𝛌) = C∑U(𝛌)g(𝛌), b = ∑M(𝛌)b(𝛌) = C∑U(𝛌)b(𝛌).  这意味着 EEW 在旧单位系统里表示为：\nEEW = C∑U(𝛌)r(𝛌) R + C∑U(𝛌)g(𝛌) G + C∑U(𝛌)b(𝛌) B  因此，EEW 在旧单位系统里的色度系数为：\nWr = ∑U(𝛌)r(𝛌) / (∑U(𝛌)r(𝛌) + ∑U(𝛌)g(𝛌) + ∑U(𝛌)b(𝛌)) Wg = ∑U(𝛌)g(𝛌) / (∑U(𝛌)r(𝛌) + ∑U(𝛌)g(𝛌) + ∑U(𝛌)b(𝛌)) Wb = ∑U(𝛌)b(𝛌) / (∑U(𝛌)r(𝛌) + ∑U(𝛌)g(𝛌) + ∑U(𝛌)b(𝛌))  可见未知缩放因子 C（即 P/N）消去了。Guild 计算出 Wr、Wg 和 Wb 的值为 0.3013、0.3140 和 0.38476。\n这个结果非常重要，因为现在我们知道了旧单位系统中 EEW 是由 Wr : Wg : wb 比例的红绿蓝混合得到的。另外根据定义，新单位系统中 EEW 应该以 1 : 1 : 1 的比例混合。于是问题本质上变回了我们之前做过的思想实验！使用跟前面相同的方法，就能将任意光谱光的色度系数从旧单位系统（以 NPL 标准白光为参考白光）转换到新单位系统（以 EEW 为参考白光）。\n最后一点，新单位系统中的亮度系数也必须改变。亮度系数表示一个单位的光的相对亮度，因为新系统里一个单位的意义被改变，所以亮度系数也肯定要跟着修改。那这该怎么计算呢？新系统里一个单位的红光等价于旧系统里 Wr 个单位的红光，后者的总亮度为 WrLʳ，在新系统里变成了一个单位红光的亮度。同样的过程也适用于绿光和蓝光。按红光系数为 1 的方式将新系统里的亮度归一化，得到的新亮度系数的值如下所示，后续将用来构造 CMF：\nLʳ = 1.0000 Lᵍ = 4.5907 Lᵇ = 0.0601  总结一下，本节按 CIE 1931 标准的设置，以 NPL 基色作为基色光，EEW 作为参考白光，定义等量基色光产生一定量参考白光的单位系统，计算出了所有光谱光的色度系数。\n图 5 绘制了用于 CIE 1931 RGB 空间的光谱色度系数。可以看到 R 值在 𝛌 = 700 nm 确实为 1，𝛌 = 546.1 nm 处 G 值为 1，𝛌 = 435.8 nm 处 B 值为 1，与我们的直觉相符。不过这些曲线还不能算是颜色匹配函数！\n图 5：CIE 1931 RGB 空间里的光谱色度系数。\n需要注意，亮度系数和 V(𝛌) 存在一个很关键的区别：V(𝛌) 表示单位功率的相对亮度，而亮度系数表示单位光量的相对亮度。一个单位的功率和一个单位的光是不同的——前者量化了一个绝对物理量，而后者是相对于我们使用的单位系统而言的。1 个单位的 R 可以有 100 瓦特，1 个单位的 B 可以有 5 瓦特，或者其它值都可以。\n5. 从色度系数到 CMF 现在我们终于有了用来导出 CMF 的数据。本节先解释 CMF 的用途，然后解释如何从色度函数（chromaticity function）构造 CMF，好让 CMF 按预期派上用场。\n5.1 为什么需要 CMF？ 我们的目标是：通过混合三种基色光产生所有颜色，无论颜色的功率谱分布（Spectral Power Distribution, SPD）有多复杂。也就是说，对于给定的 SPD，我们想知道需要多少单位的基色来匹配其颜色。而计算基色的总量需要“逐步进行”：先计算匹配单色光 𝛌 所需的基色的量，然后将所有波长所需的基色数量相加，计算出匹配目标光的基色总量。不过仅有色度图还不够，因为 SPD 表示功率的绝对数值，而色度仅表示比例关系。\n不妨令目标光 T 的 SPD 为 𝚽，其在波长 𝛌 处的功率为 𝚽(𝛌)。问题是如何确定匹配单色光 𝛌 需要多少单位的基色？色度系数 r(𝛌)、g(𝛌) 和 b(𝛌) 只告诉我们应该以什么比例混合基色来产生一个单位的单色光 𝛌，但我们想确定具体需要混合多少基色来匹配 𝚽(𝛌) 瓦特的单色光 𝛌。\nCMF 就是为了解决这一问题而引入的。颜色匹配函数 R()、G()、B() 定义为匹配 𝚽 上每种单色光 𝛌 的单位功率亮度所需的红光、绿光、蓝光的数量（以单位计）。因此获取 CMF 后就能计算目标光 T 在 𝛌 处所需的基色的量。下面是详细步骤。\n目标光 T 在 𝛌 处的功率为 𝚽(𝛌)。因为匹配一瓦特的单色光 𝛌 的亮度需要 R(𝛌) 个单位的红光、G(𝛌) 个单位的绿光，和 B(𝛌) 个单位的蓝光，所以匹配目标光中单色光 𝛌 的亮度自然需要 𝚽(𝛌)R(𝛌) 个单位的红光、𝚽(𝛌)G(𝛌) 个单位的绿光，和 𝚽(𝛌)B(𝛌) 个单位的蓝光。\n因此为了匹配目标光的每一部分，需要 ∑𝚽(𝛌)R(𝛌) 个单位的红光、∑𝚽(𝛌)G(𝛌) 个单位的绿光，和 ∑𝚽(𝛌)B(𝛌) 个单位的蓝光。如果 SPD 是连续的而非离散样本，那么得到下列熟悉的方程：\nRt = ∫𝚽(𝛌)R(𝛌)d𝛌 Gt = ∫𝚽(𝛌)G(𝛌)d𝛌 Bt = ∫𝚽(𝛌)B(𝛌)d𝛌  匹配 SPD 为 𝚽 的目标光所需的红光、绿光和蓝光的绝对数量即 Rt、Gt 和 Bt。同时 Rt、Gt 和 Bt 间的比值就是目标光的色度系数，即产生一个单位的 T 所需的红绿蓝的量。Rt、Gt 和 Bt 的现代术语是 T 的三刺激值（tristimulus values）。\n5.2 如何构造 CMF 我们已经知道了怎么用 CMF 生成颜色，下一个问题是如何构造 CMF 呢？\n请记住，色度系数 r(𝛌)、g(𝛌) 和 b(𝛌) 表示应该以什么比例混合基色以获取一个单位的单色光 𝛌。现在假设为了匹配功率为一个单位的单色光 𝛌 的亮度，需要 k(𝛌)r(𝛌)、k(𝛌)g(𝛌) 和 k(𝛌)b(𝛌) 个单位的基色。为了维持单色光 𝛌 的色度，这里基色数量的比例为 r(𝛌) : g(𝛌) : b(𝛌)。\n单色光 𝛌 的单位功率亮度为 V(𝛌)，而三基色在 𝛌 处的总亮度为 Lʳk(𝛌)r(𝛌) + Lᵍk(𝛌)g(𝛌) + Lᵇk(𝛌)b(𝛌)，因此有 V(𝛌) = Lʳk(𝛌)r(𝛌) + Lᵍk(𝛌)g(𝛌) + Lᵇk(𝛌)b(𝛌)。\n于是可以计算 k(𝛌)：\nk(𝛌) = V(𝛌) / (Lʳr(𝛌) + Lᵍg(𝛌) + Lᵇb(𝛌))  分母 Lʳr(𝛌) + Lᵍg(𝛌) + Lᵇb(𝛌) 可以视作单色光 𝛌 的亮度系数，这也是线性系统假设应用于亮度系数的一个例子。\n有了 k(𝛌) 就能计算三个 CMF 了：\nR(𝛌) = k(𝛌)r(𝛌) = V(𝛌)r(𝛌) / (Lʳr(𝛌) + Lᵍg(𝛌) + Lᵇb(𝛌)) G(𝛌) = k(𝛌)g(𝛌) = V(𝛌)g(𝛌) / (Lʳr(𝛌) + Lᵍg(𝛌) + Lᵇb(𝛌)) B(𝛌) = k(𝛌)b(𝛌) = V(𝛌)b(𝛌) / (Lʳr(𝛌) + Lᵍg(𝛌) + Lᵇb(𝛌))   译注：按前文思想实验里的步骤，k(𝛌)r(𝛌) 乘上未知因子 N 才表示亮度。那这里是漏掉 N 了吗？译者的理解是，这里所求的 CMF，实际上是一种乘以亮度系数后直接等于亮度的数量。\n 因此 CMF 可以理解为 r(𝛌)、g(𝛌) 和 b(𝛌) 缩放后的结果。注意 k(𝛌) 不是常数，是关于 𝛌 的函数。下图为 CIE 1931 RGB 的 CMF：\n图6：CIE CMF（实线）和 Wright-Guild 数据得出的 CMF（虚线），前者的参考白光是 EEW，后者是 NPL 标准白光。\nCMF 的一个重要性质是它的值跟基色和参考白光的选取都有关系，但参考白光有影响的事实经常被忽视！这是因为参考白光决定了“一个单位”对于基色的意义，由此影响到 r(𝛌)、g(𝛌)、b(𝛌)、Lʳ、Lᵍ 和 Lᵇ。\n图 6 展示了 CIE 1931 RGB CMF（实线），作为对比还叠加了基于合并的 Wright-Guild 数据的 CMF（虚线）。前者的参考白光是 EEW，后者是 NPL 标准白光。可以看到这两个系统的 CMF 趋势一致，但仍存在不同。这说明 CMF 确实取决于参考白光的选取。\n5.3 CIE 1931 RGB CMF 的性质 1. EEW 的 RGB 三刺激值都相等，因为单位系统的定义里就要求 EEW 由等量基色混合而得。因此，\n∫𝚽(𝛌)R(𝛌)d𝛌 = ∫𝚽(𝛌)G(𝛌)d𝛌 = ∫𝚽(𝛌)B(𝛌)d𝛌  因为 EEW 所有波长的 𝚽(𝛌) 都为 1（常数），所以有：\n∫R(𝛌)d𝛌 = ∫G(𝛌)d𝛌 = ∫B(𝛌)d𝛌  这三个积分表示 CMF 曲线下面的面积，这就是为什么开头引用的维基百科里说：\n Note that rather than specify the brightness of each primary, the curves are normalized to have constant area beneath them.\n 2. 根据定义，任意波长 𝛌 都满足 V(𝛌) = LʳR(𝛌) + LᵍG(𝛌) + LᵇB(𝛌)。方程两边均表示 1 W 的单色光 𝛌 具有的亮度。\n对两边积分得：\n∫V(𝛌)d𝛌 = ∫LʳR(𝛌)d𝛌 + ∫LᵍG(𝛌)d𝛌 + ∫LᵇB(𝛌)d𝛌 = Lʳ∫R(𝛌)d𝛌 + Lᵍ∫G(𝛌)d𝛌 + Lᵇ∫B(𝛌)d𝛌  因为 ∫R(𝛌)d𝛌 = ∫G(𝛌)d𝛌 = ∫B(𝛌)d𝛌，所以：\n∫R(𝛌)d𝛌 = ∫G(𝛌)d𝛌 = ∫B(𝛌)d𝛌 = ∫V(𝛌)d𝛌 / (Lʳ + Lᵍ + Lᵇ)  三条 CMF 曲线下的面积不仅相等，且等于 V(𝛌) 曲线面积的 1 / (Lʳ + Lᵍ + Lᵇ) 倍。\n3. 如今 CIE 1931 RGB 空间的光谱色度系数是由 CMF R(𝛌)、G(𝛌) 和 B(𝛌) 计算而来的。但正如我们所见，当年 Guild 和 Wright 是先计算光谱色度系数 r(𝛌)、g(𝛌) 和 b(𝛌) 再导出 CMF。那么这两种计算方式的不同会改变光谱色度系数的数值吗？\n答案是不会。因为从前面的方程可以看到，R(𝛌) : G(𝛌) : B(𝛌) 跟 r(𝛌) : g(𝛌) : b(𝛌) 是相同的。CMF 只不过是色度系数乘以 k(𝛌) 罢了。\n4. CIE 1931 标准中计算出的 Lʳ、Lᵍ 和 Lᵇ 分别为 1.000、4.5907 和 0.0601。这解释了维基百科的注解：\n The resulting normalized color matching functions are then scaled in the r : g : b ratio of 1 : 4.5907 : 0.0601 for source luminance\u0026hellip;\n 这句话的真正含义是，𝚽(𝛌)LʳR(𝛌)、𝚽(𝛌)LᵍG(𝛌) 和 𝚽(𝛌)LᵇB(𝛌) 表示匹配 𝚽(𝛌) 的亮度时红光、绿光和蓝光的亮度。\n图 7：EEW 和三基色的亮度。\n图 7 画出了 EEW 和三基色的亮度。不出所料，蓝光对亮度的贡献非常小，因为视网膜上 S 视锥细胞仅占约 2% 到 7% [Roorda 1999]。\n5. 我们已经知道了 LʳR(𝛌)、LᵍG(𝛌) 和 LᵇB(𝛌) 表示匹配 1 W 的单色光 𝛌 的亮度时，所需红光、绿光和蓝光的亮度。那此时所需基色的功率呢？\n红光的功率为 LʳR(𝛌)/V(700)，即 R(𝛌) 乘以缩放因子 Lʳ/V(700)。同理，要计算绿光和蓝光的功率，G(𝛌) 和 B(𝛌) 分别乘以 Lᵍ/V(546.1) 和 Lᵇ/V(435.8)。三个缩放因子的比例为 Lʳ/V(700) : Lᵍ/V(546.1) : Lᵇ/V(435.8)，归一化后即 72.0962 : 1.3791 : 1。\n这解释了为什么维基百科说：\n The resulting normalized color matching functions are then scaled in the r : g : b ratio of \u0026hellip; 72.0962 : 1.3791 : 1 for source radiance.\n 其完整含义是，匹配 1 W 的单色光 𝛌 的亮度时，所需红光、绿光和蓝光的功率比例为 72.0962R(𝛌) : 1.3791G(𝛌) : B(𝛌)7。\n6. 即便不知道亮度系数 Lʳ、Lᵍ 和 Lᵇ 的值，也有方法计算准确的数值。考虑到参考白光的三刺激值相等：\n∫𝚽(𝛌)R(𝛌)d𝛌 = ∫𝚽(𝛌)G(𝛌)d𝛌 = ∫𝚽(𝛌)B(𝛌)d𝛌  如果 SPD 的采样点足够多，积分可以换成求和：\n∑𝚽(𝛌)k(𝛌)r(𝛌) = ∑𝚽(𝛌)k(𝛌)g(𝛌) = ∑𝚽(𝛌)k(𝛌)g(𝛌)  代入 k(𝛌) = V(𝛌) / (Lʳr(𝛌) + Lᵍg(𝛌) + Lᵇb(𝛌)) 得：\n∑V(𝛌)𝚽(𝛌)r(𝛌) / (Lʳr(𝛌) + Lᵍg(𝛌) + Lᵇb(𝛌)) = ∑V(𝛌)𝚽(𝛌)g(𝛌) / (Lʳr(𝛌) + Lᵍg(𝛌) + Lᵇb(𝛌)) = ∑V(𝛌)𝚽(𝛌)b(𝛌) / (Lʳr(𝛌) + Lᵍg(𝛌) + Lᵇb(𝛌))  已知 V(𝛌), 𝚽(𝛌), r(𝛌), g(𝛌) 和 b(𝛌)，有三个变量 Lʳ、Lᵍ、Lᵇ 和两个方程，因此可以解出变量间的比例。\n观察 CIE RGB 数据向 CIE XYZ 变换的过程 [Fairman 1997] 发现，似乎 Lʳ、Lᵍ、Lᵇ 就是通过这种拟合方式算出来的，而不是用的 Wright-Guild 实验里被转换的数据。\n6. 致谢 我非常想向 Arthur Broadbent 表达我的感激之情。他的逆向过程手法和对实验过程的详细记录就像魔法一样厉害。他在论文里使用的电子邮件地址 abroadbe@ubishops.ca 已经失效，非常希望能有人告诉我怎么联系他，以便我向他表示感谢！\n参考 [Wright 1928] A re-determination of the trichromatic coefficients of the spectral colours. Trans Opt Soc London 1928–29;30:141–164\n[Wright 1929] A re-determination of the mixture curves of the spectrum. Trans Opt Soc London 1929–30;31:201–211.\n[Guild 1931] The colorimetric properties of the spectrum. Philos Trans Roy Soc London Ser. A 1931;230:149–187.\n[Broadbent 2004a] A Critical Review of the Development of the CIE1931 RGB Color-Matching Functions. Color Research \u0026amp; Application 29(4):267–272. August 2004.\n[Broadbent 2004b] Calculation from the original experimental data of the CIE 1931 RGB standard observer spectral chromaticity co-ordinates and color matching functions.\n[Fairman 1997] Fairman HS, Brill MH, Hemmendinger H. How the CIE1931 color- matching functions were derived from the Wright–Guild data. Color Res Appl 1997;22:11–23.\n[Service 2016] The Wright - Guild Experiments and the Development of the CIE 1931 RGB and XYZ Color Spaces.\n[Roorda 1999] The arrangement of the three cone classes in the living human eye. Nature volume 397, pages 520–522(1999).\n注释   这些数字并不精确，因为 CIE 1931 RGB CMF 是以 5 nm 的间隔发布的。我使用了最接近的值：G 对应 545 nm，B 对应 435 nm。如果需要更准确的数值，可以用高阶多项式来拟合 CMF，这也是色度学里的标准做法。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 第二篇论文大概还没有正式发表，主要是第一篇的扩充版，不过第二篇叙述方式更加平易近人。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 实践中这种关系取决于实验里旋钮的配置方式，可以是线性的、对数的，或者其它关系。理论上旋钮的读数如何反映放出的光的功率/亮度/光子数并不重要，只要我们能通过实验记录一个量到另一个量的映射关系即可。但由于光圈读数是光量的代理量，所以需要利用线性关系来应用 Grassmann 颜色线性律（Grassmann\u0026rsquo;s linear law of color）。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 正如前面所提过的，需要假设光圈读数跟光功率是线性相关的，好让我们应用 Grassmann 颜色线性律。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 实际上 Guild 1931 年论文里的值是 Lʳ = 1.0000, Lᵍ = 4.390, Lᵇ = 0.048。Broadbent 发现 Guild 当时犯了错，后来又订正了，但 Guild 从未公开发表过正确的结果。Broadbent [Broadbent 2004a, Broadbent 2004b] 根据 CIE 1931 RGB 数据逆向工程出了正确的值，也就是博文中列出的那三个。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 此处同样是 Braodbent 修正后的结果。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 给定面积和立体角时，功率之比跟辐亮度（radiance）之比是相等的。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","date":"2023-07-24","permalink":"https://zhajiman.github.io/post/color_matching_function/","tags":["色彩","翻译"],"title":"如何从最初的颜色匹配实验导出 CIE 1931 RGB 颜色匹配函数"},{"content":"前言 裁剪或者说白化，就是让填色图只显示在多边形里面，不显示在多边形外面，例如只显示 GeoAxes.contourf 在中国境内的结果。实现方法为：\nfrom matplotlib.path import Path from cartopy.mpl.patch import geos_to_path from cartopy.io.shapereader import Reader reader = Reader(filepath) geom = next(reader.geometries()) reader.close() cf = ax.contourf(X, Y, Z, transform=crs) geom = ax.projection.project_geometry(geom, crs) path = Path.make_compound_path(*geos_to_path(geom)) for col in cf.collections: col.set_clip_path(path, ax.transData)   将 crs 坐标系上的多边形对象变换到 data 坐标系上。 利用 geos_to_path 和 make_compound_path 将变换后的多边形转为 Path 对象。 对 QuadContourSet.collections 里的每个成员调用 set_clip_path 方法，并且指定 data 坐标系。  完整代码为：\nimport numpy as np import shapefile import shapely.geometry as sgeom from shapely.ops import unary_union import matplotlib.pyplot as plt from matplotlib.path import Path import cartopy.crs as ccrs from cartopy.mpl.patch import geos_to_path def test_data(): '''生成测试的二维数据.''' x = np.linspace(70, 140, 100) y = np.linspace(10, 60, 100) X, Y = np.meshgrid(x, y) Z = X + Y return X, Y, Z def load_country(): '''读取中国国界线数据.''' filepath = './data/bou2_4p.shp' with shapefile.Reader(filepath, encoding='gbk') as reader: provinces = list(map(sgeom.shape, reader.shapes())) country = unary_union(provinces) return country def make_map(extents): '''创建地图.''' map_crs = ccrs.LambertConformal( central_longitude=105, standard_parallels=(25, 47) ) data_crs = ccrs.PlateCarree() fig = plt.figure() ax = fig.add_subplot(projection=map_crs) ax.set_extent(extents, crs=data_crs) ax.coastlines() return ax X, Y, Z = test_data() country = load_country() crs = ccrs.PlateCarree() ax = make_map([75, 135, 10, 60]) ax.add_geometries(country, crs, fc='none', ec='k') cf = ax.contourf(X, Y, Z, levels=20, transform=crs) geom = ax.projection.project_geometry(country, crs) path = Path.make_compound_path(*geos_to_path(geom)) for col in cf.collections: col.set_clip_path(path, ax.transData)  但当地图的显示范围比用来裁剪的形状要小时，就会出现填色图溢出地图边界的情况。下面以东南区域为例：\n创建矩形边界小地图的代码为：\nax = make_map([100, 125, 15, 40])  创建扇形边界小地图的代码为：\nax = make_map([100, 125, 15, 40]) verts = [(100, 15), (125, 15), (125, 40), (100, 40), (100, 15)] rect = Path(verts).interpolated(100) ax.set_boundary(rect, crs)  发现填色图虽然被国界裁剪了，但西部和东北区域溢出了地图的边界，这个效果显然是不可接受的。本文的目的是解释其原因并给出两种通用且简单的解决方法。文中 Catopy 版本为 0.21。\n出界的原因 Artist.clipbox 属性是一个矩形的边界框，能够在绘制 Artist 时不让它超出这个框框的范围。Artist._clippath 属性是 Path 对象，能够在绘制 Artist 时裁剪它。Path 对象可以是任意形状，可以是带洞的多边形，可以由多个多边形组成，只要在构造 Path 时设定好 codes 参数即可。刚创建的 Artist 的这两个属性都为 None，表示不做裁剪；Artist 被添加到 Axes 上时，会用代表显示范围的矩形的 Axes.patch 属性作为 clipbox。因此 Axes.plot 和 Axes.contourf 等方法画出来的结果从来都不会有出界的情况。\n一般 Artist._clippath 属性始终为 None，我们可以通过 Artist.set_clip_path 方法来设定它，并且注意到其优先级低于 _clipbox。所以如果你在普通的 Axes 上做过地图裁剪的话，会发现并没有填色图出界的问题。实际上，出界是因为 GeoAxes.patch 并不一定是矩形的，例如全球范围的 Lambert 投影地图的边界是展开的圆锥，Mollweide 投影地图的边界是一个椭圆。为了让 Artist 的内容不超出形状各异的边界，Cartopy 选择将 GeoAxes.patch 赋给 _clippath，clipbox 保持为 None（即便地图边界实际上是矩形）。\n简言之，Cartopy 在画图时已经用地图的边界裁剪了填色图，我们之后再用中国国界做裁剪，就会破坏掉原来的裁剪效果。当中国国界小于地图边界时不会露陷，而大于时就会出现填色图超出地图边界的问题。\n解决方法 设定 bbox 注意到 Axes 和 GeoAxes 都有 bbox 属性，也能表示轴的边界框。当地图边界是矩形时，GeoAxes.patch 和 GeoAxes.bbox 表示相同的范围，因此设定 Artist.clipbox 来裁去出界的部分：\nfor col in cf.collections: col.set_clip_path(path, ax.transData) col.set_clip_box(ax.bbox)  只用加一行，矩形边界地图的出界问题就解决了。但扇形边界的地图里，左上角仍有少许出界的部分。因为 GeoAxes.bbox 只是框住整个 GeoAxes 的方框，而 GeoAxes.patch 不一定与之重合。为此下面再给出第二种方法。\n与地图边界求与 思路是提取地图边界在 data 坐标系里的坐标点，构造一个多边形对象，与做过坐标变换的、同样在 data 坐标系里的国界多边形求与（即取两个多边形相重叠的部分），用得到的新多边形去做裁剪。代码为：\npatch = ax.patch ax.draw_artist(patch) trans = patch.get_transform() - ax.transData path = patch.get_path().transformed(trans) boundary = sgeom.Polygon(path.vertices) geom = ax.projection.project_geometry(country, crs) geom = geom \u0026amp; boundary path = Path.make_compound_path(*geos_to_path(geom)) for col in cf.collections: col.set_clip_path(path, ax.transData)  GeoAxes.patch 一般基于 data 坐标系，但如果调用过 GeoAxes.set_boundary，也可能变到其它坐标系上，因此这里通过 Transform 对象的减法操作来得到 data 坐标系上的坐标点。同时注意到，GeoAxes.patch 的具体数值是在渲染过程中决定的，所以需要先调用 Axes.draw_artist 或 Canvas.draw 方法。效果如下图：\nBbox 法代码简单，但是不能正确处理非矩形边界的地图，并且有些情况下耗时更长；求与法能保证效果，但如果之后修改地图的显示范围，或者在交互模式中进行拖拽，则会出现填色图缺漏的情况。\n结语 本文找出了 Cartopy 裁剪填色图出界的原因，并给出了两种解决方法。但两种方法都不算完美，也许应该考虑在 draw_event 事件中进行裁剪并缓存 Path 对象？如果读者有好的方法的话还请多多交流。\n另外笔者上传的 frykit 包里实现了求与法，可以通过 clip_by_cn_border 函数直接用国界裁剪 contourf 和 pcolormesh 等画图结果，感兴趣的读者也可以用用。\n参考链接 matplotlib.transforms\nApply set_clip_path to contours, but the set_extend is not work. #1580\ncontour.set_clip_path(clip) beyond borders #2052\n","date":"2023-06-14","permalink":"https://zhajiman.github.io/post/cartopy_clip_outside/","tags":["cartopy","matplotlib"],"title":"Cartopy 系列：裁剪填色图出界问题"},{"content":"字体  思源黑体版本指南 思源黑体：adobe-fonts/source-han-sans。建议下载 Language Specific OTFs Simplified Chinese (简体中文) 包。 思源宋体：adobe-fonts/source-han-serif 霞鹜文楷：lxgw/LxgwWenKai 猫啃网  图源  anshumanv/awesome-anime-sources  日语  日语语法指南 Weblio 辞書・百科事典 jisho 日本語（漢字かな混じり文）にローマ字ルビ  嵌字  【漫画嵌字】漫画汉化之6分钟嵌字教程（汉化基础教程） 【嵌字教程】零基础到初级 失传技术研究所小讲堂 篇八十四：面向入门的规范嵌字教程——嵌字从入门到强迫症  PS 快捷键 查看和移动  方向键控制页面移动。 Alt + 滚轮控制放大缩小。 按住空格键能变成抓手工具。  编辑操作  Ctrl + Z 还原一次，Ctrl + Alt + Z 可以持续撤销。 Ctrl + J 原地复制并粘贴。 按住 Shift 再调整大小可以保持比例不变。  文字工具  Ctrl + Enter 完成文字录入。  画笔工具  x 键切换前景色和背景色。 右键打开画笔菜单。 [ 和 ] 调节笔刷大小。  填色  Alt + Del 填充前景色，Ctrl + Del 填充背景色。  选区  直接拖动选区会移动选区内的内容，Alt + 左键则能只拖动选区本身。 Ctrl + D 取消选区。 按住 Shift 能一次添加多个选区，或者使用“添加到选区”选项。 Shift + F5 对选中内容进行内容识别。  动作  自定义动作：创建三个图层：半透明图层、涂白图层和背景图层。其中涂白图层用于抹字。 ","date":"2023-03-24","permalink":"https://zhajiman.github.io/post/translation_resources/","tags":["资源","翻译"],"title":"一些汉化资源"},{"content":"简单汇总罗列一下我在网上找到的还不错的 Python 相关资源，包括语言本身以及各种常用库的教程，当然触手可及的官方文档就不收纳了。通通都是免费资源（付费的咱也看不到），分享给有需要的读者。不过互联网资源并非恒久不灭，说不定哪天域名就失效了，或是原作者突然隐藏文章，且看且珍惜吧。\nPython 语言 菜鸟教程：Python 3 教程：零起步的中文教程。\n廖雪峰的 Python 教程：同上，覆盖话题更广。\nA Byte of Python：快速上手 Python 的英文教程，适合有一定编程基础的读者。同时存在名为《简明 Python 教程》的中文版。\nComposing Programs：UC Berkeley 大学 CS 61A 课程的讲义，以 Python 语言讲解计算机程序的结构和阐释，其中对数据、抽象和函数的讲解鞭辟入里，非常推荐。\nPython 最佳实践指南：提供了关于 Python 安装、配置和日常使用的很多实用建议。\nPython Cookbook 第三版：非常经典的一本编程技巧合集，中文版可在线阅读。\nWhat the f*ck Python：列举了一些有趣且鲜为人知的 Python 特性，当遇到 bug 时可以来看看是不是中招了。\nPython 工匠：作者 piglei 关于 Python 编程技巧和实践的文章合集。\n古明地盆的博客：以及公众号“古明地觉的编程教室”。\nPython behind the scenes：分析 Python 底层的一系列博文。\nPythonのthreadingとmultiprocessingを完全理解：对 Python 多进程的详细介绍。\nNumPy、SciPy 和 Pandas NumPy Illustrated: The Visual Guide to NumPy：以图解的方式形象展现了 NumPy 数组的结构和常用用法，特别是强调了行向量、列向量和矩阵的关系，非常值得一读。知乎 上有中文翻译版。\nPython Data Science Handbook：虽然叫手册，但内容编排有条理，带你入门数据科学必备的几个包。\nWhy Python is Slow: Looking Under the Hood：上一本书的作者对于 Python 和 NumPy 速度差距的进一步分析。\nFrom Python to Numpy：介绍了很多高级用法和在物理学中的应用。\nScipy Lecture Notes：关于 NumPy 和 SciPy 的讲义，对于 NumPy 的讲解比常见的教程更深入一些。\nSciPy Cookbook：介绍了很多 SciPy 的应用场景，附带一些 NumPy 和 Matplotlib 的技巧。\nPython for Data Analysis：Pandas 包的作者写的电子书，比网上乱七八糟的教程要细致全面得多。\nxarray を用いたデータ解析：xarray 的开发者之一写的日文入门教程。\nMatplotlib Anatomy of Matplotlib：很经典的入门教程，比官方 User Guide 更详细一点。\nCheatsheets for Matplotlib users：速查表，有助于快速查询颜色、色表、线形、散点形状、常用函数等。\nMatplotlib 3.0 Cookbook：覆盖了方方面面的技巧，有需要可以查阅。\nScientific Visualization: Python + Matplotlib：站在科学可视化的高视点指导作图，虽然缺乏操作细节，但书中的示例非常炫酷，代码很值得学习。\nCreating publication-quality figures with Matplotlib：教你如何将出图提升到可出版的质量。\nThe Art of Effective Visualization of Multi-dimensional Data：带你巡游多维数据可视化的种种方法，貌似一些公众号里有中文翻译版。\nThe Architecture of Matplotlib：创始人关于 Matplotlib 架构的解说，不错的补充材料。\nImage Processing in Python with Pillow：利用 Pillow（即 PIL）处理图片的入门教程。PIL 在裁剪、拼接和转换格式等方面比 Matplotlib 更方便。\n气象相关 Python for Atmosphere and Ocean Scientists：超入门教程，带你过一遍 xarray、Cartopy、Git 工具链。\nAn Introduction to Earth and Environmental Data Science：比上一个更详细一些，推荐。\nUnidata Python Training：unidata 整的教程，包含 Python 基础和很多气象例子，推荐。\nATM 623: Climate Modeling：动手学习气候模式相关的知识。\nATSC 301: Atmospheric radiation and remote sensing: 大气辐射和遥感相关的课程讲义，附带需要使用 Python 的练习。\nHDF-EOS: COMPREHENSIVE EXAMPLES：处理 NASA HDF/HDF-EOS 卫星文件的例子集。\nProject Pyhtia：貌似是 NCAR 搞的在线 training 项目，目前很多教程内容都不全，但其提供的其它网站的 资源列表 非常齐全。\n气象绘图教程合集：云台书使在公众号上发布的系统性的气象绘图教程。\n摸鱼的气象：摸鱼咯在 B 站发布的手把手教学视频。\n气 Py：老李的系列教学视频，同时他还搬运了很多 MetPy Mondays 的视频。另有配套教材 气Py_Python气象数据处理与可视化。\n气象 Python 学习馆：深雨露的公众号，制作精良，讲解细致，唯一的缺点可能是数据结构都集中在 xarray 上。\n气象数据科学优质教程\u0026amp;项目集锦：和鲸社区汇总的多作者的气象教程合集。\nPython空间数据处理实战：GIS 相关的教程，含有栅格数据和 shapefile 文件的处理。\n","date":"2023-03-23","permalink":"https://zhajiman.github.io/post/python_resources/","tags":["python","资源"],"title":"Python 相关资源汇总（持续更新中）"},{"content":"前言 CALIPSO 卫星的 L2 VFM（Vertical Feature Mask）产品根据激光的后向散射和消光信息，将激光通过的各高度层分类为云或气溶胶。该产品在现实中的表现如下图所示：卫星一边在轨道上移动一边向地面发射激光脉冲，相当于在地面上缓缓拉开一幅“画卷”，VFM 描述了“画卷”上云和气溶胶的分布和分类情况。\n处理 VFM 产品的难点在于：\n VFM 数组呈 (N, 5515) 的形状，N 表示卫星移动时产生了 N 次观测，但 5515 并非表示有 5515 层高度，而是三种水平和垂直分辨率都不同的数据摊平成了长 5515 的数组。因此处理数据时需要参照文档的说明对 5515 进行变形。 文件中的经纬度和时间与 5515 的对应关系。时间数组需要解析成可用的格式。 每个 range bin 的分类结果编码到了 16 位的无符号短整型的每个比特上，需要按位解码。 网上现成的代码偏少。  网上能找到的代码有：\n CALIOPmatlab：以前 VFM 的在线文档里是给出过 MATLAB 和 IDL 的代码的，但现在链接消失了。这个仓库提供了民间改进后 MATLAB 代码。 HDF-EOS COMPREHENSIVE EXAMPLES：HDF-EOS 网站的示例，简单易理解。 MeteoInfo examples: CALIPSO data：基于 MeteoInfo 的代码，还有其它产品的例子。 Visualization of CALIPSO (VOCAL)：CALIPSO 官方基于 Python 2 的可视化工具。 星载激光雷达CALIPSO-VFM产品数据读取与显示：MATLAB 代码的讲解。  笔者也曾写过两次教程：\n NCL绘制CALIPSO L2 VFM图像：写得很烂，作图部分可能存在问题。 Python 绘制 CALIPSO L2 VFM 产品  本文是对旧教程的翻新，会对 VFM 数据的结构进行更多解释，对代码也进行了更新。本文使用 pyhdf 读取 HDF4 文件，用 Matplotlib 3.6.2 画图。为了方便画图，用了一些自制的函数（frykit）。虽然基于 Python，但希望能给使用其它语言的读者提供一点思路。\n完整代码已放入仓库 calipso-vfm-visualization。\n数据下载  EARTHDATA SEARCH EARTHDATA ASDC CALIPSO CALIPSO Search and Subsetting Web Application  廓线数据的水平和垂直分辨率 CALIPSO 532 nm 波段廓线的水平分辨率为 333 m，名义上垂直分辨率为 15 m。由于廓线原始数据体积过大，地面下载的网络带宽有限，原始数据在卫星上进行了平均处理：15 条连续的廓线构成一个 block，将 block 按高度分为 5 层，每层通过在水平方向和垂直方向上做平均的方式来降低分辨率。结果是对流层下层的廓线数据分辨率较高，高空的廓线数据分辨率较低，在不降低数据可用性的前提下大幅减小了文件体积。具体来说：\n -2.0 ~ -0.5 km：水平分辨率不变，垂直分辨率降为 300 m；block 内含 15 条廓线，高度层内含 5 个 range bin。 -0.5 ~ 8.5 km：水平分辨率不变，垂直分辨率降为 30 m；block 内含 15 条廓线，高度层内含 290 个 range bin。 8.5 ~ 20.1 km：水平分辨率降为 1000 m，垂直分辨率降为 60 m；block 内含 5 条廓线，高度层内含 200 个 range bin。 20.1 ~ 30.1 km：水平分辨率降为 1667 m，垂直分辨率降为 180 m；block 内含 3 条廓线，高度层内含 55 个 range bin。 30.1 ~ 40.0 km：水平分辨率降为 5000 m，垂直分辨率降为 300 m；block 内含 1 条廓线，高度层内含 33 个 range bin。  在线文档 的图片很好地展示了这一点：\n图左是平均处理后 block 里廓线的组成；图中间的表格是各高度层的分辨率参数；图右是说通过水平方向上的重采样就能将五个高度层的廓线数据处理成水平分辨率相同的形式，例如 20.1 ~ 30.1 km 的 3 条廓线每条重复 5 次，就能和底层的 15 条廓线对齐。\nVFM 数据的结构 VFM 文件直接用 Panoply 打开，下面列出主要变量：\n float Latitude(fakeDim0=4224, fakeDim1=1) float Longitude(fakeDim2=4224, fakeDim3=1) double Profile_UTC_Time(fakeDim4=4224, fakeDim5=1) ushort Feature_Classification_Flags(fakeDim14=4224, fakeDim15=5515)  其中 Feature_Classification_Flags 就是分类结果，4224 表示有 4224 个 block，5515 是将一个 block 的所有 range bin 摊平成一维的结果。如下图所示：\n相比上一节的 block 示意图，VFM 的 block 砍掉了 -2.0 ~ -0.5 km 和 30.1 ~ 40.0 km 这两层，只保留了中间三个高度层。因此每个 block 宽 5 km，高 -0.5 ~ 30.1 km。因为 block 里的廓线数据水平和垂直分辨率各不相同，所以无法简单用二维数组表示，只好将其按图中的序号展开成含 5515 个 range bin 的一维数组。例如获取 -0.5 ~ 8.2 km 内的 15 条廓线：\n# (N, 15, 290), 廓线高度随下标增大而增大. fcf1 = fcf[:, 1165:5515].reshape(-1, 15, 290)[:, :, ::-1]  那么如何获取 -0.5 ~ 30.1 km 完整高度的廓线呢？按上节提到的方法对 block 进行水平方向上的重采样，再将 5515 数组变形为 (15, 545) 的二维数组。545 表示共 545 个 range bin，但是存在三种不同的垂直分辨率。然后考虑到 VFM 文件的经纬度只有 N 个点，而非 N * 15 个点，我们对这 15 条廓线做水平平均，得到 N 条与经纬度相匹配的廓线。\n具体来说，Longitude、Latitude 和 Profile_UTC_Time 指的是卫星在地面扫过一个 5 km 宽的 block 的时间段里，时间中点对应的经纬度和 UTC 时间。即文件虽然可以 reshape 出 N * 15 条廓线，但只提供了 N 个 block 中点的经纬度坐标和时间戳。因此我们需要将 N * 15 条廓线处理成 N 条廓线，而水平平均便可以做到这一点。考虑到对数值离散的分类结果做平均可能不太合理，笔者的处理是每层只取第一条廓线，然后拼接成完整高度的一条廓线。即取序号为 1 ~ 55、166 ~ 365、1166 ~ 1455，图中“最左边一列”的 3 条廓线拼成一条廓线，再将经纬度和时间戳匹配给它。\nVFM 分类的解码 上一节将 Feature_Classification_Flags 处理成了形如 (N, 545) 的二维数组，数据类型为 ushort（即 16 位的无符号短整型）。为了节省存储空间，分类结果被编码到了 ushort 的每个比特上。以表示沙尘气溶胶的数值 46107 为例：\n46107 的比特表示是 1011010000011011，从右往左可以分为 7 个字段。例如前 3 个比特 011 代表 Feature Type，即大气类型，此处 011 对应十进制的 3，表示类型为气溶胶；第 10 ~ 12 比特 010 代表 Feature Sub-type，即更细分的类型，此处 010 对应十进制的 2，当 Feature Type 为气溶胶时，2 的 Sub-type 就表示沙尘气溶胶。每个字段的解读方法还请参考 官网表格。\n上面的分析采用的是 ushort -\u0026gt; 字符串 -\u0026gt; 子串 -\u0026gt; int 的处理办法，实际编程时采用位运算会更快捷，可以直接在 ndarray 上做向量运算。下面举一个提取 Feature Type 的例子：\n''' 等价于 1011010000011011 and 0000000000000111 -------------------- 0000000000000011 ''' 46107 \u0026amp; 7  \u0026amp; 运算符能让两个整型数在每个比特上取与，再将结果转为十进制。与 7 按位取与，相当于只保留 46107 最右边的 3 个比特，其它位设为零。这个操作可以类比 IP 地址掩码。\n如果要提取 Feature Sub-type，最方便的做法是先右移 9 位，再与 7 做按位取与：\n''' 右移9位, 左边补零. 1011010000011011 -\u0026gt; 0000000001011010 0000000001011010 and 0000000000000111 -------------------- 0000000000000010 ''' # \u0026gt;\u0026gt;优先级高于\u0026amp;. 46107 \u0026gt;\u0026gt; 9 \u0026amp; 7  右移后本来在 10 ~ 12 位的 3 个比特到了最右边，与 7 按位取与得到十进制的结果。\n代码实现 VfmReader 类能读取 L2 VFM 文件，以属性的形式提供云气溶胶分类、经纬度、高度和时间戳数组。这里简单认为每条廓线的高度都是相同的，从 -0.5 ~ 20.2 km，垂直分辨率由 30 m、60 m 过渡到 180 m。VOCAL 和 ccplot 的代码里似乎将廓线高度也视为一个二维数组，再通过垂直方向上的线性插值将廓线处理到等距的高度网格上。我看了一下没懂原理，所以这里还是用的简单的高度。\nimport numpy as np import pandas as pd from pyhdf.SD import SD, SDC class VfmReader: ''' 读取CALIPSO L2 VFM产品的类. Attributes ---------- lon : (nrec,) ndarray 激光足迹的经度. lat : (nrec,) ndarray 激光足迹的纬度. time : (nrec,) DatetimeIndex 激光足迹对应的UTC时间. height : (545,) ndarray 廓线每个bin对应的高度, 单位为km. 注意存在三种垂直分辨率. fcf : (nrec, 545, 7) ndarray 解码后的Feature_Classification_Flags. 7对应于文档中7个字段的值. ''' def __init__(self, filepath): self.sd = SD(str(filepath), SDC.READ) def close(self): '''关闭文件.''' self.sd.end() def __enter__(self): return self def __exit__(self, *args): self.close() @property def lon(self): return self.sd.select('Longitude')[:, 0] @property def lat(self): return self.sd.select('Latitude')[:, 0] @property def time(self): # 时间用浮点型的yymmdd.ffffffff表示. yymmddff = self.sd.select('Profile_UTC_Time')[:, 0] yymmdd = (yymmddff + 2e7).astype(int).astype(str) yymmdd = pd.to_datetime(yymmdd, format='%Y%m%d') ff = pd.to_timedelta(yymmddff % 1, unit='D') time = yymmdd + ff return time @property def height(self): height1 = (np.arange(290) + 0.5) * 0.03 - 0.5 height2 = (np.arange(200) + 0.5) * 0.06 + 8.2 height3 = (np.arange(55) + 0.5) * 0.18 + 20.2 height = np.concatenate([height1, height2, height3]) return height @property def fcf(self): # 三个高度层中都只选取第一条廓线来代表5km水平分辨率的FCF. fcf = self.sd.select('Feature_Classification_Flags')[:] fcf1 = fcf[:, 1165:1455] fcf2 = fcf[:, 165:365] fcf3 = fcf[:, 0:55] fcf = np.hstack([fcf3, fcf2, fcf1])[:, ::-1] # 利用位运算进行解码. shifts = [0, 3, 5, 7, 9, 12, 13] bits = [7, 3, 3, 3, 7, 1, 7] fcf = fcf[:, :, None] \u0026gt;\u0026gt; shifts \u0026amp; bits return fcf  画图例子 以 2021 年 3 月 15 日中国北方的沙尘暴天气为例，画出 Feature Type：\nfrom pathlib import Path import numpy as np import matplotlib.pyplot as plt import matplotlib.ticker as mticker plt.rcParams['font.family'] = 'Source Han Sans SC' import cartopy.crs as ccrs from vfm_reader import VfmReader from frykit.calc import region_ind import frykit.plot as fplt # 读取文件. dirpath = Path('../data') filepath = dirpath / 'CAL_LID_L2_VFM-Standard-V4-21.2021-03-15T19-18-09ZN.hdf' with VfmReader(filepath) as reader: lon = reader.lon lat = reader.lat time = reader.time height = reader.height fcf = reader.fcf # 用地图显示范围截取数据. extents = [100, 120, 30, 45] scan_mask = region_ind(lon, lat, extents) lon = lon[scan_mask] lat = lat[scan_mask] time = time[scan_mask] fcf = fcf[scan_mask] ftype = fcf[:, :, 0] # 构造cmap和norm. colors = [ 'white', 'lightcyan', 'skyblue', 'gold', 'red', 'seagreen', 'palegreen', 'black' ] ticklabels = [ 'invalid', 'clear air', 'cloud', 'aerosol', 'stratospheric\\nfeature', 'surface', 'subsurface', 'no signal' ] cmap, norm, ticks = fplt.make_qualitative_cmap(colors) # 构造截面图所需的x轴刻度. x, xticks, xticklabels = fplt.get_slice_xticks( lon, lat, ntick=5, decimals=1 ) crs = ccrs.PlateCarree() fig = plt.figure(figsize=(8, 6)) # 绘制地图. ax1 = fig.add_axes([0.1, 0.4, 0.8, 0.5], projection=crs) fplt.add_cn_province(ax1, lw=0.5) ax1.coastlines(resolution='10m', lw=0.5) fplt.set_extent_and_ticks( ax1, extents=extents, yticks=np.arange(-90, 91, 5), xticks=np.arange(-180, 181, 5), nx=1, ny=1 ) ax1.tick_params(labelsize='small') # 画出VFM在h0高度的水平分布. h0 = 4 ind = np.nonzero(height \u0026lt;= h0)[0][-1] ax1.plot(lon, lat, lw=4, c='gray', alpha=0.1, transform=crs) ax1.scatter( lon, lat, c=ftype[:, ind], s=0.2, cmap=cmap, norm=norm, transform=crs ) mean_time = time.mean().strftime('%Y-%m-%d %H:%M') ax1.set_title(mean_time, loc='left', fontsize='small') ax1.set_title(f'VFM at {h0} km', loc='right', fontsize='small') # 画出VFM的垂直剖面. ax2 = fplt.add_side_axes(ax1, loc='bottom', pad=0.06, depth=0.16) pc = ax2.pcolormesh(x, height, ftype.T, cmap=cmap, norm=norm, shading='nearest') ax2.axhline(h0, ls='--', c='r', lw=1, label=f'{h0} km') ax2.legend(loc='upper right', fontsize='x-small') # 设置ax2的坐标轴. ax2.set_xticks(xticks) ax2.set_xticklabels(xticklabels) if filepath.stem[-1] == 'N': ax2.invert_xaxis() ax2.set_ylim(0, 15) ax2.set_ylabel('Height (km)', fontsize='small') ax2.yaxis.set_major_locator(mticker.MultipleLocator(5)) ax2.yaxis.set_minor_locator(mticker.AutoMinorLocator(2)) ax2.tick_params(labelsize='small') # 设置colorbar. cax = fplt.add_side_axes([ax1, ax2], loc='right', pad=0.05, depth=0.02) cbar = fig.colorbar(pc, cax=cax) cbar.set_ticks(ticks) cbar.set_ticklabels(ticklabels) cbar.ax.tick_params(length=0, labelsize='x-small') plt.show()  如果要画气溶胶的 Sub-type，只需要在上面的代码中进行少量修改：\nftype = np.where(fcf[:, :, 0] == 3, fcf[:, :, 4], 0)  colors = [ 'white', 'blue', 'gold', 'red', 'green', 'brown', 'black', 'gray' ] ticklabels = [ 'not aerosol', 'clean\\nmarine', 'dust', 'polluted\\ncontinental', 'clean\\ncontinental', 'pullted\\ndust', 'smoke', 'other' ]  注意图中 'not aerosol' 的像元，实际上包含 Feature Type 不为气溶胶和 Sub-type 无法确定的两类像元。\n结语 很好，位运算学得很开心。\n","date":"2023-03-21","permalink":"https://zhajiman.github.io/post/calipso_vfm/","tags":["卫星","python","matplotlib"],"title":"CALIPSO L2 VFM 产品的读取和绘制（with Python）"},{"content":"前言 Matplotlib 中画折线图用 ax.plot(x, y)，当横坐标 x 是时间数组时，例如 datetime 或 np.datetime64 构成的列表，x 和 y 的组合即一条时间序列。Matplotlib 能直接画出时间序列，并自动设置刻度。下面以一条长三年的气温时间序列为例：\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt df = pd.read_csv('test.csv', index_col=0, parse_dates=True) series = df.loc['2012':'2014', 'T'] fig, ax = plt.subplots(figsize=(10, 4)) ax.plot(series.index, series) ax.set_ylabel('Temperature (℃)') print(ax.xaxis.get_major_locator()) print(ax.xaxis.get_major_formatter())  \u0026lt;matplotlib.dates.AutoDateLocator object at 0x000001AC6BF89A00\u0026gt; \u0026lt;matplotlib.dates.AutoDateFormatter object at 0x000001AC6BF89B20\u0026gt;  打印 x 轴的属性发现，Matplotlib 默认为时间序列设置了 AutoDateLocator 和 AutoDateFormatter，前者会自动根据 ax 的时间范围在 x 轴上选出位置、数量和间隔都比较合适的刻度，后者会自动根据主刻度的间隔，将刻度标签格式化为合适的样式。以上图为例，Matplotlib 自动选择了间隔 4 个月的刻度，刻度标签的字符串呈 YYYY-MM 的格式。\n虽然自动刻度很方便，但如果想像上图一样调整刻度间隔，追加小刻度，并修改刻度标签格式，就需要手动设置刻度。本文的目的就是介绍手动修改时间刻度的方法，内容主要分为三点：\n 了解 Matplotlib 处理时间的机制。 运用 matplotlib.dates 模块里提供的工具设置刻度。 解决 Pandas 时间序列图的问题。  本文基于 Matplotlib 3.6.2 和 Pandas 1.5.1。\nMatplotlib 处理时间的机制 matplotlib.dates（后简称 mdates）模块里有两个函数：date2num 和 num2date。前者能将一个 datetime 或 np.datetime64 对象转换成该对象离 1970-01-01T00:00:00 以来的天数（注意不是秒数），后者则是反过来转换。当 ax.plot 接受时间类型的 x 时，会在内部创建一个 mdates.DateConverter 对象，对 x 的每个元素调用 date2num，将其转换成表示天数的浮点型一维数组。Matplotlib 在内部便是以这种浮点数的形式存储时间的。下面验证一下这点：\nx0, x1 = ax.get_xlim() origin = '1970-01-01 00:00' t0 = pd.to_datetime(x0, unit='D', origin=origin) t1 = pd.to_datetime(x1, unit='D', origin=origin) print(x0, t0) print(x1, t1)  15285.200347222222 2011-11-07 04:48:30 16490.792708333334 2015-02-24 19:01:30  其中 pd.to_datetime 可以直接换成 num2date。所以后续在 ax 上画新线条时，使用时间类型或浮点类型的 x 都可以。\n此外，在脚本开头 import pandas 时，Pandas 会将一些额外的 Converter 注入到 Matplotlib 中，使之能够识别 pandas.Timestamp 和 pandas.DatetimeIndex 等类型的 x。\n使用 matplotlib.dates 提供的工具 除引言里提到的 AutoDateLocator 和 AutoDateFormatter 外，mdates 还提供其它规则的 Locator 和 Formatter。以设置月份刻度的 MonthLocator 为例：\ndates.MonthLocator(bymonth=None, bymonthday=1, interval=1, tz=None)  其中 bymonth 参数可以是表示月份的整数，或整数构成的列表，默认值是 1 - 12 月。MonthLocator 会在 ax 的 x 轴显示范围间生成一系列间隔为 interval 个月的 datetime 对象，它们的日由 bymonthday 指定，时分秒都为 0。从中挑选出月份跟 bymonth 匹配的对象，调用 date2num 函数作为最后的刻度值。因为内部实现用的是 dateutil.rrule.rrule，所以参数也是与之同名的。例如 MonthLocator() 的效果就是在每年每月 1 号 00:00:00 的位置设置一个刻度，那么一年就会有 12 个刻度。MonthLocator(bymonth=[1, 4, 7, 10]) 就是在每年 1、4、7 和 10 月设置刻度。\n除此之外 mdates 里还有 YearLocator、DayLocator、WeekDayLocator、HourLocator 等，原理和参数跟 MonthLocator 类似，就不多介绍了。\n接着以 DateFormatter 为例：\nclass matplotlib.dates.DateFormatter(fmt, tz=None, *, usetex=None)  原理非常简单，就是对刻度值 x 调用 num2date(x).strftime(fmt)，得到刻度标签。例如取 DateFormatter(fmt='%Y-%m')，就能让刻度标签呈 YYYY-MM 的格式。\n此外我们知道，如果直接向 ax.xaxis.get_major_formatter 传入一个参数为 x 和 pos 的函数，就相当于用这个函数构造了一个 FuncFormatter。所以可以简单自制一个只在每年 1 月标出年份的 Formatter：\ndef format_func(x, pos=None): x = mdates.num2date(x) if x.month == 1: fmt = '%m\\n%Y' else: fmt = '%m' label = x.strftime(fmt) return label  所以引言里的效果可以用下面的代码实现：\nimport matplotlib.dates as mdates ax.xaxis.set_major_locator(mdates.MonthLocator([1, 4, 7, 10])) ax.xaxis.set_minor_locator(mdates.MonthLocator()) ax.xaxis.set_major_formatter(format_func)  Pandas 时间序列图 Pandas 的 Series 和 DataFrame 对象自带 plot 方法，默认以 Matplotlib 为后端画图。以气温时间序列的第一年为例：\nsubset = series.loc['2012-01':'2012-12'] ax = subset.plot(figsize=(10, 4), xlabel='') print(ax.xaxis.get_major_locator()) print(ax.xaxis.get_major_formatter())  \u0026lt;pandas.plotting._matplotlib.converter.TimeSeries_DateLocator object at 0x000002639E7AD970\u0026gt; \u0026lt;pandas.plotting._matplotlib.converter.TimeSeries_DateFormatter object at 0x000002639E793CD0\u0026gt;  跟用 ax.plot 来画的一个区别是，Pandas 默认给 x 轴设置了自己实现的 TimeSeries_DateLocator 和 TimeSeries_DateFormatter。效果如上图所示，自动选取逐月刻度，以英文缩写标注月份，并且只在一月标注年份。但再仔细看，小刻度咋像乱标的。因此尝试修改 Locator 和 Formatter：\nax.xaxis.set_major_locator(mdates.MonthLocator()) ax.xaxis.set_minor_locator(mticker.NullLocator()) ax.xaxis.set_major_formatter(mdates.DateFormatter('%M'))  结果是……刻度全部消失了。检查一下浮点数范围：\nx0, x1 = ax.get_xlim() print(x0, mdates.num2date(x0)) print(x1, mdates.num2date(x1))  ValueError: Date ordinal 22089600.0 converts to 62449-04-09T00:00:00.000000 (using epoch 1970-01-01T00:00:00), but Matplotlib dates must be between year 0001 and 9999.  喜提 ValueError，说浮点数作为时间来说出界了。随后检查发现，x 轴坐标的单位是距 1970-01-01T00:00:00 的分钟数，无怪乎 mdates 里的 Locator 和 Formatter 都失效了。猜测原因是 Pandas 的 plot 虽然也会将时间转换成浮点数，但单位会根据时间的频率（即 freq）发生变化，所以 Pandas 也为其准备了特制的 Locator 和 Formatter。解决方法也很简单，如果你不满意 Pandas 自动刻度的效果，就直接用 ax.plot 来画，再使用 mdates 里的工具。具体代码见上一节。\n总结 Matplotlib 用天数的浮点数表示时间，方便内部数值计算。需要按逐月等规则设置刻度时，再在浮点数和时间对象之间来回转换。matplotlib.dates 中提供了定位和修饰时间刻度的工具，配合 Pandas 使用时可能会有冲突。\n参考资料 matplotlib.dates\ndateutil.rrule\nCustom tick formatter for time series\n","date":"2023-02-11","permalink":"https://zhajiman.github.io/post/matplotlib_time_tick/","tags":["matplotlib","时间序列"],"title":"Matplotlib 系列：手动设置时间序列折线图的刻度"},{"content":" Nicolas Vandeput 发布在 Towards Data Science 上的文章，同时也是其著作《Data Science for Supply Chain Forecasting》中的一章。\n 为预测任务挑选一个合适的指标并没有想象中那么简单，所以这次我们来研究一下 RMSE、MAE、MAPE 和 Bias 的优缺点。剧透：MAPE 是其中最差的，别用。\n衡量预测的准确度（或误差）并非易事，因为世上并不存在万能的指标。只有通过实验才能找出最适合你的关键性能指标（Key Performance Indicator, KPI）。后面我们将会看到，每个指标都各有优劣。\n首先我们要区分预测的精度和偏差：\n 偏差（Bias） 表示历史平均误差。大概指你的预测平均下来偏高（即高估了需求量）还是偏低（即低估了需求量）。这个量能告诉你误差的总体方向。 精度（Precision） 衡量预测值到真实值的分散程度。这个量能告诉你误差的量级，但不反映误差的总体方向。  正如下图所示，我们希望预测能够既精确又没有偏差。\n预测的 KPI Error 误差（error）定义为预测量减去需求量。注意当预测高估需求时误差为正值，当预测低估需求时误差为负值。\n$$ e_t = f_t - d_t $$\nBias 偏差（bias）定义为平均误差：\n$$ bias = \\frac{1}{n} \\sum e_t $$\n其中 $n$ 是既有需求值又有预测值的历史数据的数目。\n因为正误差项会抵消负误差项，所以可以有偏差特别小，但精度却很低的预测模型。显然仅凭误差并不足以评估预测的精度，但如果预测偏差特别大，说明模型肯定哪儿有问题。\nMAPE 平均绝对百分比误差（Mean Absolute Percentage Error, MAPE） 是衡量预测准确度最常用的 KPI 之一，每次预测的绝对误差除以对应的真实需求得到百分比误差，MAPE 即这些百分比误差的均值。\n$$ MAPE = \\frac{1}{n} \\sum \\frac{|e_t|}{d_t} $$\nMAPE 真的是非常奇怪的一个预测 KPI。虽然企业管理者们都对它很熟悉，但它其实是个很糟糕的准确度指标。MAPE 的公式里每个误差都单独除以了需求，所以会是偏斜（skewed）的：在需求较低的时段高误差会强烈影响 MAPE。因此优化 MAPE 会导致预测倾向于低估需求。所以，咱还是避开它为妙。\nMAE 平均绝对误差（Mean Absolute Error, MAE） 是一个很好的预测准确度 KPI。顾名思义，MAE 定义为绝对误差的平均值。\n$$ MAE = \\frac{1}{n} \\sum |e_t| $$\n该 KPI 的首要问题就是它的数值没有按平均需求的大小做调整。如果有人只告诉你某件商品的 MAE 是 10，你判断不了预测是好是坏。如果已知平均需求是 1000，那么预测效果非常好；但如果平均需求是 1，那么预测准确度就非常差。为了解决这个问题，通常将 MAE 除以平均需求换算成百分数：\n$$ MAE% = \\frac{\\frac{1}{n} \\sum |e_t|}{\\frac{1}{n} \\sum d_t} = \\frac{\\sum |e_t|}{\\sum d_t} $$\n混淆 MAPE/MAE：似乎不少从业者也管上面的 MAE 公式叫 MAPE，这会导致很多误解。所以我建议你在和别人讨论预测误差时直接展示误差是怎么计算的，以防鸡同鸭讲。\nRMSE 均方根误差（Root Mean Square Error, RMSE） 是一个稍微有点奇怪，但也很好用的 KPI，这一点我们后面再详细讨论。定义是对误差的平方求平均值后再求平方根。\n$$ RMSE = \\sqrt{\\frac{1}{n} \\sum e_t^2} $$\n类似 MAE，RMSE 同样没有按需求的大小做调整，因此这里定义 RMSE%：\n$$ RMSE% = \\frac{\\sqrt{\\frac{1}{n} \\sum e_t^2}}{\\frac{\\sum d_t}{n}} $$\n实际上很多算法（尤其是机器学习领域）都基于与之相关的均方误差（Mean Square Error, MSE）。\n$$ MSE = \\frac{1}{n} \\sum e_t^2 $$\n很多算法采用 MSE 是因为算起来比 RMSE 更快，操作起来更简单。但它的问题是误差平方后量纲与原始误差不同，导致我们无法将其与原始需求的大小联系起来。因此，我们在评估统计预测模型时不会用到 MSE。\n误差权重的问题 相比 MAE，RMSE 并没有平等对待所有误差，而是会给大误差以更高的权重。这意味着一个特别大的误差就足以使 RMSE 变得很差。下面以一条虚构的需求时间序列为例。\n假设我们想比较两组只有最后一个时段的预测值不同的预测：相比真实值，预测 #1 低估了 7 个单位，而预测 #2 仅低估了 6 个单位。\n这两组预测的 KPI 如下所示：\n有趣的是，减小最后一个时段一个单位的误差，使得总的 RMSE 降低了 6.9%（从 2.86 变为 2.66），而 MAE 仅降低了 3.6%（从 2.33 变为 2.25），即 MAE 受到的影响几乎只有 RMSE 的一半。显然，RMSE 强调那些最显著的误差，而 MAE 给予每个误差相同的重要性。你可以试试在预测最准确的那几个时段里减小误差，看看会对 MAE 和 RMSE 产生什么影响。\n剧透：RMSE 几乎不受影响。\n之后我们会看到，RMSE 还有些别的有趣的性质。\n你想预测什么？ 前面过了一遍这些 KPI（bias, MAPE, MAE, RMSE）的定义，但仍不清楚选择不同的 KPI 对我们的模型来说有多大差别。有人可能觉得用 RMSE 替代 MAE，或用 MAE 替代 MAPE 是无关紧要的，但事实并非如此。\n让我们用一个简单的例子来说明这一点。考虑一个每周需求量又低又平，但不时有大单子的产品（可能是促销或客户批量采购）。下面是目前为止观测到的每周需求量：\n接着假设我们对这一产品做了三种预测：第一种预测每天 2 件，第二种 4 件，第三种 6 件。下面画出观测到的需求量和我们做的预测：\n以 bias、MAPE、MAE 和 RMSE 为指标看看这些预测在这段时期的效果：\n只看 MAPE 的话第一种预测最好，只看 MAE 的话第二种预测最好。第三种预测在 RMSE 和 bias 上都是最优的（但在 MAE 和 MAPE 上是最差的）。现在揭晓这些预测是怎么做出来的：\n 预测 1 就是随便选了个很低的值。 预测 2 取的是需求的中值 4。 预测 3 是平均需求。  中值 vs. 平均值——数学优化 在进一步讨论其它预测 KPI 之前，让我们花些时间理解一下为什么用中值作为预测会导向不错的 MAE，而用平均值会导向不错的 RMSE。\n下面会有一点点数学，如果你对这些方程不熟悉的话也不必气馁，直接跳到关于 RMSE 和 MAE 结论的部分就行。\nRMSE 首先从 RMSE 开始：\n$$ RMSE = \\sqrt{\\frac{1}{n} \\sum e^2_t} $$\n为了简化后续的代数运算，这里采用简化版的均方误差（MSE）：\n$$ MSE = \\frac{1}{n} \\sum e^2_t $$\n如果你将 MSE 设为预测模型的优化目标，就需要最小化 MSE，即令其导数为零：\n$$ \\frac{\\partial{MSE}}{\\partial{f_t}} = \\frac{\\frac{1}{n} \\sum (f_t - d_t)^2}{\\partial{f_t}} $$\n$$ \\frac{2}{n} \\sum (f_t - d_t)= 0 $$\n$$ \\sum f_t = \\sum d_t $$\n 译注：这里设每一时刻的预测都相同，即 $f_t$ 是定值。\n 结论：为了优化预测的 MSE，模型需要以预测之和等于需求之和为目标，即优化 MSE 会产生平均水平上正确的预测，因此预测也是无偏差的。\nMAE 接着对 MAE 做同样的处理：\n$$ \\frac{\\partial{MAE}}{\\partial{f_t}} = \\frac{\\partial{\\frac{1}{n} \\sum |f_t - d_t|}}{\\partial{f_t}} $$\n因为\n$$ |f_t - d_t| = \\begin{cases} f_t - d_t, \u0026amp; d_t \u0026lt; f_t \\newline d_t - f_t, \u0026amp; d_t \\ge f_t \\end{cases} $$\n且\n$$ \\frac{\\partial{|f_t - d_t|}}{\\partial{f_t}} = \\begin{cases} 1, \u0026amp; d_t \u0026lt; f_t \\newline \\text{未定义}, \u0026amp; d_t = f_t \\newline -1, \u0026amp; d_t \u0026gt; f_t \\end{cases} $$\n所以\n$$ \\frac{\\partial{MAE}}{\\partial{f_t}} = \\frac{1}{n} \\sum \\begin{cases} 1, \u0026amp; d_t \u0026lt; f_t \\newline -1, \u0026amp; d_t \u0026gt; f_t \\end{cases} $$\n结论：要优化 MAE（使其导数为零），比预测值高的需求应该和比预测值低的需求一样多，换句话说就是我们要找一个能将数据集等分成两份的值，而这恰好就是中值的定义。\nMAPE 不幸的是 MAPE 的导数就没有如此优雅和直接的性质了。简单来说，MAPE 会将预测推向非常低的值，因为它给需求较低时的预测误差分配了更高的权重。\n结论 如上所示，对 RMSE 的优化会导向平均水平上的正确预测。与之相反的是，对 MAE 的优化试图一半时间高估需求，一半时间低估需求，这意味着以中值为优化目标。我们必须理解 MAE 和 RMSE 在数学源头上的巨大差异：前者的目标是中值，后者的目标是平均值。\nMAE 还是 RMSE——选哪个更好？ 以需求的中值为目标更坏，还是以平均值为目标更坏？其实答案并不是非黑即白的，后面的讨论里将会看到，每种方法都各有优劣，只有通过实验才能找出对当前数据集最合适的方法。你甚至可以同时使用 RMSE 和 MAE。\n让我们花时间讨论一下选 RMSE 或 MAE 会对 bias、离群值敏感度和间歇性需求的影响。\nBias 你会发现许多产品需求的中值和平均值都不相等，需求很可能是这儿那儿有几个峰，导致其概率分布是偏斜的。这种偏斜的需求分布在供应链中普遍存在，因为高峰可能是定期促销或客户大批采购造成的。这也导致需求的中值要比平均值小，如下图所示：\n 译注：图中的右偏分布一般满足众数 \u0026lt; 中值 \u0026lt; 平均值，但也存在反例。\n 这意味着优化 MAE 的预测会产生 bias，而优化 RMSE 的预测则是无偏的（因为目标就是平均值）。毫无疑问，这是 MAE 的主要缺点。\n离群值的敏感度 正如我们在前面讨论的，RMSE 会给最高的误差以更高的权重。这一性质的代价是：对离群值很敏感。让我们想象一个有着如下规律的商品。\n序列的中值是 8.5，平均值是 9.5。我们已经观察到，优化 MAE 会预测出中值（8.5），并且平均来看低估了 1 个单位（bias=-1）。你可能倾向于最小化 RMSE，预测出平均值（9.5）以避免有偏差的情况。然而，假设我们将最后一个需求值改为 100：\n中值仍为 8.5（没有变！），但平均值现在变成了 18.1。这种情况下你就很可能改用中值做预测。\n一般来说，中值在面对离群值时要比平均值更健壮（robust），这点在供应链环境中尤为关键，因为受编码错误或需求高峰（市场营销、促销、现货交易）等因素影响，我们会碰到很多离群值。\n那对离群值的健壮性一定是个好性质吗？答案是不。\n间歇性需求 不幸的是，对离群值的健壮性可能会给有间歇性需求的商品带来非常恼人的影响。\n假设我们向客户销售一样产品，其利润非常高，但我们唯一的客户只在三周里的某一周下订单，且时间上看不出任何规律。这位客户每次总是订购 100 件，因此我们每周需求量的平均值是 33 件，中值是……0。\n我们必须为该产品做每周的预测，假设我们的第一种预测方案是取平均需求（33 件），那么长期来看，总的平方误差是 6667（RMSE 是 47），总的绝对误差是 133（MAE 是 44）。\n改用中值（0）做预测的话，总的绝对误差是 100（MAE 是 33），总的平方误差是 10000（RMSE 是 58）。\n正如我们所看到的，对间歇性需求来说 MAE 是个很糟糕的 KPI。一旦你有超过一半的时刻没有需求，那么最优的预测是……0！\n结论 MAE 能防御异常值，而 RMSE 能确保我们的预测无偏差。那你应该用哪个指标呢？可惜并不存在明确的答案。作为一名供应链数据科学家，你应该动手做实验：如果使用 MAE 作为 KPI 会导致高 bias，你可能要换用 RMSE；如果数据集里有很多离群值，导致预测偏斜，那你可能要换用 MAE。\n值得注意的是，你可以选择一个或多个 KPI（通常是 MAE 和 bias）来汇报预测误差，但使用另一个 KPI（也许是 RMSE？）取优化模型。\n最后提一个关于低需求量商品的技巧：将需求聚合到更长的时间范围上。例如，如果每周的需求很低，你可以尝试做月度乃至季度的预测，之后再通过简单的除法将预测拆分到原先的时间范围上。这个技巧允许你用 MAE 作为 KPI，同时能平滑需求峰值。\n","date":"2022-11-10","permalink":"https://zhajiman.github.io/post/forecast_kpis/","tags":["时间序列","机器学习","翻译"],"title":"预测的 KPI：RMSE、MAE、MAPE 和 Bias"},{"content":"R 语言的管道 这回来介绍一下如何利用管道（pipe）风格将 Pandas 相关的代码写得更易读，不过首先让我们看看隔壁 R 语言中管道是怎么用的。假设输入是 x，经过连续四个函数的处理后得到输出 y，代码可以按顺序写：\nx1 \u0026lt;- func1(x, arg1) x2 \u0026lt;- func2(x1, arg2) x3 \u0026lt;- func3(x2, arg3) y \u0026lt;- func4(x3, arg4)  流程很清晰，但函数与函数之间会产生中间变量。这里为了方便取 x 加数字后缀形式的名字，日常编程时最好还是起个有意义点的名字，例如 x_after_func1 之类的。另一种简练的写法是：\ny \u0026lt;- func4(func3(func2(func1(x, arg1), arg2), arg3), arg4)  代码更短，也没有中间变量了，但代价是重看代码时需要像剥洋葱一样从两边向中间一层层读。并且当函数名更长参数更多时，可读性会进一步恶化，列数也很容易超出屏幕的宽度。\n这样看来似乎第一种风格更为妥当。不过，若是活用 magrittr 包里的管道符 %\u0026gt;% 的话，就能写出既清晰又简练的代码了。简单介绍一下 %\u0026gt;% 的功能：\n x %\u0026gt;% f 等价于 f(x)。 x %\u0026gt;% f(y) 等价于 f(x, y)。 x %\u0026gt;% f(y, .) 等价于 f(y, x)。 x %\u0026gt;% f(y, z = .) 等价于 f(y, z = x)。  即输入 x 通过管道 %\u0026gt;% 传给函数 f，f 里不用写 x，管道会自动把 x 作为 f 的第一个参数；如果 x 并非第一个参数，那么可以用占位符 . 代指 x。\n应用了管道符后的代码风格是：\ny \u0026lt;- x %\u0026gt;% func1(arg1) %\u0026gt;% func2(arg2) %\u0026gt;% func3(arg3) %\u0026gt;% func4(arg4)  格式整齐，代码顺序和操作顺序一致，语义清晰，没有多余的中间变量，强迫症患者感到十分舒适。这种写法的另一个好处是，增删函数就像增删空行一样简单，而前两种风格改起来就会十分烦人。\nPandas 中的管道 遗憾的是 Python 中并没有成熟的管道包，但有一种神似的写法：\nx = 'fried chicken\\n' y = x.rstrip().replace('fried', 'roast').upper().rjust(20) print(y)   ROAST CHICKEN  即对 x.rstrip() 方法返回的字符串调用 replace 方法，再对返回值调用 upper 方法，最后调用 rjust 方法，构成了方法链（method chaining）。这个写法看似简洁，实则局限很大：以一节节管道做比喻的话，R 中每节管子可以是任意函数，而 Python 中每节管子只能是输入管子的对象自带的方法。如果你想实现的操作不能用输入对象的方法达成，那么管道就连不起来，你还是得乖乖打断管道，在下一行调用函数或写表达式。\n但细分到用 Pandas 包做数据分析的领域，基于方法链的管道已经完全够用了：绝大部分操作都可以用 DataFrame 或 Series 的方法实现，并且方法返回的结果依旧是 DataFrame 或 Series 对象，保证可以接着调用方法；外部函数用 map、apply、applymap 或 pipe 方法应用到数据上。下面以处理站点气象数据表格为例：\n 查询指定站点。 丢弃站点列。 将时间列转为 DatetimeIndex。 按时间排序。 去除时间上重复的记录。 设置时间索引。 将 999999 替换成 NaN。 重采样到逐小时分辨率并插值填充。 加入风速分量列。  先来个普通风格：\ndef wswd_to_uv(ws, wd): '''风速风向转为uv分量.''' wd = np.deg2rad(270 - wd) u = ws * np.cos(wd) v = ws * np.sin(wd) return u, v station = 114514 df.query('station == @station', inplace=True) df.drop(columns='station', inplace=True) df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M') df.sort_values('time', inplace=True) df.drop_duplicates(subset='time', keep='last', inplace=True) df.set_index('time', inplace=True) df.mask(df \u0026gt;= 999999, inplace=True) df = df.resample('H').interpolate() df['u'], df['v'] = wswd_to_uv(df['ws'], df['wd'])  得益于很多方法自带原地修改的 inplace 参数，中间变量已经很少了。再来看看管道风格：\ndef set_time(df, fmt): return df.assign(time=pd.to_datetime(df['time'], format=fmt)) def add_uv(df): u, v = wswd_to_uv(df['ws'], df['wd']) return df.assign(u=u, v=v) dfa = (df .query('station == @station') .drop(columns='station') .pipe(set_time, fmt='%Y-%m-%d %H:%M') .sort_values('time') .drop_duplicates(subset='time', keep='last') .set_index('time') .mask(lambda x: x \u0026gt;= 999999) .resample('H').interpolate() .pipe(add_uv) )  个人感觉管道风格的格式更整齐，一眼就能看出每行的“动词”（方法）。去除了每行都有的 inplace 参数后，不仅视觉上更清爽，还保证了一套操作下来输入数据不会无缘无故遭到修改。接着再说说管道风格里的两个细节。\npipe 就是 Pandas 版的 %\u0026gt;%：\n df.pipe(func) 等价于 func(df)。 df.pipe(func, *args, **kwargs) 等价于 func(df, *args, **kwargs)。 df.pipe((func, 'arg2'), arg1=a) 等价于 func(arg1=a, arg2=df)。  可以将复杂的多行运算打包成形如 func(df, *args, **kwargs) 的函数，然后结合 pipe 使用。前文的 set_time 和 add_uv 函数就是例子。\nassign assign 方法的功能就是无副作用的列赋值：复制一份对象自己，在列尾添加新列或是修改已有的列，然后返回这份拷贝：\n# 相当于: # dfa = df.copy() # dfa['a'] = a # dfa['b'] = b dfa = df.assign(a=a, b=b) # 相当于: # df['a'] = a # df['b'] = b df.assign(a=a, b=b, inplace=True)  第一次看到 assign 时我只觉得多此一举，赋值不是用等号就可以吗？但后来我意识到它是搭配管道风格使用的：想要对管道内的中间变量做列赋值，同时不中断管道，就只能用 assign 方法。同时考虑到中间变量里的内容可能已经跟原始输入大不相同，assign 的参数还可以是以调用对象本身（即 self）为唯一参数的函数：\n# 省略号表示略去的方法. dfa = (df ... .assign(u=uwind, v=vwind) .assign(ws=lambda x: np.hypot(x['u'], x['v'])) ... )  这里不能写成 assign(ws=np.hypot(df['u'], df['v']))，因为 df 里本来是没有 u 和 v 的，但中间变量有，那么把匿名函数传给 assign 就可以解决这一问题。\n不只是 assign，where 和 mask 等方法，乃至 loc 和 iloc 索引器都能接受函数（准确来说是 callable 对象），方便在管道风格中使用。\n什么时候该用管道 管道并非优雅代码的万金油，而是有特定使用场景的：\n  输入经过一连串的操作得到一个输出的情况适合使用管道，输入和输出都很多时显然不太适合。\n  管道里的操作多于十个时会使 debug 变得很麻烦，因为缺少中间变量来定位 bug。建议当操作很多时适当分出中间变量，不要一个管道写到头。\n  方法链中对象的类型发生改变时建议将链条进行拆分，不然会令人迷惑。\n  参考链接 A Forward-Pipe Operator for R • magrittr\nR for Data Science: 18 Pipes\npandas 在使用时语法感觉很乱，有什么学习的技巧吗？\n","date":"2022-10-29","permalink":"https://zhajiman.github.io/post/pandas_pipe/","tags":["python","pandas"],"title":"Pandas 系列：管道风格"},{"content":"相信大伙对 NumPy 和 SciPy 里的插值比较熟：已知坐标值 xp 和变量值 fp，调用函数计算变量在目标坐标 x 上的数值。例如 np.interp 的 API 就是\nnp.interp(x, xp, fp)  Pandas 的 Series 和 DataFrame 对象也有插值方法 interpolate，默认做线性插值。但其功能与 NumPy 和 SciPy 不太一样。以一个序列对象 s 为例：\n# 缺测部分和有效部分. invalid = s.isna() valid = ~invalid # 对应于xp. s.index[valid] # 对应于fp. s.values[valid] # 对应于x. s.index # 两式大致等价. s.interpolate(method='index').values np.interp(s.index, s.index[valid], s.values[valid])  即 Pandas 的插值是要利用序列的有效值当 xp 和 fp，去填补缺测的部分。所以调用 s.interpolate 时我们不需要传入形如 x 的参数，而是应该在调用前就通过 s.reindex 之类的方法将 x 融合到 s 的索引中。这么说可能有点抽象，下面就以图像直观展示 Pandas 里插值的效果。本文不会涉及到具体的插值算法（最邻近、三次样条……），仅以线性插值为例。\n以数值为索引的序列 import numpy as np import pandas as pd index = pd.Index([1, 4], name='x') s = pd.Series(10 * index, index=index, name='y') target = np.arange(6)  作为例子的序列 s 只有两个值：10 和 40，对应的坐标是 1 和 4。现希望插值得到坐标 0 - 5 上的值，所以通过 reindex 方法将目标坐标融合到 s 的索引中，再调用 interpolate。过程如下图所示：\n图中绿色部分代表原始值和线性插值的结果，红色部分代表缺测或特殊的插值结果。可以看到 s.reindex 向序列中引入了浮点型的 NaN，所以 sa 的数据类型由整型偷偷转换成了浮点型。坐标 2 和 3 处的值由线性插值得到 20.0 和 30.0，这符合我们的预期；坐标 0 和 5 在 s 的坐标范围之外（即要做外插），sa.interpolate 的默认行为是保留序列开头的 NaN，用最后一个有效值去填充结尾处的 NaN，所以最后坐标 0 对应 NaN，5 对应 40.0。\n这个例子中目标坐标是等间距的，那如果不等间距会怎样？结果如下图所示：\n可以看到 sa.interpolate() 在 2.5 和 3.5 位置的结果是错误的。原因是 interpolate 有个指定插值方法的参数 method，默认值为 'linear'，会无视索引 x 的具体数值，认为 y 是等距排列的，进而插出错误的结果。如果你预先知道序列的每一行是等距排列的，那么可以放心调用无参数的 interpolate()，否则就需要指定 method 为 'index' 或 'values'，以 x 的数值作为目标坐标来做线性插值，得出 2.5 对应 25.0，3.5 得出 35.0。另外当 method 取 quadratic、cubic、spline 等高级方法时，自然会用上索引的数值。\n再考虑一种特殊的情况：目标坐标中不含 s 的坐标值，而是恰好穿插在其中。那么根据 reindex 的效果，s 原来的标签会被全部丢弃掉，得到一个全部缺测的序列 sa，于是 sa.interpolate 将不会有任何意义。笔者想到了三种办法来解决这一问题，其一便是用 NumPy 或 SciPy 正儿八经做插值计算，再老老实实地用得到的数组构造新序列，如下图所示：\n可以看到 sa.interpolate() 完全无效，而引入 NumPy 的线性插值后能得到预期结果。np.interp 的默认行为是用序列首尾的有效值填充外插的部分，所以图中有两处红色。\n方法二是用 xarray 代替 Pandas 做插值。虽然 xarray 是 Pandas 的亲戚，但 xarray 的插值方法 interp 反而与 NumPy 和 SciPy 接近，调用时需要给出目标坐标值。结果如下图所示：\n该方法中需要用 sa.to_xarray 将序列转为 DataArray，插值完后再用 to_series 变回序列。DataArray.interp 底层使用的是 scipy.interpolate.interp1d 函数，默认不会做外插，所以最后结果的首尾保留了 NaN。\n第三种方法只用 Pandas 自己的功能实现，但逻辑稍微麻烦些：s.index 与目标坐标求并集（会自动排序），然后进行 reindex，再调用 interpolate，最后从结果中索引出目标坐标的行。效果下图所示：\n结果嘛没什么可说的，开头保留了缺测，结尾的缺测直接前向填充。需要注意的地方是，这里 interpolate 不能取 method='linear'，否则会插出错误的数值。\n以时间为索引的序列 时间序列的插值基本同上一节的描述。一个小区别是，method 中 index 和 values 多了一个别名 time，效果是一样的。这节真正要讲的是 asfreq 和 resample 的插值。\n粗略来说，asfreq 的效果是以序列的起止时间为范围，生成一串等间距的时间戳（例如逐日、每小时、每分钟……），再以该时间戳做 reindex。所以将 asfreq 和 interpolate 方法串起来，可以轻松实现等间隔时刻的内插，如下图所示：\n图中通过 s.asfreq('D').interpolate() 便能实现逐日的线性插值。与之相对照的笨方法是：\ntarget = pd.date_range(s.index[0], s.index[-1], freq='D') s.reindex(target).interpolate()  在时间序列重采样相关的教程中可能会出现 s.resample('D').interpolate() 的用法。查看源代码会发现等价于 s.asfreq('D').interpolate()，依旧可以归纳为上图。\n结语 总结一下前面的结论：\n Pandas 中的 interpolate 的作用是通过插值填充缺测部分。 默认做无视索引数值的线性插值，可以通过 method 参数修改这一行为。 插值前需要用 reindex 之类的方法引入目标坐标。 可以用 xarray、NumPy 或 SciPy 做好插值后再导回 Pandas。 时间序列可以用 asfreq 或 resample 处理后再进行插值。  本文简单图解了 Pandas 插值的基本行为和使用场景，但考虑到 interpolate 方法的参数较为复杂，仍然可能有错漏的地方，还请读者批评指正。\n","date":"2022-10-28","permalink":"https://zhajiman.github.io/post/pandas_interpolate/","tags":["python","pandas"],"title":"Pandas 系列：图解插值"},{"content":"Python 的取模运算 r = m % n 相当于\n# 或q = math.floor(m / n) q = m // n r = m - q * n  即取模的结果是被除数减去地板除的商和除数的乘积，这一规则对正数、负数乃至浮点数皆适用。\n当 n 为正数时。显然任意实数 x 可以表示为 x = r + k * n，其中 0 \u0026lt;= r \u0026lt; n，k 是某个整数。那么有\nx // n = floor(r/n + k) = k x % n = x - x // n = r  即 x % n 的结果总是一个大小在 [0, n) 之间的实数 r。当 n = 10 时，以 x = 12 和 x = -12 为例：\n如果以 n 为一个周期，那么 x = 12 就相当于往右一个周期再走 2 格，x % n 会消去这个周期，剩下不满一个周期的 2；x = -12 相当于往左两个周期后再往右走 8 格，x % n 会消去这两个周期，剩下不满一个周期且为正数的 8。\n再本质点说，取模运算就是在 [0, 10) 的窗口内进行“衔尾蛇”移动：\n 12 向右超出窗口两格， 12 % 10 = 2，即右边出两格那就左边进两格。 -12 向左超出窗口 12 格，-12 % n = 8，即左边出 12 格那就右边进 12 格，发现还是超出左边两格，再从右边进两格，最后距离零点 8 格。  下面介绍取模运算的两个应用。\n地球的经度以本初子午线为起点，自西向东绕行一圈，经度的数值从 0° 增长到 360°。不过经度还可以大于 360°，表示绕行一圈以上，甚至还可以是负数，表示自东向西绕行。显然这跟取模运算的衔尾蛇特性完美契合，通过取模运算可以将 [0, 360) 范围外的经度变换回这个范围内：\nimport numpy as np lon = np.arange(-360, 720 + 1, 180) print(lon) print(lon % 360)  [-360, -180, 0, 180, 360, 540, 720] [ 0, 180, 0, 180, 0, 180, 0]  另外一个常用的经度范围是 [-180, 180)，即经度跨过太平洋上的对向子午线时经度会从正数跳变到负数。问题是如何将 [0, 360] 范围内的经度变换到 [-180, 180) 范围内。显然 [-180, 180) 是一个窗口，我们希望范围在 [180, 360] 的经度从窗口右边离开，再从窗口左边进入。但因为窗口范围不满足 [0, n) 的形式，所以不能直接取模，而是应该先向右偏移 180°，在正轴完成衔尾蛇移动后再偏移回负轴：\n(lon + 180) % 360 - 180  注意，这一算法中 180° 会被算到 -180°，360° 会被算到 0°：\nlon = lon = np.arange(0, 360 + 1, 180) print(lon) print((lon + 180) % 360 - 180)  [ 0, 180, 360] [ 0, -180, 0],  第二个应用是将月份换算成季节。气候学上春季指 3、4、5 月份，夏季指 6、7、8 月份，秋季指 9、10、11 月份，冬季指 12 月和来年 1、2 月。这里暂时不考虑冬季跨年的问题（可参考笔者的 Period 文章），只是将 [1, 12] 的月份映射到 [1, 4] 上，1、2、3、4 分别表示春夏秋冬。\n首先可以想到，地板除能将 12 个月等分为 4 组：\nmonth = np.arange(1, 13) print(month) print((month - 1) // 3 + 1)  [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] [ 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]  可惜春天是在 1、2、3 月的基础上向右偏移两个月；冬天是在 10、11、12 月的基础上向右偏移两个月，超出 12 月的部分从左边重新进入（即 1、2 月）。那么可以考虑通过取模把月份向左“旋转”两格，让春天排在前三格的位置，冬天排在最后三格的位置，这样就能应用地板除做分组了：\n(month - 3) % 12 // 3 + 1  当然，这两个问题都可以用更简单的方式来解决：经度可以用 np.where(lon \u0026gt; 180, 360 - lon, lon) 转换，季节可以用 if 判断或字典来做映射。但取模运算能将你的代码精简至一行，同时方便迷惑其它读者（大雾）。\n","date":"2022-10-24","permalink":"https://zhajiman.github.io/post/python_modulo/","tags":["python"],"title":"Python 系列：衔尾蛇一样的取模"},{"content":" 最近学习用 PyTorch 做时间序列预测，发现只有 TensorFlow 官网的教程 把时间窗口的选取和模型的设置讲得直观易懂，故改编如下。本人也只是入门水平，翻译错误之处还请指正。\n 本文是利用深度学习做时间序列预测的入门教程，用到的模型包括卷积神经网络（CNN）和循环神经网络（RNN）。全文分为两大部分，又可以细分为：\n 预测单个时间步：  预测一个特征。 预测所有特征。   预测多个时间步：  单发预测：模型跑一次输出所有时间步的结果。 自回归：每次输出一个时间步的预测，再把结果喂给模型得到下一步的预测。    本文用到的数据和 notebook 可以在 GitHub 仓库 找到。\n基本设置 import numpy as np import pandas as pd from scipy.fft import rfft import torch from torch import nn, optim from torch.utils.data import Dataset, DataLoader import matplotlib.pyplot as plt import seaborn as sns %config InlineBackend.figure_format = 'retina' plt.rcParams['figure.figsize'] = (8, 6) plt.rcParams['axes.grid'] = False  之后的代码均在 Jupyter Notebook 中运行。\n天气数据集 示例数据集采用马克斯普朗克生物地球化学研究所的 天气时间序列数据集，点击链接就能下（不过需要翻墙），在本地解压后得到 CSV 表格。该数据集包含 14 个特征，例如气温、气压和湿度等。时间范围从 2009 年到 2016 年，采样分辨率为 10 分钟。\n本教程只考虑逐小时的预测，因此这里通过跳步索引降采样得到逐小时的数据：\n# 从第5行开始, 每6条记录选中一条. df = pd.read_csv('jena_climate_2009_2016.csv')[5::6] df.index = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S') df.head()  画出其中一些特征的时间序列：\nplot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)'] plot_features = df[plot_cols] plot_features.plot(subplots=True) plot_features = df[plot_cols][:480] plot_features.plot(subplots=True)  查看并清理数据 接下来看看数据集的基本统计量：\ndf.describe().transpose()  风速 上表中最明显的就是风速 wv (m/s) 和最大风速 max. wv (m/s) 两个特征的最小值跑到了 -9999，意味着很可能有错。因为风速肯定是非负的，所以这里用零替换掉这些异常值：\ndf['wv (m/s)'] = df['wv (m/s)'].clip(lower=0) df['max. wv (m/s)'] = df['max. wv (m/s)'].clip(lower=0)  特征工程 深入学习如何建模之前，需要理解你的数据，并保证传入模型的数据有合理的格式。\n风 数据表格的最后一列是以角度为单位的风向。角度对模型来说并不是一个好的输入：0° 和 360° 在数值上差很多，但在几何上应该非常接近且可以平滑过渡。另外无风时（风速为零）风向不应该起作用。\n目前风向和风速的联合分布如下图所示：\nplt.hist2d(df['wd (deg)'], df['wv (m/s)'], bins=(50, 50), vmin=0, vmax=400) plt.colorbar() plt.xlabel('Wind Direction [deg]') plt.ylabel('Wind Velocity [m/s]')  如果将风向和风速转为风矢量的话，模型将更容易解读风数据：\nwv = df.pop('wv (m/s)') max_wv = df.pop('max. wv (m/s)') # 风向转为极坐标的风向, 单位转为弧度. wd_rad = np.deg2rad(270 - df.pop('wd (deg)')) # 计算风速的xy分量. df['Wx'] = wv * np.cos(wd_rad) df['Wy'] = wv * np.sin(wd_rad) # 计算最大风速的xy分量. df['max Wx'] = max_wv * np.cos(wd_rad) df['max Wy'] = max_wv * np.sin(wd_rad)  转换后风速分量的联合分布更易于被模型解读：\nplt.hist2d(df['Wx'], df['Wy'], bins=(50, 50), vmin=0, vmax=400) plt.colorbar() plt.xlabel('Wind X [m/s]') plt.ylabel('Wind Y [m/s]') plt.axis('tight')  时间 时间索引也非常有用，不过当然不是指字符串形式的。首先转换成秒：\ntimestamp_s = df.index.map(pd.Timestamp.timestamp)  跟风向的情况类似，用秒表示的时间对模型来说用处也不大。我们注意到天气数据明显存在以日和年为周期的周期性，你可以用余弦和正弦函数来表示这些周期信号：\nday = 24 * 60 * 60 year = 365.2425 * day df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day)) df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day)) df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year)) df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))  plt.plot(np.array(df['Day sin'])[:25]) plt.plot(np.array(df['Day cos'])[:25]) plt.xlabel('Time [h]') plt.title('Time of day signal')  这些值给模型提供了最重要的频率特征，不过这需要你提前确定哪些频率是重要的。如果你缺少这一信息，可以考虑使用快速傅里叶变换（FFT）来找出这些频率。为了验证前面提出的日和年周期的假设，下面对气温序列应用 scipy.fft.rfft。图中 1/year 和 1/day 频率处有显著的峰值：\nfft = rfft(df['T (degC)'].to_numpy()) f_per_dataset = np.arange(0, len(fft)) n_samples_h = len(df['T (degC)']) hours_per_year = 24 * 365.2524 years_per_dataset = n_samples_h / hours_per_year f_per_year = f_per_dataset / years_per_dataset plt.step(f_per_year, np.abs(fft)) plt.xscale('log') plt.ylim(0, 400000) plt.xlim([0.1, max(plt.xlim())]) plt.xticks([1, 365.2524], labels=['1/Year', '1/day']) _ = plt.xlabel('Frequency (log scale)')  数据划分 下面用 (70%, 20%, 10%) 的比例划分训练集、验证集和测试集。特别注意不要在划分前随机打乱数据，原因有两个：\n 保证后续可以将数据切成许多由连续样本构成的窗口。 保证用于评估的验证集和测试集是在模型训练完后收集的，使评估结果更符合实际情况。  n = len(df) i1 = int(n * 0.7) i2 = int(n * 0.9) train_df = df.iloc[:i1] val_df = df.iloc[i1:i2] test_df = df.iloc[i2:] num_features = df.shape[1]  标准化数据 在训练神经网络前最好对特征进行放缩，而标准化就是放缩的常用手法：为每个特征减去平均值再除以标准差。平均值和标准差只能在训练集上计算，以防模型接触到验证集和测试集。\n一个有争议的观点是：模型在训练阶段不应该接触训练集中的未来值，并且应该使用滑动平均来做标准化。本教程的重点并不在此，而且验证集和测试集的划分已经够你得出较为可信的预报评分了。所以方便起见，本教程只是简单做个平均。\ntrain_mean = train_df.mean() train_std = train_df.std() train_df = (train_df - train_mean) / train_std val_df = (val_df - train_mean) / train_std test_df = (test_df - train_mean) / train_std  现在我们来看一眼所有特征的分布。有些特征确实拖着长尾，但至少没有 -9999 的风速那种明显的错误。\ndf_std = (df - train_mean) / train_std df_std = df_std.melt(var_name='Column', value_name='Normalized') plt.figure(figsize=(12, 6)) ax = sns.violinplot(x='Column', y='Normalized', data=df_std) _ = ax.set_xticklabels(df.keys(), rotation=90)  数据分窗 本教程中的模型基于连续样本构成的窗口来做预测。这种输入窗口的主要特征是：\n 输入和标签窗口的宽度（即时间步数）。 输入和标签窗口间的时间偏移量。 哪些特征充当输入，哪些充当标签，哪些二者皆是。  本教程会构造一系列模型（包括线性回归、DNN、CNN 和 RNN 模型），用它们做两类预测：\n 单变量和多变量输出的预测。 单时间步和多时间步的预测。  本节重点介绍如何实现数据分窗，以便在后续的所有模型中复用。\n根据具体任务和模型类型的不同，你可能需要生成不同结构的数据窗口，下面列举几例：\n 例如，已知 24 小时的历史数据，预测 24 小时后那个时刻的天气，你可能会定义这样的窗口：  已知 6 小时的历史数据向后预测 1 小时，需要这样的窗口：  本节剩下的部分会定义一个 WindowGenerator 类，它可以：\n 处理上面那些图示中的下标索引和偏移。 将窗口中的特征划分为 (features, labels) 对。 画出窗口中的时间序列。 利用 PyTorch 的 Dataset 和 DataLoader，从训练集、验证集和测试集中生成批数据。  1. 下标索引和偏移 先从创建 WindowGenerator 类开始吧。__init__ 方法包含了处理输入和标签下标索引相关的所有逻辑。这里还需要训练集、验证集和测试集的 DataFrame，后续用来生成 torch.utils.data.DataLoader。\nclass WindowGenerator: def __init__( self, input_width, label_width, shift, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=None ): # 存储原始数据. self.train_df = train_df self.val_df = val_df self.test_df = test_df # 找出标签列的下标索引. self.columns = train_df.columns if label_columns is None: self.label_columns = self.columns else: self.label_columns = pd.Index(label_columns) self.label_column_indices = [ self.columns.get_loc(name) for name in self.label_columns ] # 计算窗口的参数. self.input_width = input_width self.label_width = label_width self.shift = shift self.total_window_size = input_width + shift self.input_slice = slice(input_width) self.input_indices = np.arange(input_width) self.label_start = self.total_window_size - label_width self.label_slice = slice(self.label_start, None) self.label_indices = np.arange(self.label_start, self.total_window_size) def __repr__(self): return '\\n'.join([ f'Total window size: {self.total_window_size}', f'Input indices: {self.input_indices}', f'Label indices: {self.label_indices}', f'Label column names(s): {self.label_columns.to_list()}' ])  下面的代码构造了本节开头图示中的两种窗口：\nw1 = WindowGenerator( input_width=24, label_width=1, shift=24, label_columns=['T (degC)'] ) w1  Total window size: 48 Input indices: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] Label indices: [47] Label column names(s): ['T (degC)']  w2 = WindowGenerator( input_width=6, label_width=1, shift=1, label_columns=['T (degC)'] ) w2  Total window size: 7 Input indices: [0 1 2 3 4 5] Label indices: [6] Label column names(s): ['T (degC)']  2. 划分 split_window 方法可以将一串连续的输入值转为由输入值组成的一个窗口，和由标签值组成的一个窗口。前面定义的 w2 会被分割成这个样子：\n虽然上图并没有展示数据中 features 所在的维度，但 split_window 方法是可以正确处理 label_columns 的，所以既能用在单变量输出，也能用在多变量输出的例子里。\ndef split_window(self, features): inputs = features[self.input_slice, :] labels = features[self.label_slice, self.label_column_indices] return inputs, labels WindowGenerator.split_window = split_window  下面来试试：\nexample_window = train_df.iloc[:w2.total_window_size].to_numpy() example_inputs, example_labels = w2.split_window(example_window) print('All shapes are: (time, features)') print(f'Window shape: {example_window.shape}') print(f'Inputs shape: {example_inputs.shape}') print(f'Labels shape: {example_labels.shape}')  All shapes are: (time, features) Window shape: (7, 19) Inputs shape: (6, 19) Labels shape: (1, 1)  Pytorch 中数组的形状通常表现为：最外层的下标索引对应批大小的维度，中间的下标索引对应时间或空间（宽高）维，而最内层的下标索引对应每种特征。\n上面代码的功能是，将宽 7 个时间步，每步含 19 个特征的窗口划分为宽 6 个时间步，含 19 个特征的输入窗口，和 1 个时间步宽，只含 1 个特征的标签窗口。因为 w2 在初始化时指定了 label_columns=['T (degC)']，所以标签窗口只含一个特征。本教程在起步阶段还是先搭一些预测单变量的模型。\n3. 生成 DataLoader 先自定义一个 TimeseriesDataset，接收数组 data 并将其转为张量，以 window 作为窗口宽度。假设 data 形如 (time, features)，那么 dataset[0] 对应于 data[0:window]，dataset[1] 对应 data[1:window+1]，以此类推直至 data[time-window:time]，即 dataset 中共计 time - window + 1 个窗口。然后将 split_window 作为 transform 参数传入，将每个窗口切成 (input_window, label_window) 对，最后将 dataset 传给 DataLoader，在窗口的第一维堆叠出批维度。\nclass TimeseriesDataset(Dataset): def __init__(self, data, window, transform=None): self.data = torch.tensor(data, dtype=torch.float) self.window = window self.transform = transform def __len__(self): return len(self.data) - self.window + 1 def __getitem__(self, index): if index \u0026lt; 0: index += len(self) features = self.data[index:index+self.window] if self.transform is not None: features = self.transform(features) return features def make_dataloader(self, df): data = df.to_numpy() dataset = TimeseriesDataset( data=data, window=self.total_window_size, transform=self.split_window ) dataloader = DataLoader( dataset=dataset, batch_size=32, shuffle=True ) return dataloader WindowGenerator.make_dataloader = make_dataloader  再定义一些类属性，能直接以 DataLoader 的形式获取 WindowGenerator 对象里存着的训练集、验证集和测试集的数据。为了方便测试和画图还加了个 example 属性，返回切好的一批窗口：\n@property def train(self): return self.make_dataloader(self.train_df) @property def val(self): return self.make_dataloader(self.val_df) @property def test(self): return self.make_dataloader(self.test_df) @property def example(self): '''获取并缓存一个批次的(inputs, labels)窗口.''' result = getattr(self, '_example', None) if result is None: result = next(iter(self.train)) self._example = result return result WindowGenerator.train = train WindowGenerator.val = val WindowGenerator.test = test WindowGenerator.example = example  现在你能用 WindowGenerator 对象获取 DataLoader 并轻松迭代整个数据集了。让我们看看迭代 DataLoader 时元素的形状：\nexample_inputs, example_labels = w2.example print(f'Inputs shape (batch, time, features): {example_inputs.shape}') print(f'Labels shape (batch, time, features): {example_labels.shape}')  Inputs shape (batch, time, features): torch.Size([32, 6, 19]) Labels shape (batch, time, features): torch.Size([32, 1, 1])  4. 画图 为了简单展示一下分出来的窗口，这里定义画图方法：\ndef tensor_to_numpy(tensor): '''张量转为NumPy数组.''' if tensor.requires_grad: tensor = tensor.detach() if tensor.device.type == 'cuda': tensor = tensor.cpu() return tensor.numpy() def plot(self, model=None, plot_col='T (degC)', max_subplots=3): # 从缓存的一批窗口中获取输入和标签. inputs, labels = self.example if model is not None: model.eval() with torch.no_grad(): predictions = tensor_to_numpy(model(inputs)) inputs = tensor_to_numpy(inputs) labels = tensor_to_numpy(labels) plt.figure(figsize=(12, 8)) plot_col_index = self.columns.get_loc(plot_col) max_n = min(max_subplots, len(inputs)) # 子图数量不超过max_subplots和批大小. for n in range(max_n): plt.subplot(max_n, 1, n + 1) plt.ylabel(f'{plot_col} [normed]') plt.plot( self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=1 ) # 标签窗口里没有plot_col时则跳过. try: label_col_index = self.label_columns.get_loc(plot_col) except KeyError: continue plt.scatter( self.label_indices, labels[n, :, label_col_index], c='#2ca02c', edgecolors='k', label='Labels' ) # 画出预测值. if model is not None: plt.scatter( self.label_indices, predictions[n, :, label_col_index], c='#ff7f0e', s=64, marker='X', edgecolors='k', label='Predictions' ) if n == 0: plt.legend() plt.xlabel('Time [h]') WindowGenerator.plot = plot  该方法会按时间对齐输入序列、标签序列和（之后产生的）预测序列：\nw2.plot()  默认画气温，也可以画其它特征，但作为例子的 w2 窗口中只有气温这一个标签特征。\nw2.plot(plot_col='p (mbar)')  总结 前面 WindowGenerator 类的定义分布得比较零散，为了方便使用，这里把定义总结在一起：\nclass WindowGenerator: def __init__( self, input_width, label_width, shift, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=None ): # 存储原始数据. self.train_df = train_df self.val_df = val_df self.test_df = test_df # 找出标签列的下标索引. self.columns = train_df.columns if label_columns is None: self.label_columns = self.columns else: self.label_columns = pd.Index(label_columns) self.label_column_indices = [ self.columns.get_loc(name) for name in self.label_columns ] # 计算窗口的参数. self.input_width = input_width self.label_width = label_width self.shift = shift self.total_window_size = input_width + shift self.input_slice = slice(input_width) self.input_indices = np.arange(input_width) self.label_start = self.total_window_size - label_width self.label_slice = slice(self.label_start, None) self.label_indices = np.arange(self.label_start, self.total_window_size) def __repr__(self): return '\\n'.join([ f'Total window size: {self.total_window_size}', f'Input indices: {self.input_indices}', f'Label indices: {self.label_indices}', f'Label column names(s): {self.label_columns.to_list()}' ]) def split_window(self, features): inputs = features[self.input_slice, :] labels = features[self.label_slice, self.label_column_indices] return inputs, labels def make_dataloader(self, df): data = df.to_numpy() dataset = TimeseriesDataset( data=data, window=self.total_window_size, transform=self.split_window ) dataloader = DataLoader( dataset=dataset, batch_size=32, shuffle=True ) return dataloader @property def train(self): return self.make_dataloader(self.train_df) @property def val(self): return self.make_dataloader(self.val_df) @property def test(self): return self.make_dataloader(self.test_df) @property def example(self): '''获取并缓存一个批次的(inputs, labels)窗口.''' result = getattr(self, '_example', None) if result is None: result = next(iter(self.train)) self._example = result return result def plot(self, model=None, plot_col='T (degC)', max_subplots=3): # 从缓存的一批窗口中获取输入和标签. inputs, labels = self.example if model is not None: model.eval() with torch.no_grad(): predictions = tensor_to_numpy(model(inputs)) inputs = tensor_to_numpy(inputs) labels = tensor_to_numpy(labels) plt.figure(figsize=(12, 8)) plot_col_index = self.columns.get_loc(plot_col) max_n = min(max_subplots, len(inputs)) # 子图数量不超过max_subplots和批大小. for n in range(max_n): plt.subplot(max_n, 1, n + 1) plt.ylabel(f'{plot_col} [normed]') plt.plot( self.input_indices, inputs[n, :, plot_col_index], label='Inputs', marker='.', zorder=1 ) # 标签窗口里没有plot_col时则跳过. try: label_col_index = self.label_columns.get_loc(plot_col) except KeyError: continue plt.scatter( self.label_indices, labels[n, :, label_col_index], c='#2ca02c', edgecolors='k', label='Labels' ) # 画出预测值. if model is not None: plt.scatter( self.label_indices, predictions[n, :, label_col_index], c='#ff7f0e', s=64, marker='X', edgecolors='k', label='Predictions' ) if n == 0: plt.legend() plt.xlabel('Time [h]')  单步模型 基于这种数据，最简单的模型就是利用当前时间步的信息预测下个时间步（一小时后）的一个特征值。所以我们先来搭个预测下小时 T (degC) 的模型。\n设置一个 WindowGenerator 对象，构造如图所示的单步的 (input, label) 对：\nsingle_step_window = WindowGenerator( input_width=1, label_width=1, shift=1, label_columns=['T (degC)'] ) single_step_window  Total window size: 2 Input indices: [0] Label indices: [1] Label column names(s): ['T (degC)']  这个 window 对象能用训练集、验证集和测试集的数据创建 DataLoader 对象，方便你在不同批次的数据上进行迭代：\nexample_inputs, example_labels = single_step_window.example print(f'Inputs shape (batch, time, features): {example_inputs.shape}') print(f'Labels shape (batch, time, features): {example_labels.shape}')  Inputs shape (batch, time, features): torch.Size([32, 1, 19]) Labels shape (batch, time, features): torch.Size([32, 1, 1])  模型类 TensorFlow 的 tf.keras.Model 类可以通过 fit 方法一键训练，通过 evaluate 方法在测试集上评估模型的表现。而 PyTorch 的 torch.nn.Module 类只提供计算前向传播的功能，在数据集上进行训练和评估的功能需要手工实现。这里仿照 Keras 定义一个 Model 类，实现训练和评估相关的方法：\nclass Model(nn.Module): def compile(self, loss_fn, metric_fn, optimizer=None): self.loss_fn = loss_fn self.metric_fn = metric_fn self.optimizer = optimizer def train_epoch(self, dataloader): self.train() avg_loss = 0 avg_metric = 0 for x, y in dataloader: yp = self(x) loss = self.loss_fn(y, yp) self.optimizer.zero_grad() loss.backward() self.optimizer.step() avg_loss += loss.item() avg_metric += self.metric_fn(y, yp).item() num_batches = len(dataloader) avg_loss /= num_batches avg_metric /= num_batches return avg_loss, avg_metric @torch.no_grad() def evaluate(self, dataloader): self.eval() avg_loss = 0 avg_metric = 0 for x, y in dataloader: yp = self(x) avg_loss += self.loss_fn(y, yp).item() avg_metric += self.metric_fn(y, yp).item() num_batchs = len(dataloader) avg_loss /= num_batchs avg_metric /= num_batchs return avg_loss, avg_metric  本教程后续还会用早停法提前终止训练，这里也实现一个。原理是 EarlyStopping 类会记录训练过程中出现过的最小损失 min_loss，当传入的 loss 连续 patience 次超过 min_loss + min_delta 时，认为后续 loss 只会不断增长，该停止训练了。\nclass EarlyStopping: def __init__(self, min_delta=0, patience=1): self.min_delta = min_delta self.patience = patience self.min_loss = np.inf self.counter = 0 def __call__(self, loss): if loss \u0026lt; self.min_loss: self.min_loss = loss self.counter = 0 if loss \u0026gt; (self.min_loss + self.min_delta): self.counter += 1 return self.counter \u0026gt;= self.patience  基准模型 在构造一个可训练的模型之前，最好先整一个基准模型作为对照组，稍后还可以跟更复杂的模型比较预测表现。\n我们的第一个任务是根据当前所有特征的数值预测一小时后的气温。注意当前所有特征包含当前气温。因此，可以让模型直接返回当前气温作为预测值，即预言气温不会变。考虑到现实中逐小时的气温变化并不算大，这一基准模型还是比较合理的。当然，如果你要预测更远的未来的话，这个基准模型就很不靠谱了。\nclass Baseline(Model): def __init__(self, label_index=None): super(Baseline, self).__init__() self.label_index = label_index def forward(self, inputs): if self.label_index is None: return inputs else: return inputs[:, :, [self.label_index]]  实例化对象并直接在验证集和测试集上评估预测表现：\nbaseline = Baseline(label_index=df.columns.get_loc('T (degC)')) baseline.compile(loss_fn=nn.MSELoss(), metric_fn=nn.L1Loss()) val_performance = {} test_performance = {} loss, metric = baseline.evaluate(single_step_window.val) print(f'loss: {loss:.4f} - metric: {metric:.4f}') val_performance['Baseline'] = baseline.evaluate(single_step_window.val) test_performance['Baseline'] = baseline.evaluate(single_step_window.test)  loss: 0.0128 - metric: 0.0784  其中评分 metric 使用的是 nn.L1Loss，即平均绝对误差（MAE）。虽然这几行代码把一些评分打印在了屏幕上，但很难让我们对模型的好坏有直观的认识。WindowGenerator 有画出输入、标签和预测结果的方法，但其输入和输出的时间步都只有一步，恐怕画不出什么有意思的图像。\n为了方便演示，这里创建一个更宽的 WindowGenerator 对象，每次能切出连续 24 小时的输入和标签序列。虽然结构与 single_step_window 不同，但 wide_window 并没有改变模型的预测方式，模型依旧用一个小时的输入预测下一小时的气温。这种情况下 time 维就好比 batch 维：不同时间步的预测都是独立产生的，互不干涉。\nwide_window = WindowGenerator( input_width=24, label_width=24, shift=1, label_columns=['T (degC)'] ) wide_window  Total window size: 25 Input indices: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] Label indices: [ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24] Label column names(s): ['T (degC)']  扩展后的窗口能直接传给 baseline 模型做预测，不需要修改任何代码。这是因为初始化窗口对象时指定了输入和标签序列的时间步数相同，并且基准模型只是从输入中抽取特定几列特征作为输出，没有什么复杂的前向过程：\nprint('Inputs shape:', wide_window.example[0].shape) print('Output shape:', baseline(wide_window.example[0]).shape)  Inputs shape: torch.Size([32, 24, 19]) Output shape: torch.Size([32, 24, 1])  然后来画基准模型的预测结果，发现预测结果就是输入序列右移一个小时而已：\nwide_window.plot(baseline)  关于上图的一些解释：\n 蓝色的 Inputs 折线表示每个时间步上输入的气温。注意模型是输入了所有特征的，但这里只画出了气温而已。 绿色的 Labels 散点表示目标时间步上的真实气温。这些点显示在预测时刻而非输入时刻，这也是 Labels 的时间范围要比 Inputs 的范围向右偏移一步的原因。 橙色的 Predictions 散点是模型在输出时间步上的预测值。如果 Predictions 的叉叉和 Labels 的圆点重合，说明模型完美预测了气温。  线性模型 对于单步预测的任务，最简单的可训练模型就是在输入和输出之间插入线性变换层。此时输出完全由当前时间步的输入决定：\n一层 torch.nn.Linear就是一个线性模型，只会对输入的最后一维进行变换，将形如 (batch, time, in_features) 的数据变成 (batch, time, out_features) 的形状。且不同的 batch 和 time 下标都对应一个线性变换，这些变换之间互相独立：\nclass Linear(Model): def __init__(self, in_features, out_features): super(Linear, self).__init__() self.layer = nn.Linear(in_features, out_features) def forward(self, inputs): return self.layer(inputs)  linear = nn.Linear(num_features, 1) print('Input shape:', single_step_window.example[0].shape) print('Output shape:', linear(single_step_window.example[0]).shape)  Input shape: torch.Size([32, 1, 19]) Output shape: torch.Size([32, 1, 1])  本教程将会训练很多模型，因此把训练流程打包成一个函数：\ndef compile_and_fit(model, window, max_epochs=20, patience=2): model.compile( loss_fn=nn.MSELoss(), metric_fn=nn.L1Loss(), optimizer=optim.Adam(model.parameters()) ) early_stopping = EarlyStopping(patience=patience) for t in range(max_epochs): loss, metric = model.train_epoch(window.train) val_loss, val_metric = model.evaluate(window.val) info = ' - '.join([ f'[Epoch {t + 1}/{max_epochs}]', f'loss: {loss:.4f}', f'metric: {metric:.4f}', f'val_loss: {val_loss:.4f}', f'val_metric: {val_metric:.4f}' ]) print(info) if early_stopping(val_loss): break  训练线性模型并评估其表现：\ncompile_and_fit(linear, single_step_window) val_performance['Linear'] = linear.evaluate(single_step_window.val) test_performance['Linear'] = linear.evaluate(single_step_window.test)  [Epoch 1/20] - loss: 0.1688 - metric: 0.2229 - val_loss: 0.0176 - val_metric: 0.1021 [Epoch 2/20] - loss: 0.0151 - metric: 0.0901 - val_loss: 0.0098 - val_metric: 0.0734 [Epoch 3/20] - loss: 0.0096 - metric: 0.0718 - val_loss: 0.0089 - val_metric: 0.0701 [Epoch 4/20] - loss: 0.0092 - metric: 0.0704 - val_loss: 0.0089 - val_metric: 0.0703 [Epoch 5/20] - loss: 0.0092 - metric: 0.0704 - val_loss: 0.0088 - val_metric: 0.0690 [Epoch 6/20] - loss: 0.0092 - metric: 0.0702 - val_loss: 0.0090 - val_metric: 0.0711 [Epoch 7/20] - loss: 0.0091 - metric: 0.0700 - val_loss: 0.0088 - val_metric: 0.0696  跟 baseline 模型类似，linear 模型也可以直接用在宽窗口产生的批量数据上，这种用法能让模型在一串连续的时间步上给出一组互相独立的预测。此时 time 维的功能跟 batch 维类似。每个时间步上的预测互不影响。\nprint('Input shape:', wide_window.example[0].shape) print('Output shape:', linear(wide_window.example[0]).shape)  Input shape: torch.Size([32, 24, 19]) Output shape: torch.Size([32, 24, 1])  注意 wide_window 只是方便一次性预测多个时间步和画图，训练和评估还得用 single_step_window，不然相当于批大小从 batch_size 增大为 batch_size * input_width。\n下面画出几例 linear 在 wide_window 上的预测结果，可见大部分时刻线性模型的效果比直接用输入时刻的气温当预测更好，但在有些时刻要更差些：\nwide_window.plot(linear)  线性模型的一大好处就是易于解读，因为线性层的权重就是多变量线性回归里的系数。你可以对所有输入特征对应的权重做可视化：\nx = np.arange(num_features) _, ax = plt.subplots() ax.bar(x, tensor_to_numpy(linear.layer.weight[0, :])) ax.set_xticks(x) ax.set_xticklabels(train_df.columns, rotation=90)  有时训练出来的模型里 T (degC) 的权重都不是最高的（例如本图就是……），这算是随机初始化权重所带来的一个毛病。\n密集层模型 在我们尝试多时间步输入的模型之前，有必要先来测试一下更深更强力的单时间步输入模型。\n下面这个模型跟 linear 很像，不过在输入和输出之间多加了两层线性层和激活函数：\nclass Dense(Model): def __init__(self, in_features, out_features): super(Dense, self).__init__() self.layers = nn.Sequential( nn.Linear(in_features, 64), nn.ReLU(), nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, out_features) ) def forward(self, x): return self.layers(x) dense = Dense(num_features, 1) compile_and_fit(dense, single_step_window) val_performance['Dense'] = dense.evaluate(single_step_window.val) test_performance['Dense'] = dense.evaluate(single_step_window.test)  [Epoch 1/20] - loss: 0.0189 - metric: 0.0783 - val_loss: 0.0074 - val_metric: 0.0624 [Epoch 2/20] - loss: 0.0076 - metric: 0.0625 - val_loss: 0.0074 - val_metric: 0.0636 [Epoch 3/20] - loss: 0.0072 - metric: 0.0606 - val_loss: 0.0075 - val_metric: 0.0615  多步密集层模型 单时间步的模型无法获知输入在时间维度上的“上下文信息”，即模型看不到输入特征随时间的变化情况。为此模型在做预测时应该获取多个时间步的输入：\nbaseline、linear 和 dense 模型都是单独处理每个时间步的输入，而这里要介绍的模型将会一次性接收多个时间步的输入并输出单个时间步的标签。创建窗口时注意 shift 参数指的是输入输出窗口末尾间的偏移量：\nCONV_WIDTH = 3 conv_window = WindowGenerator( input_width=CONV_WIDTH, label_width=1, shift=1, label_columns=['T (degC)'] ) conv_window  Total window size: 4 Input indices: [0 1 2] Label indices: [3] Label column names(s): ['T (degC)']  conv_window.plot() ax = plt.gcf().axes[0] ax.set_title('Given 3 hours of inputs, predict 1 hour into the future.')  要实现这种模型，可以在 dense 模型前加一层 nn.Flatten，将形如 (batch, time, features) 的输入摊平成 (batch, time * features)，此处 time=3，这样一来前三个时间步的特征都会输进模型。网络最后一层再通过 torch.Tensor.reshape 将 (batch, 1 * 1) 的输出转为 (batch, 1, 1) 。\nclass nnReshape(nn.Module): def __init__(self, *shape): super(nnReshape, self).__init__() self.shape = shape def forward(self, x): return x.reshape(self.shape) class MultiStepDense(Model): def __init__(self, in_features, out_features): super(MultiStepDense, self).__init__() self.layers = nn.Sequential( nn.Flatten(), nn.Linear(CONV_WIDTH * in_features, 32), nn.ReLU(), nn.Linear(32, 32), nn.ReLU(), nn.Linear(32, out_features), nnReshape(-1, 1, out_features) ) def forward(self, x): return self.layers(x)  multi_step_dense = MultiStepDense(num_features, 1) print('Input shape:', conv_window.example[0].shape) print('Output shape:', multi_step_dense(conv_window.example[0]).shape)  Input shape: torch.Size([32, 3, 19]) Output shape: torch.Size([32, 1, 1])  compile_and_fit(multi_step_dense, conv_window) val_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.val) test_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.test)  [Epoch 1/20] - loss: 0.0270 - metric: 0.0899 - val_loss: 0.0070 - val_metric: 0.0598 [Epoch 2/20] - loss: 0.0073 - metric: 0.0609 - val_loss: 0.0066 - val_metric: 0.0583 [Epoch 3/20] - loss: 0.0070 - metric: 0.0596 - val_loss: 0.0062 - val_metric: 0.0552 [Epoch 4/20] - loss: 0.0068 - metric: 0.0585 - val_loss: 0.0076 - val_metric: 0.0619 [Epoch 5/20] - loss: 0.0066 - metric: 0.0577 - val_loss: 0.0067 - val_metric: 0.0580  conv_window.plot(multi_step_dense)  该模型的主要缺点是，要求输入和标签数据的窗口宽度必须为 CONV_WIDTH 和 1，而 wide_window 对象划分的数据就不能传入。\nprint('Input shape:', wide_window.example[0].shape) try: print('Output shape:', multi_step_dense(wide_window.example[0]).shape) except Exception as e: print(f'\\n{type(e).__name__}:{e}')  Input shape: torch.Size([32, 24, 19]) RuntimeError:mat1 and mat2 shapes cannot be multiplied (32x456 and 57x32)  下一节的卷积模型将会解决这个问题。\n卷积神经网络 卷积层（torch.nn.Conv1d）同样可以用多个时间步的输入做预测。下面是跟 multi_step_dense 相同的模型，不过用卷积改写了一下。改动在于：\n 用 nn.Conv1d 替换掉了 nn.Flatten 和 nn.Linear。不过因为 PyTorch 的一维卷积是对输入的最后一维做的，而这里希望对时间维，也就是第二维做，因此要用 torch.Tensor.transpose 转置一下。 最后不需要 nnReshape 层了，因为前面都保留了时间维。  class nnTranspose(nn.Module): def __init__(self, dim0, dim1): super(nnTranspose, self).__init__() self.dim0 = dim0 self.dim1 = dim1 def forward(self, x): return x.transpose(self.dim0, self.dim1) class ConvModel(Model): def __init__(self, in_features, out_features): super(ConvModel, self).__init__() self.layers = nn.Sequential( nnTranspose(1, 2), nn.Conv1d(in_features, 32, CONV_WIDTH), nnTranspose(1, 2), nn.Linear(32, 32), nn.ReLU(), nn.Linear(32, out_features) ) def forward(self, x): return self.layers(x)  在 example 批上检查一下模型输出的形状：\nconv_model = ConvModel(num_features, 1) print('Input shape:', conv_window.example[0].shape) print('Output shape:', conv_model(conv_window.example[0]).shape)  Input shape: torch.Size([32, 3, 19]) Output shape: torch.Size([32, 1, 1])  在 conv_window 上训练并评估，其表现应该和 multi_step_dense 非常接近。\ncompile_and_fit(conv_model, conv_window) val_performance['Conv'] = conv_model.evaluate(conv_window.val) test_performance['Conv'] = conv_model.evaluate(conv_window.test)  [Epoch 1/20] - loss: 0.0158 - metric: 0.0789 - val_loss: 0.0081 - val_metric: 0.0667 [Epoch 2/20] - loss: 0.0075 - metric: 0.0618 - val_loss: 0.0081 - val_metric: 0.0676 [Epoch 3/20] - loss: 0.0073 - metric: 0.0608 - val_loss: 0.0068 - val_metric: 0.0596 [Epoch 4/20] - loss: 0.0070 - metric: 0.0594 - val_loss: 0.0068 - val_metric: 0.0589 [Epoch 5/20] - loss: 0.0070 - metric: 0.0593 - val_loss: 0.0061 - val_metric: 0.0547 [Epoch 6/20] - loss: 0.0068 - metric: 0.0583 - val_loss: 0.0062 - val_metric: 0.0551  conv_model 和 multi_step_dense 的差异在于，conv_model 能在任意长度的输入上进行预测。一维卷积相当于滑动窗口版的线性层：\n如果你在更宽的输入上跑一下模型，会自动产生更宽的输出：\nprint('Wide window') print('Input shape:', wide_window.example[0].shape) print('Labels shape:', wide_window.example[1].shape) print('Output shape:', conv_model(wide_window.example[0]).shape)  Wide window Input shape: torch.Size([32, 24, 19]) Labels shape: torch.Size([32, 24, 1]) Output shape: torch.Size([32, 22, 1])  可以看到输出的长度要比输入短两格，这是因为无论输入有多宽，开头 CONV_WIDTH 个输入总得用来“启动”第一个预测，导致预测的长度总是比输入短 CONV_WIDTH - 1 格。以前面的图示为例，t=0,1,2 时刻的输入产生 t=3 的预测，t=1,2,3 时刻的输入产生 t=4 的预测，后面以此类推。但 t=1 的预测需要 t=-2,-1,0 的输入，t=2 的预测需要 t=-1,0,1 的输入，而这里并没有 t=-2,-1 的数据，因此预测序列要比输入序列短两格。\n显然 wide_window 产生的标签窗口并不满足这一点，所以如果想在更宽的输入上训练或画图，就需要 WindowGenerator 对象切出的标签窗口比输入窗口右移一步，同时开头要短两格：\nLABEL_WIDTH = 24 INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1) wide_conv_window = WindowGenerator( input_width=INPUT_WIDTH, label_width=LABEL_WIDTH, shift=1, label_columns=['T (degC)'] ) wide_conv_window  Total window size: 27 Input indices: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25] Label indices: [ 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26] Label column names(s): ['T (degC)']  print('Wide conv window') print('Input shape:', wide_conv_window.example[0].shape) print('Labels shape:', wide_conv_window.example[1].shape) print('Output shape:', conv_model(wide_conv_window.example[0]).shape)  Wide conv window Input shape: torch.Size([32, 26, 19]) Labels shape: torch.Size([32, 24, 1]) Output shape: torch.Size([32, 24, 1])  现在终于能在更宽的窗口上画出模型的预测结果了。注意前三个输入值后才出现第一个预测值，因为每步预测都源于前三步的输入：\nwide_conv_window.plot(conv_model)  循环神经网络 循环神经网络（RNN）是一种特别适合处理时间序列的神经网络。RNN 会一步接一步地处理时间序列，同时用一个内部状态记录每步输入的信息。你可以在 用 RNN 生成文本 的教程和 用 Keras 学循环神经网络（RNN） 的指南里学到更多。\n本教程用到的 RNN 层是长短期记忆（LSTM，torch.nn.LSTM）。将一个序列输入 LSTM 后能得到等长的序列和最后一个 cell 的状态（hidden state 和 cell state），用来做单步预测的话大致有两种思路：\n 输出序列的最后一步已经包含了输入序列所有时间步的信息，输出序列前面每步的运算相当于是在预热（warmup）模型。这最后一步可以用作下一时刻的预测。  直接以输出序列作为下一时刻的预测值，相当于一次性给出了与输出序列等长的预测序列。  下面用思路 2 进行演示：\nclass LstmModel(Model): def __init__(self, in_features, out_features): super(LstmModel, self).__init__() self.lstm = nn.LSTM(in_features, 32, batch_first=True) self.linear = nn.Linear(32, out_features) def forward(self, x): output, _ = self.lstm(x) return self.linear(output)   提示：这种用法可能使模型的表现变差，因为输出的第一步并只用到了输入第一步的信息，后面的输出里才会逐渐积累历史输入的信息。因此输出序列前几步的表现可能不比简单的 linear 和 dense 之类的模型强。\n lstm_model = LstmModel(num_features, 1) print('Input shape:', wide_window.example[0].shape) print('Output shape:', lstm_model(wide_window.example[0]).shape)  Input shape: torch.Size([32, 24, 19]) Output shape: torch.Size([32, 24, 1])  compile_and_fit(lstm_model, wide_window) val_performance['LSTM'] = lstm_model.evaluate(wide_window.val) test_performance['LSTM'] = lstm_model.evaluate(wide_window.test)  [Epoch 1/20] - loss: 0.0311 - metric: 0.0900 - val_loss: 0.0063 - val_metric: 0.0552 [Epoch 2/20] - loss: 0.0063 - metric: 0.0548 - val_loss: 0.0058 - val_metric: 0.0526 [Epoch 3/20] - loss: 0.0059 - metric: 0.0529 - val_loss: 0.0056 - val_metric: 0.0517 [Epoch 4/20] - loss: 0.0057 - metric: 0.0521 - val_loss: 0.0056 - val_metric: 0.0513 [Epoch 5/20] - loss: 0.0056 - metric: 0.0514 - val_loss: 0.0055 - val_metric: 0.0508 [Epoch 6/20] - loss: 0.0055 - metric: 0.0510 - val_loss: 0.0055 - val_metric: 0.0512 [Epoch 7/20] - loss: 0.0054 - metric: 0.0505 - val_loss: 0.0057 - val_metric: 0.0519  wide_window.plot(lstm_model)  预测表现 在本教程的数据集上，这些模型的表现应该一个比一个强：\ndef plot_performance(val_performance, test_performance, ylabel): x = np.arange(len(val_performance)) width = 0.3 metric_index = 1 val_mae = [v[metric_index] for v in val_performance.values()] test_mae = [v[metric_index] for v in test_performance.values()] plt.ylabel(ylabel) plt.bar(x - 0.17, val_mae, width, label='Validation') plt.bar(x + 0.17, test_mae, width, label='Test') plt.xticks(ticks=x, labels=val_performance.keys(), rotation=45) plt.legend() plot_performance( val_performance, test_performance, ylabel='mean_absolute_error [T (degC), normalized]' )  for name, value in test_performance.items(): print(f'{name:18s}: {value[1]:0.4f}')  Baseline : 0.0853 Linear : 0.0668 Dense : 0.0625 Multi step dense : 0.0578 Conv : 0.0592 LSTM : 0.0534   译注：卷积神经网络一节说 Conv 的性能应该和 Multi step dense 持平，但图里不是明显更差么……\n 多变量输出模型 目前为止介绍的模型都是在单个时间步上预测单个特征 T (degC)。\n要让这些模型输出多个特征其实非常简单，只需要修改输出层的特征数，让 WindowGenerator 产生的标签包含所有特征即可：\nsingle_step_window = WindowGenerator(input_width=1, label_width=1, shift=1) wide_window = WindowGenerator(input_width=24, label_width=24, shift=1) example_inputs, example_labels = wide_window.example print(f'Inputs shape (batch, time, features): {example_inputs.shape}') print(f'Labels shape (batch, time, features): {example_labels.shape}')  Inputs shape (batch, time, features): torch.Size([32, 24, 19]) Labels shape (batch, time, features): torch.Size([32, 24, 19])  现在 example_labels 中 features 一维的长度和 example_inputs 相同了（以前是 1，现在是 19）。\n基准模型 缺省 label_index 参数即可重复所有输入特征：\nbaseline = Baseline() baseline.compile(loss_fn=nn.MSELoss(), metric_fn=nn.L1Loss())  val_performance = {} test_performance = {} loss, metric = baseline.evaluate(single_step_window.val) print(f'loss: {loss:.4f} - metric: {metric:.4f}') val_performance['Baseline'] = baseline.evaluate(single_step_window.val) test_performance['Baseline'] = baseline.evaluate(single_step_window.test)  loss: 0.0892 - metric: 0.1593  密集层模型 dense = Dense(num_features, num_features) compile_and_fit(dense, single_step_window) val_performance['Dense'] = dense.evaluate(single_step_window.val) test_performance['Dense'] = dense.evaluate(single_step_window.test)  [Epoch 1/20] - loss: 0.1069 - metric: 0.1845 - val_loss: 0.0726 - val_metric: 0.1448 [Epoch 2/20] - loss: 0.0724 - metric: 0.1430 - val_loss: 0.0717 - val_metric: 0.1405 [Epoch 3/20] - loss: 0.0711 - metric: 0.1394 - val_loss: 0.0705 - val_metric: 0.1382 [Epoch 4/20] - loss: 0.0703 - metric: 0.1370 - val_loss: 0.0710 - val_metric: 0.1383 [Epoch 5/20] - loss: 0.0699 - metric: 0.1355 - val_loss: 0.0698 - val_metric: 0.1342 [Epoch 6/20] - loss: 0.0692 - metric: 0.1335 - val_loss: 0.0690 - val_metric: 0.1328 [Epoch 7/20] - loss: 0.0689 - metric: 0.1326 - val_loss: 0.0681 - val_metric: 0.1307 [Epoch 8/20] - loss: 0.0686 - metric: 0.1315 - val_loss: 0.0687 - val_metric: 0.1314 [Epoch 9/20] - loss: 0.0684 - metric: 0.1308 - val_loss: 0.0683 - val_metric: 0.1316  RNN 模型 %%time lstm_model = LstmModel(num_features, num_features) compile_and_fit(lstm_model, wide_window) val_performance['LSTM'] = lstm_model.evaluate(wide_window.val) test_performance['LSTM'] = lstm_model.evaluate(wide_window.test)  [Epoch 1/20] - loss: 0.1299 - metric: 0.2077 - val_loss: 0.0688 - val_metric: 0.1384 [Epoch 2/20] - loss: 0.0662 - metric: 0.1318 - val_loss: 0.0643 - val_metric: 0.1277 [Epoch 3/20] - loss: 0.0638 - metric: 0.1262 - val_loss: 0.0633 - val_metric: 0.1253 [Epoch 4/20] - loss: 0.0628 - metric: 0.1240 - val_loss: 0.0627 - val_metric: 0.1234 [Epoch 5/20] - loss: 0.0622 - metric: 0.1227 - val_loss: 0.0624 - val_metric: 0.1227 [Epoch 6/20] - loss: 0.0618 - metric: 0.1218 - val_loss: 0.0623 - val_metric: 0.1225 [Epoch 7/20] - loss: 0.0615 - metric: 0.1213 - val_loss: 0.0624 - val_metric: 0.1218 [Epoch 8/20] - loss: 0.0613 - metric: 0.1209 - val_loss: 0.0625 - val_metric: 0.1217 CPU times: total: 12.7 s Wall time: 2min 23s  高级方法：残差连接 前面的基准模型利用了数据集这样一个特性：序列中相邻两步间的差异并不是很大。而其它需要训练的模型首先会随机初始化权重参数，然后才在训练中逐渐学到输出值相比前一步只变化了一点的事实。虽然你可以通过微调初始化方法来解决这一问题，但更简单的做法是把这种关系纳入模型结构中。\n不直接预测下一步的数值而预测下一步的改变量，在时间序列建模中是很常见的策略。类似的，深度学习中的 残差神经网络（ResNets）就是一种将每层输出加到模型累积结果里的结构。\n改变很小这一特性，就是这样被利用起来的。\n本质上讲，该结构相当于把模型初始化到跟 Baseline 一样的状态。对于当下的预测任务，该结构能使模型收敛更快，稍稍提高模型的性能。除此之外，该结构还能结合本教程提到的其它模型使用。\n这里结合 LSTM 进行演示，注意用到了 torch.nn.init.zeros_ 将 LSTM 最后一层的权重置零，以确保训练刚开始时预测出的改变量足够小，并且不会盖过残差连接的效果。因为置零仅对最后一层进行，所以不必担心梯度会出现 symmetry-breaking 的问题。\nclass ResidualWrapper(Model): def __init__(self, model): super(ResidualWrapper, self).__init__() self.model = model def forward(self, x): dx = self.model(x) return x + dx  %%time lstm_model = LstmModel(num_features, num_features) nn.init.zeros_(lstm_model.linear.weight) # 直接修改参数的话需要用no_grad包裹. residual_lstm = ResidualWrapper(lstm_model) compile_and_fit(residual_lstm, wide_window) val_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val) test_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test)  [Epoch 1/20] - loss: 0.0658 - metric: 0.1238 - val_loss: 0.0631 - val_metric: 0.1193 [Epoch 2/20] - loss: 0.0620 - metric: 0.1181 - val_loss: 0.0624 - val_metric: 0.1182 [Epoch 3/20] - loss: 0.0609 - metric: 0.1169 - val_loss: 0.0622 - val_metric: 0.1178 [Epoch 4/20] - loss: 0.0603 - metric: 0.1163 - val_loss: 0.0619 - val_metric: 0.1174 [Epoch 5/20] - loss: 0.0598 - metric: 0.1159 - val_loss: 0.0619 - val_metric: 0.1173 [Epoch 6/20] - loss: 0.0593 - metric: 0.1155 - val_loss: 0.0623 - val_metric: 0.1175 CPU times: total: 15.4 s Wall time: 1min 55s  预测表现 下面是这些多变量输出模型的总体表现：\nplot_performance( val_performance, test_performance, ylabel='MAE (average over all outputs)' )  for name, value in test_performance.items(): print(f'{name:15s}: {value[1]:0.4f}')  Baseline : 0.1633 Dense : 0.1332 LSTM : 0.1237 Residual LSTM : 0.1188  以上评分取模型所有输出的平均值。\n多步模型 前面几节的单变量输出和多变量输出模型都只能预测一个时间步，即一小时后。本节将介绍如何将这些模型扩展成能预测多个时间步的版本。\n在做多步预测时，模型需要学习预测未来一段时间范围的值。因此多步模型不像单步模型那样只能给出未来一个时刻的预测，而是会输出一段预测序列。为此大致有两种方法：\n 单发预测（single-shot），即一次性预测整段时间序列。 自回归预测（autoregressive），即模型做单步预测后，再把这步结果作为输入喂给模型，得到下下步的预测，以此类推。  本节的模型将在输出时间步上预测所有特征（即多变量输出）。\n对于多步模型，训练数据同样由每小时的样本组成。不同的是模型将学习用过去 24 小时的输入预测未来 24 小时的特征。\n下面的 Window 对象会从数据集中生成我们需要的切片：\nOUT_STEPS = 24 multi_window = WindowGenerator( input_width=24, label_width=OUT_STEPS, shift=OUT_STEPS ) multi_window.plot() multi_window  Total window size: 48 Input indices: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] Label indices: [24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47] Label column names(s): ['p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'Wx', 'Wy', 'max Wx', 'max Wy', 'Day sin', 'Day cos', 'Year sin', 'Year cos']  基准模型 对该任务来说一个简单的基准模型就是把输入最后一步的数值重复 OUT_STEPS 次：\nclass MultiStepLastBaseline(Model): def __init__(self): super(MultiStepLastBaseline, self).__init__() def forward(self, x): return x[:, -1:, :].tile(1, OUT_STEPS, 1) last_baseline = MultiStepLastBaseline() last_baseline.compile(loss_fn=nn.MSELoss(), metric_fn=nn.L1Loss()) multi_val_performance = {} multi_test_performance = {} multi_val_performance['Last'] = last_baseline.evaluate(multi_window.val) multi_test_performance['Last'] = last_baseline.evaluate(multi_window.test) multi_window.plot(last_baseline)  loss: 0.6286 - metric: 0.5007  因为预测任务是用过去 24 小时预测未来 24 小时，两段时间正好都是一天的长度，所以另一个简单的方法是假设明天和今天的时间序列差不多，直接重复今天的序列作为预测：\nclass RepeatBaseline(Model): def __init__(self): super(RepeatBaseline, self).__init__() def forward(self, x): return x repeat_baseline = RepeatBaseline() repeat_baseline.compile(loss_fn=nn.MSELoss(), metric_fn=nn.L1Loss()) loss, metric = repeat_baseline.evaluate(multi_window.val) print(f'loss: {loss:.4f} - metric: {metric:.4f}') multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val) multi_test_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test) multi_window.plot(repeat_baseline)  loss: 0.4271 - metric: 0.3959  单发模型 一个更高级点的方法是使用“单发”模型，即模型会一次性预测未来的整段序列。\n将 torch.nn.Linear 的输出特征数设为 OUT_STEPS * features 即可轻松实现这种模型，只是模型的最后一层需要将输出变形成 (batch, OUT_STEPS, features) 的形状。\n线性模型 一个用输入序列最后一步做预测的简单的线性模型表现会比前面两种基准模型要好，但好也好不到哪里去。该模型会把单步输入线性投影成 OUT_STEPS 步的预测结果，因此只能捕捉到序列行为里低维的部分，很可能主要靠时间在一天或一年中的位置来做预测。\nclass MultiLinearModel(Model): def __init__(self, in_features, out_features): super(MultiLinearModel, self).__init__() self.layers = nn.Sequential( # Shape =\u0026gt; [batch, 1, OUT_STEPS * features] nn.Linear(in_features, OUT_STEPS * out_features), # Shape =\u0026gt; [batch, OUT_STEPS, features] nnReshape(-1, OUT_STEPS, out_features) ) nn.init.zeros_(self.layers[0].weight) def forward(self, x): # Shape [batch, time, features] =\u0026gt; [batch, 1, features] return self.layers(x[:, -1:, :]) multi_linear_model = MultiLinearModel(num_features, num_features) compile_and_fit(multi_linear_model, multi_window) multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val) multi_test_performance['Linear'] = multi_linear_model.evaluate(multi_window.test) multi_window.plot(multi_linear_model)  [Epoch 1/20] - loss: 0.3296 - metric: 0.3990 - val_loss: 0.2585 - val_metric: 0.3232 [Epoch 2/20] - loss: 0.2570 - metric: 0.3127 - val_loss: 0.2558 - val_metric: 0.3059 [Epoch 3/20] - loss: 0.2562 - metric: 0.3074 - val_loss: 0.2559 - val_metric: 0.3055 [Epoch 4/20] - loss: 0.2561 - metric: 0.3072 - val_loss: 0.2550 - val_metric: 0.3049 [Epoch 5/20] - loss: 0.2560 - metric: 0.3071 - val_loss: 0.2558 - val_metric: 0.3052 [Epoch 6/20] - loss: 0.2560 - metric: 0.3071 - val_loss: 0.2555 - val_metric: 0.3050  密集层模型 在线性模型的输入输出之间再加一层 torch.nn.Linear 能增强其表现，不过说到底还是只用了输入最后一步的信息。\nclass MultiDenseModel(Model): def __init__(self, in_features, out_features): super(MultiDenseModel, self).__init__() self.dense = nn.Sequential( # Shape =\u0026gt; [batch, 1, 512] nn.Linear(in_features, 512), nn.ReLU(), # Shape =\u0026gt; [batch, 1, OUT_STEPS * features] nn.Linear(512, OUT_STEPS * out_features), # Shape =\u0026gt; [batch, OUT_STEPS, features] nnReshape(-1, OUT_STEPS, out_features) ) nn.init.zeros_(self.dense[2].weight) def forward(self, x): # Shape [batch, time, features] =\u0026gt; [batch, 1, features] return self.dense(x[:, -1:, :]) multi_dense_model = MultiDenseModel(num_features, num_features) compile_and_fit(multi_dense_model, multi_window) multi_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val) multi_test_performance['Dense'] = multi_dense_model.evaluate(multi_window.test) multi_window.plot(multi_dense_model)  [Epoch 1/20] - loss: 0.2346 - metric: 0.2957 - val_loss: 0.2249 - val_metric: 0.2854 [Epoch 2/20] - loss: 0.2204 - metric: 0.2826 - val_loss: 0.2217 - val_metric: 0.2845 [Epoch 3/20] - loss: 0.2169 - metric: 0.2798 - val_loss: 0.2208 - val_metric: 0.2835 [Epoch 4/20] - loss: 0.2146 - metric: 0.2781 - val_loss: 0.2179 - val_metric: 0.2806 [Epoch 5/20] - loss: 0.2130 - metric: 0.2771 - val_loss: 0.2194 - val_metric: 0.2824 [Epoch 6/20] - loss: 0.2118 - metric: 0.2763 - val_loss: 0.2194 - val_metric: 0.2809  CNN 卷积模型会用固定宽度的历史输入做预测，因为能看到输入是如何随时间变化的，所以表现可能比密集层模型要好点：\nCONV_WIDTH = 3 class MultiConvModel(Model): def __init__(self, in_features, out_features): super(MultiConvModel, self).__init__() self.layers = nn.Sequential( # Shape =\u0026gt; [batch, 1, 256] nnTranspose(1, 2), nn.Conv1d(in_features, 256, CONV_WIDTH), nnTranspose(1, 2), nn.ReLU(), # Shape =\u0026gt; [batch, 1, OUTSTEPS * features] nn.Linear(256, OUT_STEPS * out_features), # Shape =\u0026gt; [batch, OUTSTEPS, features] nnReshape(-1, OUT_STEPS, out_features) ) nn.init.zeros_(self.layers[4].weight) def forward(self, x): # Shape [batch, time, features] =\u0026gt; [batch, CONV_WIDTH, features] return self.layers(x[:, -CONV_WIDTH:, :]) multi_conv_model = MultiConvModel(num_features, num_features) compile_and_fit(multi_conv_model, multi_window) multi_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val) multi_test_performance['Conv'] = multi_conv_model.evaluate(multi_window.test) multi_window.plot(multi_conv_model)  [Epoch 1/20] - loss: 0.2360 - metric: 0.3019 - val_loss: 0.2223 - val_metric: 0.2874 [Epoch 2/20] - loss: 0.2188 - metric: 0.2853 - val_loss: 0.2193 - val_metric: 0.2854 [Epoch 3/20] - loss: 0.2149 - metric: 0.2819 - val_loss: 0.2174 - val_metric: 0.2838 [Epoch 4/20] - loss: 0.2122 - metric: 0.2797 - val_loss: 0.2176 - val_metric: 0.2846 [Epoch 5/20] - loss: 0.2101 - metric: 0.2781 - val_loss: 0.2169 - val_metric: 0.2822 [Epoch 6/20] - loss: 0.2083 - metric: 0.2765 - val_loss: 0.2164 - val_metric: 0.2816 [Epoch 7/20] - loss: 0.2069 - metric: 0.2755 - val_loss: 0.2124 - val_metric: 0.2782 [Epoch 8/20] - loss: 0.2058 - metric: 0.2744 - val_loss: 0.2161 - val_metric: 0.2825 [Epoch 9/20] - loss: 0.2049 - metric: 0.2738 - val_loss: 0.2154 - val_metric: 0.2806  RNN RNN 模型能够学习用长期的历史数据做预测。下面这个模型会用内部状态积累 24 小时历史数据里的信息，然后一次性预报接下来 24 小时。\n我们在这种单发预测里只需要 LSTM 输出的最后一个时间步：\nclass MultiLstmModel(Model): def __init__(self, in_features, out_features): super(MultiLstmModel, self).__init__() self.lstm = nn.LSTM(in_features, 32, batch_first=True) self.linear = nn.Sequential( # Shape =\u0026gt; [batch, 1, OUT_STEPS * features] nn.Linear(32, OUT_STEPS * out_features), # Shape =\u0026gt; [batch, OUT_STEPS, features] nnReshape(-1, OUT_STEPS, out_features) ) nn.init.zeros_(self.linear[0].weight) def forward(self, x): # Shape [batch, time, features] =\u0026gt; [batch, time, 32] output, _ = self.lstm(x) # Shape =\u0026gt; [batch, 1, 32] return self.linear(output[:, -1:, :]) multi_lstm_model = MultiLstmModel(num_features, num_features) compile_and_fit(multi_lstm_model, multi_window) multi_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val) multi_test_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test) multi_window.plot(multi_lstm_model)  [Epoch 1/20] - loss: 0.2907 - metric: 0.3612 - val_loss: 0.2290 - val_metric: 0.3052 [Epoch 2/20] - loss: 0.2167 - metric: 0.2937 - val_loss: 0.2203 - val_metric: 0.2947 [Epoch 3/20] - loss: 0.2095 - metric: 0.2853 - val_loss: 0.2178 - val_metric: 0.2913 [Epoch 4/20] - loss: 0.2054 - metric: 0.2809 - val_loss: 0.2188 - val_metric: 0.2896 [Epoch 5/20] - loss: 0.2027 - metric: 0.2780 - val_loss: 0.2133 - val_metric: 0.2853 [Epoch 6/20] - loss: 0.2006 - metric: 0.2761 - val_loss: 0.2124 - val_metric: 0.2832 [Epoch 7/20] - loss: 0.1989 - metric: 0.2744 - val_loss: 0.2142 - val_metric: 0.2844 [Epoch 8/20] - loss: 0.1973 - metric: 0.2730 - val_loss: 0.2133 - val_metric: 0.2823  高级方法：自回归模型 上面几个模型都是一次性预报一整段序列，但有些时候让模型把预测分解到每一步上会更有帮助。然后，模型在每一步上的输出都会回馈给模型自身，下一步的预测可以基于前一步的预测来做，就像经典的 用循环神经网络生成序列 一样。\n这类模型最显著的优点是，能够产生任意时间长度的预测。\n你当然可以用教程前面训练好的单步模型做自回归的循环（输出单步再输回去），但这里我们想搭一个直接把自回归纳入训练过程的模型。\nRNN 本教程只演示自回归的 RNN 模型，但其套路可以沿用于其它单步输出的模型。该模型的基础和之前的单步 LSTM 模型一样：torch.nn.LSTM 输出序列的最后一步被 torch.nn.Linear 变换成了预测值。\nclass FeedBack(Model): def __init__(self, num_features, out_steps): super(FeedBack, self).__init__() self.out_steps = out_steps self.lstm = nn.LSTM(num_features, 32, batch_first=True) self.linear = nn.Linear(32, num_features) def next(self, x, hc=None): # Shape [batch, time, features] =\u0026gt; [batch, 1, features] output, hc = self.lstm(x, hc) prediction = self.linear(output[:, -1:, :]) return prediction, hc def forward(self, x): predictions = [] prediction, hc = self.next(x) predictions.append(prediction) for i in range(1, self.out_steps): prediction, hc = self.next(prediction, hc) predictions.append(prediction) # Shape =\u0026gt; [batch, out_steps, features] predictions = torch.cat(predictions, dim=1) return predictions  最开始我们将形如 (batch, time, features) 的输入序列传入 next 方法中，得到形如 (batch, 1, features) 的单步预测值 prediction 和 LSTM 网络中最后一个 cell 的状态 hc。本来应该是把输入序列和 prediction 连起来作为新的输入序列传入 next 方法，得到下下步的预测。但得益于 hc 记录了之前序列里的信息，我们可以只把 prediction 和 hc 传入 next 方法，得到下下步的预测。如此循环操作 out_steps 步，再将这些结果用 torch.cat 连起来，便得到了形如 (batch, out_steps, features) 的输出序列。\nfeedback = FeedBack(num_features, OUT_STEPS) compile_and_fit(feedback, multi_window) multi_val_performance['AR LSTM'] = feedback.evaluate(multi_window.val) multi_test_performance['AR LSTM'] = feedback.evaluate(multi_window.test) multi_window.plot(feedback)  [Epoch 1/20] - loss: 0.3593 - metric: 0.4166 - val_loss: 0.2748 - val_metric: 0.3589 [Epoch 2/20] - loss: 0.2522 - metric: 0.3367 - val_loss: 0.2539 - val_metric: 0.3354 [Epoch 3/20] - loss: 0.2369 - metric: 0.3208 - val_loss: 0.2415 - val_metric: 0.3198 [Epoch 4/20] - loss: 0.2264 - metric: 0.3079 - val_loss: 0.2348 - val_metric: 0.3141 [Epoch 5/20] - loss: 0.2207 - metric: 0.3013 - val_loss: 0.2339 - val_metric: 0.3080 [Epoch 6/20] - loss: 0.2172 - metric: 0.2975 - val_loss: 0.2287 - val_metric: 0.3036 [Epoch 7/20] - loss: 0.2136 - metric: 0.2941 - val_loss: 0.2268 - val_metric: 0.3012 [Epoch 8/20] - loss: 0.2114 - metric: 0.2917 - val_loss: 0.2270 - val_metric: 0.3006 [Epoch 9/20] - loss: 0.2086 - metric: 0.2889 - val_loss: 0.2231 - val_metric: 0.2963 [Epoch 10/20] - loss: 0.2065 - metric: 0.2873 - val_loss: 0.2260 - val_metric: 0.3001 [Epoch 11/20] - loss: 0.2040 - metric: 0.2850 - val_loss: 0.2286 - val_metric: 0.3018  预测表现 在多步预测的问题上，显然收益随模型复杂度的升高而递减：\nplot_performance( multi_val_performance, multi_test_performance, ylabel='MAE (average over all times and outputs)' )  本教程多变量输出模型一节的评分图是对所有输出特征做平均后画出来的，这里的评分也类似，不过进一步在输出的时间步上也做了平均。\nfor name, value in multi_test_performance.items(): print(f'{name:8s}: {value[1]:0.4f}')  Last : 0.5156 Repeat : 0.3774 Linear : 0.2983 Dense : 0.2756 Conv : 0.2744 LSTM : 0.2730 AR LSTM : 0.2922  将密集层模型升级成卷积模型和循环模型后获得的收益仅有微小的几个百分点，自回归模型甚至表现变得更差了。因此，这些复杂的模型可能并不适用于该问题，但实际应用中模型是好是坏只有动手试一试才会知道，说不定这些模型有助于解决你的问题呢。\n下一步 本教程简单介绍了如何使用 PyTorch 预测时间序列。更多相关教程请参阅：\n 《机器学习实战：基于 Scikit-Learn、Keras 和 TensorFlow》 第二版的 15 章。 《Python 深度学习》 的第 6 章。 Udacity 的 Intro to TensorFlow for Deep Learning 的第 8 课，包含 练习题。  此外，虽然本教程只关注 PyTorch 内置的功能，但你也可以用来实现任何 经典的时间序列模型。\n","date":"2022-10-15","permalink":"https://zhajiman.github.io/post/pytorch_time_series_tutorial/","tags":["pytorch","时间序列","翻译"],"title":"PyTorch 时间序列预测入门"},{"content":" 这是物理海洋学家 Ken Hughes 在 2021 年发表的博客文章，原文标题为 A better way to code up scientific figures。以 Matplotlib 和 Matlab 为例，强调了模块化思想对于科研作图代码的帮助。我很少看到关于作图方法论的文章，所以翻译出来交流学习。\n 我画一张出版级别的科研配图一般需要写 100 - 200 行代码，这个长度有点点危险，因为很容易写出能正常运行但又一团糟的东西。如果代码片段都很短还可以从头重写，但如果代码有上千行，提前做好规划会更明智一些。不过在这两种极端情况之间潜藏着另一种吸引人的做法：写出一段当时感觉无比连贯，但以后会让你吃苦头的脚本。\n假设你想画一张中等复杂度的图片，类似下面这张：\n相应的脚本可以被设想为一系列步骤：\n 从 csv 文件中读取数据 去掉被标记（flagged）的数据 创建四张子图（subplot） 第一行里画数据随时间的变化 给 y 轴添加标签（label） 设置 y 轴的范围 第二行和第三行重复步骤 4 - 6 添加填色图（contour）和灰色的等高线 给时间轴添加标签 添加各种标注（annotation）  如果你对 Python、Matlab 或 R 之类的语言很熟，就能轻松地将步骤 1 - 10 扩充为一股“意识流”。像什么添加子图、给多个面板（panel）加标签、设置坐标轴范围等操作都可以不假思索地写出，因此你的脚本常常在不知不觉间超过 100 行。\n一般来说，笔记本电脑的屏幕或者外接显示器最多显示 40 - 50 行代码，所以你没法一眼看出脚本里的所有步骤。相反，你得靠你的短期记忆。\n不过先等一下！假设你想快速测试几个改动，于是你临时注释掉了几行代码，临时重写了一些变量，或者临时新添了一个面板图。\n恐怕你已经有种不详的预感了吧？这些临时改动中有一些会被保留，剩下的会改回去。最后原本简单的步骤 1 -10 变成了 1，1b，2，2b，3，3b，6，5，4，7，8，9，10，10b，10c，11，12。\n当你几个月后必须重温这个混乱的脚本时（例如第二审稿人给了点修改意见），其毛病才会真正显露出来。你写这个脚本的时候是靠短期记忆来理解所有片段是如何组合在一起的，但几个月后你肯定会忘个精光。\n作为一名科学家，过去几年里我写了太多这种混乱的作图脚本。即便到了现在，有时为了快速出结果我还是会这么写。不过在大部分时间里，我都会采用一种更好的编写方法。\n编写作图脚本的模块化方法 Ten simple rules for quick and dirty scientific programming 中的第四条就是模块化你的代码，并且这也正是本文将要给出的建议。\n我写的每个作图脚本都由十几个函数组成，有读取数据的函数、画多个面板的函数、每张折线图对应的函数、给所有 axes 加标签的函数等。下面是一个极简的 Python 例子，在我屏幕上的效果是这样的：\n你可能会觉得定义一堆单行函数有点小题大做，还会把脚本的长度变成原来的两倍。但请你相信我，只要你的脚本比这个小例子更复杂，模块化方法就能使你受益。\n具体来说，将多行代码归为函数有以下四个好处：\n1. 强迫你为脚本列出大纲 我在前面提过，很容易将一个作图脚本设想为一系列步骤，但据我的经验来看，科学家们很少会把这些步骤记录下来。不过如果你创建了一系列函数，就要求你先有一个高层级的概览（overview）。在这个例子里，最后五行代码自然构成了大纲。\n2. 你可以用大白话描述你的代码 你不会 Python 也能看懂例子脚本中的步骤，函数都是用大白话取的名，你只用看函数名就行。你当然可以用注释达成类似的效果，但在编写过程中注释内容往往会和代码的实际作用脱节。\n3. 定位到具体的命令更简单 假设你想改变一张面板图里几条线的颜色，如果你的作图脚本有几百行，就得花点时间定位到需要改动的地方。但当几百行代码被细分到少量的函数里时，定位就会快很多。这跟用目录来查教科书的某一面是一个道理。\n4. 你可以只注释一行而不是一整块代码 在迭代到成品图的过程中，你可能会测试不同的排列、数值或图形种类。通过注释和反注释代码块来实现当然是可以的，但这种做法不仅麻烦，而且可以说是一个 坏习惯。相反，如果你写的每个函数都只完成特定的任务，你就只用注释或反注释一行代码来进行调整。比方说我要修改上面的例子，操作大概如下图所示：\n你的函数不必完美 在我最初的示例中只有一个函数带参数，这无疑是种糟糕的写法，按理来说每个函数都应该有参数或能接受变量。对此我倒是不怎么担心，因为我知道当 Python 在函数体或输入里找不到所需的变量时就会到函数外面去找。由于每个函数都只用一次，所以不显式传递变量也没问题。（你可能会疑惑创建一次性的函数有没有价值，我的回答是 肯定的。）\n在写 Matlab 时我也采取了几乎一样的方针，唯一的差别是需要额外将整个脚本用一个父函数封装，不然没法定义嵌套函数。我的脚本大致长这样：\n“多用函数”的建议并不新鲜 科学领域常用的编程语言（Python、Matlab、R、Julia）非常适合交互式使用。在命令行窗口输入 284*396 就会输出 112464。但命令行也就止步于此了，你很快意识到你想连续执行多行命令。因此你把这组连续的命令移到了一个脚本里，然后点击运行按钮。与命令行窗口不同的是，这种脚本能带你走得更远。（当我提到脚本时，我也在暗指那些能计算的 notebook。类似脚本，notebook 同样混乱，而且会助长糟糕的编程习惯。）\n许多科学家能在不懂函数的情况下完成任务，而一个不会用函数的程序员则压根找不着工作。这种不一致性使我不确定本文的定位是否合适。一方面来说，多用函数的建议听起来像废话，就好比我建议科学家写论文的时候记得带标题一样。另一方面，我看过了太多擅长计算机的科学家写出来的混乱不堪的作图脚本，所以坚信“多用函数”是一条既有价值又深刻的建议。\n我并不是唯一一个试图弥合程序员和科学家之间鸿沟的人。正如软件可持续性研究所的 Simon Hettrick 所说：“这对写代码的科学家意味着什么？只有当他们当了软件工程师才算真正的程序员吗？我觉得不是。我认为科学家们应该把计算机编程作为一种探索性的工具来推动他们领域里的发现，这跟他们使用其它方法和工具并没有什么两样。但是作为科学家的程序员也能通过学习模块化、抽象化和数据结构而受益。”\n","date":"2022-09-17","permalink":"https://zhajiman.github.io/post/matplotlib_better_code/","tags":["matplotlib","翻译"],"title":"编写科研作图代码有更好的方法"},{"content":"前言 说到测量程序的运行时间这件事，我最早的做法是在桌上摆个手机，打开秒表应用，右手在命令行里敲回车的同时左手启动秒表，看屏幕上程序跑完后再马上按停秒表，最后在纸上记下时间。后来我在 Linux 上学会了在命令开头添加一个 time，终于摆脱了手动计时的原始操作。这次就想总结一下迄今为止我用过的那些测量时间的工具/代码。\n测试代码是读取河北省省界的 GeoJSON 文件，利用射线法判断网格点有没有落入省界内部，最后通过 Matplotlib 画出示意图并保存。GeoJSON 数据来自阿里云的 DataV.GeoAtlas。代码 test.py 的内容如下\nimport json import numpy as np import matplotlib.pyplot as plt def contain(polygon, x, y): '''判断点是否落入多边形中.''' if polygon['type'] == 'Polygon': coords_polygons = [polygon['coordinates']] elif polygon['type'] == 'MultiPolygon': coords_polygons = polygon['coordinates'] else: raise ValueError('输入不是多边形') # 对每个多边形应用射线法. for coords_polygon in coords_polygons: flag = False for coords_ring in coords_polygon: for i in range(len(coords_ring) - 1): x0, y0 = coords_ring[i] x1, y1 = coords_ring[i + 1] if y0 \u0026lt; y \u0026lt;= y1 or y1 \u0026lt; y \u0026lt;= y0: if x \u0026lt; (x1 - x0) / (y1 - y0) * (y - y0) + x0: flag = not flag if flag: return flag return False def main(): with open('河北省.json', encoding='utf-8') as f: geoj = json.load(f) hebei = geoj['features'][0]['geometry'] xs = [] ys = [] for x in np.linspace(110, 125, 200): for y in np.linspace(35, 45, 200): if contain(hebei, x, y): xs.append(x) ys.append(y) fig, ax = plt.subplots() ax.set_aspect('equal') ax.plot(xs, ys, 'o', c='C3', ms=0.2) fig.savefig('hebei.png', dpi=200, bbox_inches='tight') plt.close(fig) if __name__ == '__main__': main()  输出图片为\n系统命令 Linux 的 time 在 Linux 的 Bash 中通过在命令前加上 time，即可在命令执行结束后打印出三种耗时\n(base) laptop@zhajiman:/code$ time python test.py real 0m22.993s user 0m19.905s sys 0m0.096s  其中 real 指总耗时（墙上时钟经过的时间，即 wall time），user 指用户态代码耗费的 CPU 时间，sys 指系统态代码耗费的 CPU 时间。一般看 real 的数值即可，关于三种时间的解释可见 linux time命令详解与坑。\nWindows 的 Measure-Command Windows 中类似的命令是 Powershell 的 Measure-Command 命令\n(base) PS D:\\code\u0026gt; Measure-Command {python test.py} Days : 0 Hours : 0 Minutes : 0 Seconds : 16 Milliseconds : 442 Ticks : 164424518 TotalDays : 0.000190306155092593 TotalHours : 0.00456734772222222 TotalMinutes : 0.274040863333333 TotalSeconds : 16.4424518 TotalMilliseconds : 16442.4518  打印出了各种单位的耗时，一般看其中的 TotalSeconds 即可。不过该命令会吞掉 Python 程序本身的打印结果，这时可以通过管道在测量结束后把内容再打印出来\nMeasure-Command {python test.py | Out-Default}  显然 Linux 的 time 命令用起来要方便的多……\nIPython 的魔法命令 %run IPython 中的 %run 命令可以直接执行脚本文件，加上 -t 参数时会输出耗时\nIn [1]: %run -t test.py IPython CPU timings (estimated): User : 16.17 s. System : 0.00 s. Wall time: 16.37 s.  输出结果跟 Linux 的 time 命令很像，不过文档说 Windows 下的 System 时间直接设成了 0。加上 -N \u0026lt;N\u0026gt; 参数可以重复执行 \u0026lt;N\u0026gt; 次，输出里会多出总时间和平均时间之分。\n%time %time 会打印出单条语句或表达式的耗时\nIn [2]: %time main() CPU times: total: 16.6 s Wall time: 16.8 s  结果由 CPU 时间和墙上时间组成。\n%timeit 类似 %time，但为了得到精准的测量结果，会自动测量多次，以得到较为准确的平均耗时，还会为打印出来的结果挑选合适的单位（秒、毫秒、微秒等）。例如\nIn [3]: with open('河北省.json', encoding='utf-8') as f: ...: geoj = json.load(f) ...: hebei = geoj['features'][0]['geometry'] ...: In [4]: %timeit contain(hebei, 115, 40) 284 µs ± 4.63 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)  %timeit 默认跑 7 组，每组自动设置成了 1000 次循环，最后计算平均耗时和标准差。加上 -n \u0026lt;N\u0026gt; 可以指定循环 \u0026lt;N\u0026gt; 次，-r \u0026lt;R\u0026gt; 可以指定跑 \u0026lt;R\u0026gt; 组。例如测一次 main 函数的耗时\nIn [5]: %timeit -n 1 -r 1 main() 15.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)  无论是 %time 还是 %timeit 都有一个缺陷：只能在 IPython 顶层的作用域中使用。例如我想测试 contain(hebei, x, y) 对于单点的速度，因为变量 hebei 在函数 main 的作用域里，而外部没有，所以我在使用 %timeit 前又在全局新建了 hebei 变量。我还试过设置断点通过 ipdb 跳到函数内，结果发现此时 %timeit 用不了，不知道读者有没有比较好的解决方法。\n标准库的 time 模块 time 和 perf_counter 函数 time 模块的一个主要功能就是获取当前时间，所以只要在想计时的代码块开头获取一次时间，再在结尾获取一次，最后计算二者的差值就能得到代码块的耗时。例如\nimport time if __name__ == '__main__': t0 = time.time() main() t1 = time.time() dt = t1 - t0 print(f'{dt:.1f}', 's')  打印结果为 15.4 s。其中 time.time 函数没有参数，在 Windows 和 Linux 平台上调用后返回 Unix 时间戳（自 1970-01-01 00:00:00 UTC 以来的秒数）。这个函数的问题是与系统时间相关联，如果两次调用之间系统时间发生了变动（例如在任务栏里手动修改、联网自动校正等），那么第二次调用的结果也会跟着变化，最后计算出错误的 dt。另外该函数在 Windows 平台的时间分辨率也比较低，会测不准耗时较短的语句。\ntime.perf_counter 是 Python 3.3 起引入的新函数，名字是 performance counter 之意，即专门用来测量性能（耗时）的计时器。调用后返回的浮点数本身没有明确的意义，只有两次调用时结果的差值有意义，表示过程中经过的秒数。相比 time，perf_counter 的时间分辨率更高，且不受系统时间变动的影响，结果始终保证单调递增。所以在测量程序耗时中更推荐使用 perf_counter。\nif __name__ == '__main__': t0 = time.perf_counter() main() t1 = time.perf_counter() dt = t1 - t0 print(f'{dt:.1f}', 's')  另外 time 模块里还有一个 process_time 函数，说是能测量当前进程的用户态和系统态 CPU 时间之和，且不包含睡眠时间（例如调用 time.sleep 函数）。但我测试后发现测出来的时间经常比 perf_counter 的结果要大，实在让人摸不着头脑，所以这里就不多介绍了。\n装饰器 直接在程序中插入 perf_counter 的用法虽然很灵活，但改起来十分麻烦。如果只想测量函数的耗时，使用装饰器语法更方便：无需修改函数体或主程序，只需在定义函数的 def 语句前插入一行，就能为函数增加计时功能。下面编写一个带可选参数的计时装饰器\nimport time import functools def timer(func=None, *, prompt=True, fmt=None, prec=None, out=None): ''' 计时用的装饰器. Parameters ---------- func : callable 需要被计时的函数或方法. prompt : bool 是否打印计时结果. fmt : str 打印格式. %n表示函数名, %t表示耗时. prec : int 打印时的小数位数. out : list 收集耗时的列表. ''' if fmt is None: fmt = '[%n] %t s' if func is None: return functools.partial( timer, prompt=prompt, fmt=fmt, prec=prec, out=out ) @functools.wraps(func) def wrapper(*args, **kwargs): t0 = time.perf_counter() result = func(*args, **kwargs) t1 = time.perf_counter() dt = t1 - t0 # 是否打印结果. if prompt: st = str(dt) if prec is None else f'{dt:.{prec}f}' info = fmt.replace('%n', func.__name__).replace('%t', st) print(info) # 是否输出到列表中. if out is not None: out.append(dt) return result return wrapper  装饰器相关的知识可见 入门Python装饰器。用法是在 main 函数前一行加上 @timer 即可，还可以通过参数控制装饰器的输出\nresults = [] @timer(fmt='Time spent by %n is %t s', prec=1, out=results) def main(): \u0026lt;函数体省略\u0026gt;  输出为\nTime spent by main is 16.9 s  同时耗时被保存到了全局的 results 列表中。如果想要测量的函数并非定义在当前文件里（例如第三方包里的函数），那么可以通过包装目标函数来实现。例如测量 figsave 方法的用时\ntimer(prec=1)(fig.savefig)('hebei.png', dpi=200, bbox_inches='tight')  输出为\n[savefig] 0.148 s  标准库的 cProfile 模块 time 模块适合测量代码块或函数的总耗时，如果想深入了解组成代码的所有函数各占了多长时间，推荐使用标准库的 cProfile 模块。该模块能够对程序进行性能剖析（profile），详尽给出各种函数和方法被调用的次数和耗时。但也因为监控的层级过于深入，会在一定程度上拖慢原程序的运行速度。所以该模块适合分析程序内各部分的相对用时，而不适合做精确的性能比较（benchmark）。\n命令行调用 直接在命令行调用会对整个脚本进行测量，无需对代码进行任何修改\npython -m cProfile [-o output_file] [-s sort_order] myscript.py  -o 表示将剖析结果输出为二进制文件，如果不给出就将结果以表格的形式打印在屏幕上。-s 表示如果不输出文件，那么可以按 pstats 模块 Stats.sort_stats 方法的规则对打印的表格记录进行排序。由于表格一般巨长无比，所以打印到屏幕上意义不大，我会用管道输出到文本文件后再用编辑器来看。例如在 PowerShell 中可以\npython -m cProfile -s cumtime test.py | Out-File -FilePath test.txt  表格前 20 行长这个样子\n 3372760 function calls (3338684 primitive calls) in 19.628 seconds Ordered by: cumulative time ncalls tottime percall cumtime percall filename:lineno(function) 782/1 0.003 0.000 19.629 19.629 {built-in method builtins.exec} 1 0.000 0.000 19.629 19.629 test.py:1(\u0026lt;module\u0026gt;) 1 0.028 0.028 17.907 17.907 test.py:33(main) 40000 17.519 0.000 17.532 0.000 test.py:9(contain) 96 0.003 0.000 3.383 0.035 __init__.py:1(\u0026lt;module\u0026gt;) 871/15 0.006 0.000 1.760 0.117 \u0026lt;frozen importlib._bootstrap\u0026gt;:1022(_find_and_load) 868/15 0.004 0.000 1.760 0.117 \u0026lt;frozen importlib._bootstrap\u0026gt;:987(_find_and_load_unlocked) 833/16 0.004 0.000 1.755 0.110 \u0026lt;frozen importlib._bootstrap\u0026gt;:664(_load_unlocked) 773/15 0.002 0.000 1.754 0.117 \u0026lt;frozen importlib._bootstrap_external\u0026gt;:877(exec_module) 1106/16 0.001 0.000 1.754 0.110 \u0026lt;frozen importlib._bootstrap\u0026gt;:233(_call_with_frames_removed) 535/25 0.001 0.000 1.659 0.066 {built-in method builtins.__import__} 1086/417 0.002 0.000 1.264 0.003 \u0026lt;frozen importlib._bootstrap\u0026gt;:1053(_handle_fromlist) 2652/2432 0.038 0.000 0.926 0.000 {built-in method builtins.__build_class__} 1 0.000 0.000 0.883 0.883 pyplot.py:1(\u0026lt;module\u0026gt;) 101 0.001 0.000 0.690 0.007 artist.py:128(_update_set_signature_and_docstring)  可以看到加了 cProfile 后程序耗时从 16 秒上升至 19 秒，期间共发生 3372760 次函数调用（不算递归的原始调用是 3338684 次）。表格各列的意义如下：\n ncalls：函数调用的次数。正斜杠区分总次数和原始调用。 tottime：每次调用的耗时之和，但如果函数内调用了其它函数，刨除掉在子函数中经过的时间。 percall：tottime 除以 ncalls 的值。 cumtime：每次调用的耗时之和，包含花费在子函数上的时间。 percall：cumtime 除以原始调用次数的值。 filename:lineno(function)：函数所在的文件名、定义所在的行号，和函数名。  -s 选项可以按这些列名进行排序，文件名和行号的话用 filename 和 line。表格中总耗时排第一的是内置方法 exec，可能是 cProfile 需要用 exec 执行 test.py 中的语句；第二是作为整个模块的 test.py，再是 main 函数；第四的 contain 函数调用 200 * 200 = 40000 次，耗时 17 秒，说明整个程序中最耗时的操作就是判断点是否落入多边形。而画图相关的函数在表格中的排位并不算高。\n程序内调用 如果想要指定测量范围，就需要在程序中显式调用 cProfile 模块。测量单条语句的耗时可以用：\n cProfile.run(command, filename=None, sort=-1)：command 是用字符串表示的 Python 语句，该方法会用 exec 函数执行该语句并测量其耗时。filename 参数用于将结果输出为文件，缺省时会打印出结果表格。sort 参数类似上一节，对打印结果进行排序。 cProfile.runctx(command, globals, locals, filename=None, sort=-1)：传入 run 的语句仅能引用全局作用域中的对象，而 runctx 可以指定全局作用域和局部作用域的字典。  例如测量 main() 这一句的耗时，输出与上一节类似\nimport cProfile if __name__ == '__main__': cProfile.run('main()', sort='cumtime')  测量代码块的耗时需要构造 cProfile.Profile 对象，它相当于一个计时器，通过调用方法来开关，会把计时结果保存下来，之后可以选择打印或输出文件。具体方法为：\n enable()：开始测量。 disable()：结束测量。 print_stats(sort=-1)：在内部创建一个 Stats 对象并用它打印表格，用 sort 参数进行排序。 dump_stats(filename)：将测量结果输出到文件。 run(cmd)：类似 cProfile.run，但没了打印和输出文件功能。 runctx(cmd, globals, locals)：类似 cProfile.runctx。 runcall(func, /, *args, **kwargs)：相当于先 enable()，再跑 func(*args, **kwargs)，然后 disable()。  以画图部分的代码块为例，先在开头开启计时器，再在结尾停止计时\nprofile = cProfile.Profile() profile.enable() fig, ax = plt.subplots() ax.set_aspect('equal') ax.plot(xs, ys, 'o', c='C3', ms=0.2) fig.savefig('hebei.png', dpi=200, bbox_inches='tight') plt.close(fig) profile.disable() profile.print_stats('cumtime')  另外 Profile 类支持上下文管理器，允许用 with 语句控制计时器的开关。所以上例可以改写为\nwith cProfile.Profile() as profile: fig, ax = plt.subplots() ax.set_aspect('equal') ax.plot(xs, ys, 'o', c='C3', ms=0.2) fig.savefig('hebei.png', dpi=200, bbox_inches='tight') plt.close(fig) profile.print_stats('cumtime')  前 20 行结果为，可以看到保存图片的语句耗时最长\n 199075 function calls (195323 primitive calls) in 0.454 seconds Ordered by: cumulative time ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 0.251 0.251 figure.py:2923(savefig) 1 0.000 0.000 0.251 0.251 backend_bases.py:2192(print_figure) 1 0.000 0.000 0.192 0.192 pyplot.py:1321(subplots) 1 0.000 0.000 0.165 0.165 pyplot.py:686(figure) 1 0.000 0.000 0.165 0.165 pyplot.py:324(new_figure_manager) 1 0.000 0.000 0.130 0.130 backend_bases.py:3487(new_figure_manager) 1 0.000 0.000 0.129 0.129 _backend_tk.py:940(new_figure_manager_given_figure) 2 0.000 0.000 0.125 0.063 artist.py:71(draw_wrapper) 212/2 0.001 0.000 0.125 0.063 artist.py:32(draw_wrapper) 2 0.000 0.000 0.125 0.063 figure.py:2813(draw) 4/2 0.000 0.000 0.122 0.061 image.py:114(_draw_list_compositing_images) 2 0.000 0.000 0.122 0.061 _base.py:3022(draw) 4 0.000 0.000 0.113 0.028 axis.py:1150(draw) 4 0.000 0.000 0.111 0.028 deprecation.py:384(wrapper) 2 0.000 0.000 0.109 0.054 backend_bases.py:1595(wrapper)  装饰器 编写一个测量函数用时并保存结果的装饰器\nimport cProfile import functools def cprofiler(filename): '''cProfile的装饰器. 保存结果到指定路径.''' def decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): with cProfile.Profile() as profile: result = func(*args, **kwargs) profile.dump_stats(filename) return result return wrapper return decorator  使用方法为\n@cprofiler('main.prof') def main(): \u0026lt;函数体省略\u0026gt;  可视化结果 cProfile 模块输出的二进制文件可以用 pstats 模块的 Stats 类进行读取，它相当于一个表格对象，能对记录进行复杂的排序、只打印前 n 条记录等。还可以直接用记录了测量结果的 Profile 对象构造 Stats 对象。Profile.print_stats 方法其实就是借助 Stats.print_stats 实现的。不过个人觉得这个模块挺鸡肋，不如直接 Profile.print_stats('cumtime') 结合管道导出文本文件，然后再在文本编辑器中查看。\n除此之外，另一个直观查看结果的方式就是用第三方包做可视化。这里介绍 snakeviz 包，通过 pip 或 conda 即可安装，使用方法非常简单，在命令行执行\nsnakeviz main.prof  就能解析 main.prof 文件，在弹出的网页里展示可视化结果。下图是各函数耗时的冰柱图（icicle）：最顶层的长条矩形是被 cProfile.Profile 包裹的 main 函数，下一层则将上一层的矩形细分为不同颜色的子矩形，对应于 main 的函数体中被调用的各种函数，矩形长度正比于耗时。再下一层又会细分上一层的函数调用，如此不断深入，直至某个函数仅由内置语句和函数组成，或调用了其它语言的库文件。最后图片会自上而下形成一个个小山丘，又似冬天屋檐上结成的冰柱，各函数的耗时情况一目了然。Snakeviz 的网页里可以用鼠标点击感兴趣的矩形，会以这个函数为顶层放大冰柱图的细节，之后可以点击左边的 Reset Zoom 按钮返回初始状态。\n网页下半部分还有表格结果，点击列名以排序，点击行以做出该函数为顶层的冰柱图。这样一来连打印表格的 Profile.print_stats 都可以省去了。\n除 snakeviz 外，还有 gprof2dot、flameprof 等工具可选。\nline_profiler 模块 cProfile 只能监测函数和方法的耗时，并且会深入到最底层的调用，有时我们并不需要如此详细的信息，只是想了解每一行的耗时。这种情况下第三方模块 line_profiler 可能会更合适，正如它的名字所示，测量的是函数体每一行语句的耗时。同样是 pip 或 conda 安装。\n程序内调用 line_profiler.LineProfiler 只提供对函数的测量功能，在构造时需要传入目标函数，也可以后面再添加\nfrom line_profiler import LineProfiler profile = LineProfiler(f, g) profile.add_function(h)  常用的方法有：\n run(cmd)：测量一条语句的耗时。但如果语句中不含对目标函数的调用，那就啥也测不到。 runctx(cmd, globals, locals)：可以传入环境的 run。 runcall(func, *args, **kw)： 测量 func(*args, **kw) 耗时。当然前提是之前添加过 func 函数。 enable_by_count()：据文档说是解决了嵌套安全性的 enable。 disable_by_count()：同上。 print_stats(self, stream=None, output_unit=None, stripzeros=False)：打印结果，stream 指定输出流（默认屏幕），output_unit 指定结果中的时间单位（后面再解释），stripzeros 指定是否隐藏未被调用的函数。 dump_stats(self, filename)：以二进制格式导出文件。  以 main 函数为例\nif __name__ == '__main__': profile = LineProfiler(main) profile.enable_by_count() main() profile.disable_by_count() profile.dump_stats('main.lprof')  生成的二进制文件通过下面的命令打印\npython -m line_profiler main.lprof  Timer unit: 1e-06 s Total time: 60.5731 s File: D:\\code\\python\\profiler\\test.py Function: main at line 34 Line # Hits Time Per Hit % Time Line Contents ============================================================== 34 def main(): 35 2 87.8 43.9 0.0 with open('河北省.json', encoding='utf-8') as f: 36 1 895.7 895.7 0.0 geoj = json.load(f) 37 1 1.4 1.4 0.0 hebei = geoj['features'][0]['geometry'] 38 39 1 0.7 0.7 0.0 xs = [] 40 1 0.5 0.5 0.0 ys = [] 41 201 295.5 1.5 0.0 for x in np.linspace(110, 125, 200): 42 40200 59299.5 1.5 0.1 for y in np.linspace(35, 45, 200): 43 40000 60157225.0 1503.9 99.3 if contain(hebei, x, y): 44 5226 9045.1 1.7 0.0 xs.append(x) 45 5226 3818.1 0.7 0.0 ys.append(y) 46 47 1 188088.4 188088.4 0.3 fig, ax = plt.subplots() 48 1 34.6 34.6 0.0 ax.set_aspect('equal') 49 1 1567.8 1567.8 0.0 ax.plot(xs, ys, 'o', c='C3', ms=0.2) 50 1 145367.5 145367.5 0.2 fig.savefig('hebei.png', dpi=200, bbox_inches='tight') 51 1 7339.0 7339.0 0.0 plt.close(fig)   Line：行号。 Hits：这一行被“命中”（执行）了几次。 Time：这一行的总耗时。乘上开头的时间单位 Timer unit 后才是真实数值。 Per Hit：Time / Hits 的值，表示每次执行的平均耗时。 % Time：百分数形式的 Time。 Line Contents：这一行语句的内容。  因为表格内容非常清晰易懂，所以不需要做什么可视化，并且也不会像 cProfile 那样出现一堆见都没见过的函数。令人大跌眼镜的是，被测量的 main 函数耗时从 16 秒暴增至 60 秒。暗示循环语句太多时测量会严重拖慢程序，幸好我们更关心的是 % Time 列的内容，即哪一行占的时间最多。显然 contain 占据了 99.3 %，这提示我们 contain 函数应该是优化的首要对象。\n上面的用法还可以改写为\nif __name__ == '__main__': profile = LineProfiler(main) profile.runcall(main) profile.print_stats()  然后用管道输出文本结果，这样就可以省去中间的 test.lprof 文件和 python -m 命令。\n装饰器 LineProfiler 类的实例可以直接用作装饰器，会自动通过 add_function 方法添加目标函数。例如\nprofile = LineProfiler() @profile def contain(polygon, x, y): \u0026lt;函数体省略\u0026gt; if __name__ == '__main__': main() profile.print_stats()  另外也可以自己实现一个保存结果的装饰器\ndef lprofiler(filename): '''line_profiler的装饰器. 保存结果到指定路径.''' def decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): profile = LineProfiler(func) result = profile.runcall(func, *args, **kwargs) profile.dump_stats(filename) return result return wrapper return decorator  不过自己写的版本有个毛病，反复调用被装饰的函数会重写结果文件，导致无法测量累积用时。\nkernprof kernprof 是 line_profiler 附带的一个脚本，作用是简化调用 LineProfiler 的流程，自动输出结果文件。在命令行里用它代替 Python 命令执行脚本时，会先构造一个名为 profile 的 LineProfiler 对象，并将其注入脚本的作用域。这样一来脚本开头无需添加 import 语句就能直接引用 profile 变量，接着用它装饰目标函数即可。这也是 line_profiler 文档的推荐用法。例如在脚本中加上一行\n@profile def main(): \u0026lt;函数体省略\u0026gt;  也不需要写什么 profile.dump_stats(filename)，直接在命令行执行\nkernprof -l test.py  就会在当前目录自动生成 test.py.lprof 文件。kernprof 的详细用法可以用 -h 选项查看，下面列举几条常用的：\n -l, --line-by-line：给出时使用 line_profiler，否则使用 cProfile。 -b, --builtin：是否把 profile 注入脚本。-l 时默认开启。 -o OUTFILE, --outfile OUTFILE：将结果输出至 OUTFILE 文件。不给出时默认以 脚本名.prof 或 脚本名.lprof 为名保存到当前目录。 -v, --view：是否顺便打印出结果。  其中 -l 选项提到了可以使用 cProfile，kernprof 对 cProfile.Profile 进行了包装，为其增加了装饰器功能，用法跟前面一样。以测量代码块为例\nwith profile: \u0026lt;代码块省略\u0026gt;  不加 -l，记得加 -b\nkernprof -b test.py  会在当前目录自动生成 test.py.prof 文件，接着可以交给 snakeviz 去可视化。\n结语 由简到繁总结一下上述工具的使用场景：\n 测量脚本用时：系统命令或 IPython 的 %run。 测量函数用时：IPython 的 %time、%timeit。 测量代码块用时：time 模块。 测量函数每一行的用时：line_profiler 模块。 测量函数或代码块所有调用的用时：cProfile 模块。  因笔者对 Jupyter Notebook 不熟，所以没能介绍 %time 和 %timeit 的 cell 版本。另外也没能测试多进程下各工具的表现，专门提到这个是因为笔者在实际操作中经历过 line_profiler 和 kernprof 在多进程脚本中失效的现象。以后有机会再总结一下这些内容。\n参考资料 Point-In-Polygon Algorithm — Determining Whether A Point Is Inside A Complex Polygon\nWindows equivalent to UNIX \u0026ldquo;time\u0026rdquo; command\nIPython: Built-in magic commands\nPython Docs: time\nUnderstanding time.perf_counter() and time.process_time()\n关于python中time.perf_counter() 与 time.process_time()分析与疑问\nPython Cookbook: 9.6 带可选参数的装饰器\nPython Docs: The Python Profilers\nSNAKEVIZ\npyutils/line_profiler\nPython优化第一步: 性能分析实践\n好用的 Python Profile（性能/耗时分析）工具\n","date":"2022-08-03","permalink":"https://zhajiman.github.io/post/python_measure_time/","tags":["python"],"title":"Python 系列：测量程序的运行时间"},{"content":"二维平面上一系列点的坐标由 x 和 y 数组描述，同时准备一个形状相同的 mask 数组。若第 i 个点落入了平面上一个多边形的内部，则令 mask[i] = True；若在多边形外，则令 mask[i] = False。由此得到的 mask 数组即掩膜（mask）数组，它可以作为布尔索引分出多边形内外的点\nx_in, y_in = x[mask], y[mask] x_out, y_out = x[mask], y[mask]  它可以作为掩膜，掩盖多边形范围外的值——即把外面的值设为 NaN，以便进行后续的计算\nz[~mask] = np.nan z_mean = np.nanmean(z)  下图展示了两个应用：左小图的多边形是一个中心带洞的正方形，给定一系列散点的坐标，计算出掩膜后可以把多边形内的散点画成红色，多边形外的散点画成蓝色；右小图的多边形是中国全域，给定 (50, 50) 形状的经纬度网格，计算出掩膜后用橙色画出掩膜为 True 的部分，这张掩膜之后可以用来处理网格上的其它变量。\n本文的目的是介绍如何用 Python 制作掩膜数组，并尽量优化其运行时间。从 shapefile 中读取中国国界并转化为 Shapely 中的多边形对象等操作，已经在博文 Cartopy 系列：探索 shapefile 中详细介绍过了，本文是对其的一个补充。\n基本思路 首先准备多边形和测试用的坐标点。多边形使用中国国界，通过 cnmaps 包的 get_adm_maps 函数获取\nfrom cnmaps import get_adm_maps china = get_adm_maps(level='国', record='first', only_polygon=True)  由此得到的 china 是 MapPolygon 对象，继承自 Shapely 的 MultiPolygon 对象，即中国由很多个多边形组成（大陆和海岛）。MultiPolygon.contains 方法可以用来检查另一个 Shapely 的几何对象是否被多边形所包含。对于坐标点来说，要求点落入多边形内部，恰好落在多边形的边界上并不算数。\n坐标点选用覆盖中国范围的网格\nimport numpy as np lonmin, lonmax = 60, 150 latmin, latmax = 0, 60 npt = 50 x = np.linspace(lonmin, lonmax, npt) y = np.linspace(latmin, latmax, npt) x, y = np.meshgrid(x, y)  生成掩膜数组的思路非常简单：循环遍历 x 和 y，每个点对应一个 Shapely 的 Point 对象，调用 MultiPolygon.contains 方法检查点是否落入多边形中，最后返回收集好的结果。代码如下\nimport shapely.geometry as sgeom def polygon_to_mask(polygon, x, y): '''生成落入多边形的点的掩膜数组.''' x = np.atleast_1d(x) y = np.atleast_1d(y) mask = np.zeros(x.shape, dtype=bool) # 判断每个点是否落入polygon, 不包含边界. for index in np.ndindex(x.shape): point = sgeom.Point(x[index], y[index]) if polygon.contains(point): mask[index] = True return mask  其中 np.ndindex 是用来遍历多维数组的迭代器类，简单理解一下就是用了它就可以少写多重循环。使用方法很简单\nmask = polygon_to_mask(china, x, y)  计时发现，该函数对单个点需要 57 毫秒，对 10 * 10 = 100 个点需要 3 秒，对 50 * 50 = 2500 个点需要 88 秒。显然这个速度太慢了，假设耗时与点数成线性增长关系，对于 70° - 140°E，10° - 60°N，分辨率为 0.25° 的 ERA5 格点数据，恐怕要跑 50 分钟以上。结论是，当点数只有几十个时 polygon_to_mask 还能用用，几百个点以上时该函数基本没有实用价值。\n利用 shapely.prepared.prep 进行优化 在翻阅 Shapely 的文档时我注意到了 Prepared Geometry Operations 一节，提到使用 shapely.prepared.prep 函数将几何对象转为“准备好了”（prepared）的对象后，能加速 contains 和 intersects 等方法的批处理。于是 polygon_to_mask 有了第二个版本\nfrom shapely.prepared import prep def polygon_to_mask(polygon, x, y): '''生成落入多边形的点的掩膜数组.''' x = np.atleast_1d(x) y = np.atleast_1d(y) mask = np.zeros(x.shape, dtype=bool) # 判断每个点是否落入polygon, 不包含边界. prepared = prep(polygon) for index in np.ndindex(x.shape): point = sgeom.Point(x[index], y[index]) if prepared.contains(point): mask[index] = True return mask  相比第一个版本，函数体几乎只有一行的改动。这次单个点耗时 14 毫秒，10 * 10 = 100 个点耗时 0.02 秒，50 * 50 = 2500 个点耗时 0.06 秒。速度可以说提升了两到三个数量级，作为 Python 函数来说终于有了实用性。不过对于 1000 * 1000 = 1e6 个点还有些吃力，需要 10 秒。\n利用递归分割进行优化 在找到 prepared 模块前我曾在 Github Gist 上看到了 perrette 设计的 shp_mask 函数，他的思路是：\n 先确定坐标点的边界框（形如 (xmin, xmax, ymin, ymax) 的矩形区域）。 如果边界框在多边形外，这些坐标点对应的掩膜直接设为 False。 如果边界框被多边形包含，这些坐标点对应的掩膜直接设为 True。 如果边界框与多边形相交，将边界框等分成四个子区域，对每个子区域递归应用上面的流程。 如果某层递归只传入了单个点，直接返回多边形与这个点的包含关系。  于是我想能不能借鉴递归分割的思路，同时加上 prepared 的加速效果。shp_mask 函数接受的 x 和 y 要求是张成网格的一维坐标，不过我希望 polygon_to_mask 接受的 x 和 y 不一定非得是网格坐标，无序摆放的散点也可以。按这个要求修改后得到第三个版本\nimport math def polygon_to_mask(polygon, x, y): '''生成落入多边形的点的掩膜数组.''' x = np.atleast_1d(x) y = np.atleast_1d(y) if x.shape != y.shape: raise ValueError('x和y的形状不匹配') prepared = prep(polygon) def recursion(x, y): '''递归判断坐标为x和y的点集是否落入多边形中.''' xmin, xmax = x.min(), x.max() ymin, ymax = y.min(), y.max() xflag = math.isclose(xmin, xmax) yflag = math.isclose(ymin, ymax) mask = np.zeros(x.shape, dtype=bool) # 散点重合为单点的情况. if xflag and yflag: point = sgeom.Point(xmin, ymin) if prepared.contains(point): mask[:] = True else: mask[:] = False return mask xmid = (xmin + xmax) / 2 ymid = (ymin + ymax) / 2 # 散点落在水平和垂直直线上的情况. if xflag or yflag: line = sgeom.LineString([(xmin, ymin), (xmax, ymax)]) if prepared.contains(line): mask[:] = True elif prepared.intersects(line): if xflag: m1 = (y \u0026gt;= ymin) \u0026amp; (y \u0026lt;= ymid) m2 = (y \u0026gt;= ymid) \u0026amp; (y \u0026lt;= ymax) if yflag: m1 = (x \u0026gt;= xmin) \u0026amp; (x \u0026lt;= xmid) m2 = (x \u0026gt;= xmid) \u0026amp; (x \u0026lt;= xmax) if m1.any(): mask[m1] = recursion(x[m1], y[m1]) if m2.any(): mask[m2] = recursion(x[m2], y[m2]) else: mask[:] = False return mask # 散点可以张成矩形的情况. box = sgeom.box(xmin, ymin, xmax, ymax) if prepared.contains(box): mask[:] = True elif prepared.intersects(box): m1 = (x \u0026gt;= xmid) \u0026amp; (x \u0026lt;= xmax) \u0026amp; (y \u0026gt;= ymid) \u0026amp; (y \u0026lt;= ymax) m2 = (x \u0026gt;= xmin) \u0026amp; (x \u0026lt;= xmid) \u0026amp; (y \u0026gt;= ymid) \u0026amp; (y \u0026lt;= ymax) m3 = (x \u0026gt;= xmin) \u0026amp; (x \u0026lt;= xmid) \u0026amp; (y \u0026gt;= ymin) \u0026amp; (y \u0026lt;= ymid) m4 = (x \u0026gt;= xmid) \u0026amp; (x \u0026lt;= xmax) \u0026amp; (y \u0026gt;= ymin) \u0026amp; (y \u0026lt;= ymid) if m1.any(): mask[m1] = recursion(x[m1], y[m1]) if m2.any(): mask[m2] = recursion(x[m2], y[m2]) if m3.any(): mask[m3] = recursion(x[m3], y[m3]) if m4.any(): mask[m4] = recursion(x[m4], y[m4]) else: mask[:] = False return mask return recursion(x, y)  运行时间如下图所示\n蓝色柱状图对应于加了 prepared 的循环版，橙色柱状图对应于递归版。当点数小于 50 * 50 = 2500 时，递归版反而更慢；而当点数达到 1e5 量级时，递归版的优势就非常显著了。例如对于 (1000, 1000) 的输入，循环版耗时 10.4 秒，递归版耗时 0.5 秒；对于 (5000, 5000) 的输入，循环版耗时 290.5 秒，递归版耗时 6.8 秒。不过我还没看内存占用的差异，估计递归所需的内存会高一些，感兴趣的读者可以用 memory_profiler 包测试一下。\n总结 对于 (1000, 1000) 形状的输入，我们通过优化，使 polygon_to_mask 函数的耗时从 57000 秒（理论上）缩短到 10 秒，再缩短到 0.5 秒，一共加快约 114514 倍，可以说非常惊人了。不过我觉得可能还有优化的余地，例如对于 MultiPolygon，可以先计算每个成员 Polygon 的掩膜数组，收集起来叠加成 masks 数组，最后通过 mask = np.any(masks, axis=0) 合并掩膜。当然追求极致效率的读者可以了解一下 GDAL 库里的 gdal_rasterize 命令。\n","date":"2022-07-31","permalink":"https://zhajiman.github.io/post/cartopy_polygon_to_mask/","tags":["cartopy","shapely"],"title":"Cartopy 系列：利用多边形生成掩膜数组"},{"content":"前言 Cartopy 可以通过 feature 模块向地图添加国界 BORDER 和省界 STATES，因其底层采用的 Natural Earth 地图数据并不符合我国的政治主张，所以我们经常需要自备 shapefile 文件来画中国省界，以下面的代码为例\nimport matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.io.shapereader as shpreader extents = [70, 140, 0, 60] crs = ccrs.PlateCarree() fig = plt.figure() ax = fig.add_subplot(111, projection=crs) ax.set_extent(extents, crs) filepath = './data/bou2_4/bou2_4p.shp' reader = shpreader.Reader(filepath) geoms = reader.geometries() ax.add_geometries(geoms, crs, lw=0.5, fc='none') reader.close() plt.show()  图就不放了，这段代码足以应付大部分需要画省界的情况。然而我在无脑粘贴代码的过程中逐渐产生了疑惑：为什么 shapefile 会由三个文件组成？省界是以何种形式存储在文件中？Cartopy 和 Matplotlib 又是怎样将省界画出来的？调查一番源码后总结出了这段代码底层实现的流程：\n 利用 PyShp 包读取 shapefile 文件中的每个形状。 利用 Shapely 包将形状转换为几何对象。 利用 Cartopy 包将几何对象投影到地图所在的坐标系上。 用投影后的坐标构造 Matplotlib 的 Path 对象，最后画在地图上。  本文的目的即是从头到尾解说一下这段流程，希望加深对 shapefile 格式，Matplotlib 和 Cartopy 包的理解。令人意外的是，随着探索的不断深入，我发现自己自然而然地学会了如何实现省份填色、省份合并，地图白化等，以前看起来十分困难的操作。本文也会一并介绍这些应用。\nShapefile 简介 首先要解答的问题是：shapefile 是什么，里面装的什么东西，并且是怎么摆放的。Shapefile 是 ESRI 公司为旗下的 GIS 软件设计的一种存储矢量地理特征（feature）的格式。地理特征由形状（shape）和属性（attribute）构成。形状是一个二维平面图形，可以概括为点（point）、线（line）和面（area）三种，一对 x 和 y 的坐标值构成点，多个点按顺序相连构成线，若一条线能首尾相连绕出一个多边形则构成面，如下图所示（引自 Vector 00: Open and Plot Shapefiles in R）\n一个形状绑定一条属性，属性由若干个字段（field）组成，每个字段含有一个值（字符串、数字等）。特征的例子包括：一个气象站点的位置需要用点来表示，对应的属性字段可以是站点的名称和编号；一条河需要用线来表示，对应的属性字段可以是河流的名称和长度；一个省需要用面来表示，对应的属性字段可以是省名、行政区号、周长和面积。一系列特征可以按顺序摆放在 shapefile 文件中，并且要求单个文件中所有特征的形状类型必须相同。于是乎，一个 shapefile 文件可以表示全国所有气象站点的位置，或者一片水系中所有河流的位置，抑或是全国省份的形状等。\nShapefile 的特殊之处在于，一个完整的 shapefile 文件实际上是由同目录下一系列同名但扩展名不同的子文件组成的，其中必须有的三个是：\n .shp：存储形状的二进制文件。其中每个形状占据的字节数是可变的。 .shx：存储 .shp 中每个形状从第几个字节开始的二进制文件。用于加快对 .shp 文件的索引操作。 .dbf：存储属性的 dBASE IV 格式文件。每条属性与 .shp 中的每个形状一一对应。  可选的另有：\n .prj：WKT 格式的坐标系信息。只有数行的文本文件。 .cpg：存有 .dbf 中字符编码格式的文本文件。 ……  其实可选的子文件还有很多，但因为并不常见所以不再细述，详见 维基。缺少 .prj 文件时，GIS 软件通常会将 .shp 中形状的坐标解读为 EPSG:4326（WGS84）坐标系中的经纬度。这解释了为什么网上下载到的 shapefile 文件通常只由三个文件组成。\n接下来着重介绍 .shp 和 .dbf 中的内容。.shp 中只含两种数据类型：32-bit 整数和 64-bit 双精度浮点数，整数主要用来表示序号和下标之类的量，而浮点数主要用来表示形状的坐标值，并且不允许出现无限（infinity）和缺测（NaN）的浮点数。.shp 的结构组织如下图所示\n.shp 文件开头前 100 个字节用来存放文件头信息（file header），后面跟着一条又一条的记录（record），每条记录又由记录头信息（record header）和记录内容（record content）组成。文件头信息中比较重要的是形状类型（shape type）和边界框（bounding box）。前文虽然说形状分为点、线、面三种，但实际上可以细分为 14 种，如下表所示\n   Value Shape Type     0 Null Shape   1 Point   3 PolyLine   5 Polygon   8 MultiPoint   11 PointZ   13 PolyLineZ   15 PolygonZ   18 MultiPointZ   21 PointM   23 PolyLineM   25 PolygonM   28 MultiPointM   31 MultiPatch    文件头信息中存储的是形状类型对应的整数代码。虽然类型看起来很多，但一般只会用到 Point、MultiPoint、PolyLine 和 Polygon 这四种，从名字上就能看出直接对应于点、线、面。其它类型的话，Null Shape 表示一个占位的空形状，没有坐标值；带有 M 后缀的类型表示除了坐标值 X 和 Y 以外，还会存储一个测量值（measurement）M，并且这个值允许缺测；带有 Z 后缀的类型在 X、Y 和 M 的基础上又加入了垂直方向上的坐标 Z；MultiPatch 类型甚至能表示建筑物那样的立体图形。边界框定义为能恰好框住文件中所有形状的方框，以 Xmin、Ymin、Xmax 和 Ymax 的形式存储。\n记录头信息存有每条记录的编号（从 1 开始）和记录内容的长度，一般用不上。重点还是在于记录内容，因为对地理特征来说最重要的形状特征就存储在记录内容里。首先，下图是 Point 的记录内容\n记录中存有三个数值：形状类型的代码、坐标值 X 和 Y。之前说过文件中要求所有形状的类型是相同的，所以这里的代码与文件头信息中的一样都为 1。在缺少 .prj 文件的情况下，X 和 Y 通常用来表示经度和纬度。然后是 MultiPoint 的记录内容\nMultiPoint 顾名思义，即许多 Point 的组合，一条 MultiPoint 的记录表示一簇点。记录内容里的 Box 即刚好包住这些点的边界框，由四个浮点数组成所以占 32 个字节；NumPoints 表示一共有几个点；Points 即所有点的坐标，以 [X1, Y1, X2, Y2, X3, Y3, ...] 的形式排列。接着是代表线的形状 PolyLine\nPolyLine 顾名思义是折线，一条折线由至少两个点依次相连而成。但实际上，一条 PolyLine 记录可以表示数条相互独立的折线，每条线称作一个片段（part），片段间可以相交，片段数由 NumParts 描述。NumPoints 表示这些折线的点数之和，Points 是所有点的坐标。为了区分出 Points 中的点都属于哪一片段，又引入了 Parts 数组，标识出每个片段的第一个点对应于 Points 中的第几个点（从 0 开始计数）。举个例子，片段一由两个点组成，片段二由三个点组成，那么片段一满足 Parts[0] = 0，片段二满足 Parts[1] = 2，片段三满足 Parts[2] = 5。最后是最复杂的代表面的 Polygon\nPolygon 顾名思义是多边形，一个多边形由一个环（ring）绕成。所谓环即首尾闭合的折线，要求折线的第一个点与最后一个点是相同的，所以环至少由四个点组成，还要求点与点之间的线段不能发生交叉。然而，多边形是可以有洞的，以甜甜圈为例，外部的圆环与内部的圆环共同绕出了甜甜圈的区域，所以一个多边形可以由一个外环和多个内环（多个洞）组成。为了区分外环内环，人为规定外环沿顺时针方向绕行，内环沿逆时针方向绕行。跟 PolyLine 类似的是，一条 Polygon 记录也可以含有复数个多边形，所以 Polygon 同样存在片段的概念，不过一个片段对应一个环，而非一个片段对应一个独立的多边形。\nNumParts 即环的个数，NumPoints 是所有环的点数，Points 是依次排列的所有环的坐标，Parts 则标识每个环的第一个点对应于 Points 中的第几个点。值得注意的是，shapefile 并不关心环的顺序，你可以把所有外环的坐标排列在一起，再把所有内环的坐标坐标排列在一起，某种意义上来说 shapefile 只记录有没有洞，而并不关心洞到底开在哪个多边形上。下面这个带洞正方形的例子应该能形象说明 Polygon 的记录内容\n顺带一提，因为 MutliPoint、PolyLine 和 Polygon 的记录内容可以含有任意个点，所以记录的长度是不定的。.shx 文件的意义便是通过存储这些记录的起始点相对于文件开头的字节偏移量，帮助 GIS 软件快速定位 .shp 中每条记录的位置。\n介绍完 .shp，接下来介绍存储属性的 .dbf。.dbf 基于 dBASE IV，是一种互联网早期的数据库格式，这里不打算细讲其规范（我也偷懒没看），只是简单演示一下如何快速查看其内容。以 bou2_4p.dbf 为例， 据说这是国家地理信息系统很久以前公开发布的文件，如今已无官方下载渠道，但可以在网上简单下载到，例如在 GitHub 上一搜就是一把。bou2_4p 中 bou 表示边界（boundary），数字 1 ~ 4 代表国家、省、市、县的四级行政划分，这里的 2 就表示省界，4 表示比例为 400 万分之一，p 猜测是多边形的意思（参考 中国地图GIS的官方数据（shp文件））。.dbf 文件直接拖入 Excel 便能打开，内容如下图所示\n这样看来，所谓 .dbf 不过就是一张二维表格而已。图中 7 列代表 7 个字段，最后两列较为重要：行政区划代码（adcode）和对应的省份名。每条记录对应 .shp 中的一个形状，图中辽宁省的记录出现了十几次，是因为辽宁省含有许多海岛，每个海岛作为单独的一个形状被记录到了 .shp 和 .dbf 中。继续拖动鼠标滚轮会发现沿海省份都是这个样子。之前提到过，一条 Polygon 记录中可以存储复数个多边形，所以也有些 shapefile 会把本土和海岛合并成单条记录。继续查看表格会看到香港特别行政区上面有一条区划代码为 0，名称空着的记录，在 QGIS 中打开后发现原来这是表示香港行政区划的记录，所以多边形覆盖到了海面上，如下图所示\n把表格看完后你应该会发现，这里面居然没有澳门特别行政区（820000）。澳门回归是在 1999 年 12 月 20 日，而表中区划代码的字段名 ADCODE99 中刚好有一个 99，让人不禁怀疑这个 shapefile 怕不是用的 99 年初的数据。总之，提前用 Excel 查看一下 .dbf 中的内容，有助于我们理解 shapefile 中存储的内容，便于之后进行更复杂的操作。\n最后提一下 shapefile 作为一种地理矢量特征存储格式的优缺点：\n 优点：文件结构简单，读取后画图速度快，应用非常广泛。 缺点：不含几何拓扑信息，子文件太多，dBASE 格式限制较多。  本节的内容基本摘自 ESRI 关于 shapefile 的 技术文档，非常推荐想深入了解 shapefile 格式的同学读读这个文档。dBASE 格式的规范见 Xbase File Format Description。\n用 PyShp 读写 shapefile 前面属于是纸上谈兵环节，本节就来用 Python 实际操作一下 shapefile。目前主流的能读写 shapefile 的包有四个：PyShp、GDAL、Fiona 和 GeoPandas。其中 PyShp 是纯 Python 实现，没有任何依赖；GDAL 中的 OGR 能够进行矢量数据的 IO 操作，因为底层是 C++ 所以速度更快；Fiona 对 OGR 的 API 做了 Python 风格的封装；而 GeoPandas 又对 Fiona 进行了封装。因为 GDAL 太难装了，并且 Cartopy 也默认使用 PyShp，所以这里基于 PyShp 进行演示，版本为 2.3.0。\nPyShp 在程序中的名字就叫 shapefile，用 Reader 类表示文件，用 Shape 类表示形状，用 Record 类表示属性，用 ShapeRecord 类表示同时含有形状和属性的一条记录。下面以 bou2_4p.shp 文件为例\nimport shapefile # 文件名可以不加后缀. # 默认用UTF-8解码属性, 而bou2_4p的编码是GBK. reader = shapefile.Reader('bou2_4p', encoding='gbk') # 打印文件头信息. print(reader.numRecords, reader.numShapes) print(reader.shapeType, reader.shapeTypeName) print(reader.bbox)  输出为\n925 925 5 POLYGON [73.44696044921875, 6.318641185760498, 135.08583068847656, 53.557926177978516]  即该文件含 925 条多边形（类型代码为 5）的记录，边界框围绕中国。\n读取形状\n# 读取第一个形状. shape = reader.shape(0) # 获取含所有形状的序列. shapes = reader.shapes() shape = shapes[0] # 迭代器版本的shapes. for shape in reader.iterShapes(): pass # 打印形状的头信息. print(shape.oid) print(shape.bbox) print(shape.shapeType, shape.shapeTypeName) # 打印形状内容. print(shape.parts) print(shape.points)  输出为\n924 [114.2989273071289, 22.175479888916016, 114.30238342285156, 22.17897605895996] 5 POLYGON [0] [(114.2989273071289, 22.17812156677246), (114.3006362915039, 22.17891502380371), (114.30207061767578, 22.17897605895996), (114.30238342285156, 22.178001403808594), (114.30198669433594, 22.176328659057617), (114.3009033203125, 22.175479888916016), (114.2995376586914, 22.176170349121094), (114.2990493774414, 22.177350997924805), (114.2989273071289, 22.17812156677246)]  oid 为 924 即最后一个形状。parts 为 [0] 表示这个多边形仅由一个环组成。points 是 (x, y) 点对构成的列表，可以看到起点和终点是同一个点，并且绕行方向为顺时针，说明是外环。\n读取属性的方法类似\n# 读取第一条属性. record = reader.record(0) # 获取含所有属性的序列. records = reader.records() record = records[0] # 迭代器版本的records. for record in reader.iterRecords(): pass # 打印属性字段和数值. print(reader.fields) print(record.as_dict())  输出为\n[('DeletionFlag', 'C', 1, 0), ['AREA', 'N', 12, 3], ['PERIMETER', 'N', 12, 3], ['BOU2_4M_', 'N', 11, 0], ['BOU2_4M_ID', 'N', 11, 0], ['ADCODE93', 'N', 6, 0], ['ADCODE99', 'N', 6, 0], ['NAME', 'C', 34, 0]] {'AREA': 0.0, 'PERIMETER': 0.011, 'BOU2_4M_': 926, 'BOU2_4M_ID': 3115, 'ADCODE93': 810000, 'ADCODE99': 810000, 'NAME': '香港特别行政区'}  为了解读输出内容，需要了解一下 PyShp 中字段的表示。字段由四个元素的序列描述，元素依次为：\n Field name：字符串表示的字段名。 Field type：字段类型，可以细分为：  'C'：字符型。 'N'：数值型，因为实际上保存为字符所以整数或浮点数都行。 'F'：浮点型，同 'N'。 'L'：逻辑型，用来表示真假值。 'D'：日期型。 'M'：备忘录型，在 shapefile 中用不到。   Field length：表示字段数值的字符的长度。 Decimal length：'N' 或 'F' 型中小数部分的位数。  所以行政区划代码的字段表示为 ['ADCODE99', 'N', 6, 0]，即一个六位整数；而省名的字段表示为 ['NAME', 'C', 34, 0]，即最长 34 个字符的字符串。然而测试后发现 field length 实际上表示字节数，对于 GBK 或 UTF-8 编码来说并不等同于字符数。值得注意的是，字段中有一个奇怪的 DeletionFlag，表示一条记录已经被删除但还未从文件中移除。这个字段几乎没有用处，只是在创建 shapefile 时会自动生成，所以我们直接忽略即可。具体某个字段的值可以以属性或字典的风格来提取：record['NAME'] 和 record.NAME。而上面的代码中则是通过 record.as_dict() 以字典的形式直接显示出所有字段的值。\n除了 Shape 和 Record，还可以通过 shapeRecord 同时获取形状和属性，同样有 shapeRecord、shapeRecords 和 iterShapeRecords 三种方法。下面是一个筛选河北省记录的例子\nshapeRecs = [] for shapeRec in reader.iterShapeRecords(): if shapeRec.record['NAME'] == '河北省': shapeRecs.append(shapeRec) print(len(shapeRecs))  输出为 9，即 bou2_4p 中有 9 条河北省相关的记录。\n接着简单演示一下如何用 PyShp 创建新文件\nimport numpy as np def make_circle(xc, yc, r, npt=100): '''创建一个逆时针绕行的圆.''' theta = np.linspace(0, 2 * np.pi, npt) x = r * np.cos(theta) + xc y = r * np.sin(theta) + yc xy = np.column_stack((x, y)) return xy # 需要将数组转为列表. ring = make_circle(1, 1, 1).tolist()[::-1] hole = make_circle(1, 1, 0.5).tolist() # 如果首尾不相连, PyShp会在末尾自动补上起点. rect = [[3, 0], [3, 2], [5, 2], [5, 0], [3, 0]] # 采用默认的UTF-8编码. writer = shapefile.Writer('donut') # 仅设置名称字段, 并且采用默认的字段长度. writer.field('name', 'C') # 写入甜甜圈形状. writer.poly([ring, hole]) writer.record('donut') # 写入方框形状. writer.poly([rect]) writer.record('rectangle') writer.close()  PyShp 用 Writer 类创建文件对象，用 field 方法逐个设置字段，参数就是前文提到的四个元素。再用 null、point、multipoint、line 和 poly 等方法写入形状，同时每个形状必须对应一条 record 方法写入的记录。结果是生成了 donut.shp、donut.shx 和 donut.dbf 三个文件，含有甜甜圈和矩形两个记录。用 GIS 软件打开便能直接验证新建的文件是否正确，不过下节将会介绍如何用 Matplotlib 来绘制其形状。\n本节只是一个简单的介绍，PyShp 相关的更多操作请参考其 主页。\n用 Matplotlib 绘制 shapefile 对于 Point 类型的 shapefile，可以有\n# 这里的x和y是标量. for shape in reader.iterShapes(): x, y = shape.points[0] ax.scatter(x, y)  对于 MultiPoint 类型的 shapefile，可以有\n# 这里的x和y是数组. for shape in reader.iterShapes(): x, y = zip(*shape.points) ax.scatter(x, y)  对于 PolyLine 类型的 shapefile，需要根据 parts 里存储的下标，将 points 划分为相互独立的折线\nfor shape in reader.iterShapes(): for i, start in enumerate(shape.parts): try: end = shape.parts[i + 1] except IndexError: end = None x, y = zip(*shape.points[start:end]) ax.plot(x, y)  对于 Polygon 类型，固然可以用 ax.plot 方法用线表示多边形的环，但如果想实现颜色填充的效果的话，需要引入 matplotlib.path 模块中的 Path 类。Path 直译为路径，由一系列顶点（vertices）和控制顶点处连线的代码（code）构成。Path 本身并非 Aritist 对象，但将 Path 转为 PathPatch 或 PathCollection 对象后便可以画在 Axes 上，路径连线的粗细和颜色，路径环绕出的区域内的填色等要素都可以进行设置。Matplotlib 中用来画方框、椭圆、多边形等图形的 Patch 类便是基于 Path 实现。有人可能会问，那为什么不直接使用 matplotlib.patches.Polygon 来画 shapefile 呢？这是因为这个类不能区分出多边形中的洞，而底层的 Path 却可以。\n下面是 Path 相关的控制代码，以类属性的形式引用\n   Code Vertices Description     MOVETO 1 结束上一条折线，提起画笔从当前顶点开始画。   LINETO 1 从上一个顶点画直线到当前顶点。   STOP 1 (ignored) 忽略当前顶点，连线停留在上一个顶点。   CLOSEPOLY 1 (ignored) 忽略当前顶点，从上一个顶点画直线到当前折线的起点以实现闭合。    除此之外还有指定二次贝塞尔曲线的 CURVE3 和三次贝塞尔曲线的 CURVE4，不过因为和本文的主题无关，所以这里省略掉了。类似 shapefile 的 PolyLine，一条路径可以由多条折线构成，只要用控制代码标识出折线的分组即可。当折线首尾相连成环，或者每条折线的最后一个顶点都设置成 CLOSEPOLY 时，即可用路径来描述多边形。Shapefile 中要求多边形的外环沿顺时针方向，内环沿逆时针方向，而路径只需要内环和外环的绕行方向不同即可（参考 Matplotlib 的甜甜圈示例）。以一个镂空的正方形为例，路径需要描述两段方向相反的环\nfrom matplotlib.path import Path verts = [ (0, 0), (0, 3), (3, 3), (3, 0), (0, 0), # 顺时针外环. (1, 1), (2, 1), (2, 2), (1, 2), (1, 1) # 逆时针内环. ] codes = [ Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY, Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY ] path = Path(verts, codes)  最后以上一节的 donut.shp 文件为例\nimport shapefile import matplotlib.pyplot as plt import matplotlib.path as mpath import matplotlib.patches as mpatches def ring_codes(n): '''为长度为n的环生成codes.''' codes = [mpath.Path.LINETO] * n codes[0] = mpath.Path.MOVETO codes[-1] = mpath.Path.CLOSEPOLY return codes def polygon_to_path(shape): '''将Polygon类型的Shape转换为Path.''' if shape.shapeTypeName != 'POLYGON': raise ValueError('输入不是多边形') verts = shape.points codes = [] for i, start in enumerate(shape.parts): try: end = shape.parts[i + 1] except IndexError: end = len(verts) codes += ring_codes(end - start) path = mpath.Path(verts, codes) return path # 将Shape全部转换为Path. paths = [] with shapefile.Reader('donut') as reader: for shape in reader.iterShapes(): paths.append(polygon_to_path(shape)) # 将Path转为Patch后画在Axes上. fig, ax = plt.subplots() for i, path in enumerate(paths): color = plt.cm.tab10(i) patch = mpatches.PathPatch(path, fc=color, ec='k', lw=1) ax.add_patch(patch) # 将Axes的横纵坐标比例设为1:1 ax.set_aspect('equal') # 添加patch时Axes不会自动调节显示范围. ax.autoscale_view() plt.show()  同理 bou2_4p 也能用同样的方法画出。可能有人会问，既然能直接用 Matplotlib 画，为什么还需要 Cartopy 呢？理由是 Cartopy 提供的方法能简化读取和绘制 shapefile 的过程，不需要像上面那样对 points 手动进行分割，并且能自动处理投影坐标系间的变换。不过为此需要先引入 GeoJSON 格式和 Shapely 包。\nGeoJSON 简介 Shapely 包提供了对几何对象进行计算的功能，例如求两条线段的交点、判断点是否在多边形内、合并两个多边形得到一个大多边形，以及求两个多边形重叠的部分等。所以将 PyShp 包读取到的 shape 对象转为 Shapely 包中的几何对象（geometry），有助于进行后续的处理。但在转换过程中需要以 GeoJSON 作为中间格式，所以这一节先简单介绍一下 GeoJSON。并且 GeoJSON 在 WebGIS 领域（例如高德地图那种）非常流行，以后很可能会在工作中遇到，多了解一点也没有坏处。下文基于 2015 年的 RFC 7946 规范。\nGeoJSON 也是一种常见的存储地理空间数据的格式，通过一系列格式上的约定得以用 JSON 表现特征、形状和属性信息。GeoJSON 中的对象相当于 Python 的字典，有一个名为 type 的键，其字符串值表示对象类型。特征在 GeoJSON 中以 Feature 对象表示，一系列特征排列在数组中便构成了 FeatureCollection 对象。Feature 中含有的 geometry 对应于几何（形状）对象，properties 以字典形式存储属性字段及其数值。\nGeoJSON 定义的几何对象与 shapefile 相似，点、线、面分别用 Point、LineString 和 Polygon 对象表示。几何对象含有一个 coordinates 数组，存放所有点的坐标值，这与 shapefile 形状的 Points 数组完全一致。GeoJSON 明确使用 MultiPoint、MultiLineString 和 MultiPolygon 表示多个点、多条折线和多个多边形，对应的 coordinates 数组中嵌套存放多个形状的坐标值。因此 GeoJSON 不需要 shapefile 中表示下标的 Parts 数组，处理起来更简单。另外 GeoJSON 还提供一种 GeometryCollection 几何对象，可以存放不同类型的几何对象，但用的要相对少些。\n首尾相连的折线称作线性环（linear ring），Polygon 便是由一个或多个线性环组成，但线性环本身并不算正式定义的几何对象。GeoJSON 中的多边形同样有内外环之分，区别在于 shapefile 要求外环顺时针绕行，内环逆时针绕行，且 Points 中内外环的摆放顺序随意；而 GeoJSON 中要求外环逆时针绕行，内环顺时针绕行，且 coordiantes 中第一个子数组是外环的坐标对序列，后面的子数组才对应于内环。换句话说，GeoJSON 会记录洞和多边形的归属信息，这有助于我们后面使用 Shapely 创建多边形对象。不过 GeoJSON 为了保证对老规范的兼容性，允许内外环的绕行方向不合法，毕竟还可以利用 coordinates 中成员的位置来确定。下面是各种几何对象的文本表示\n// 点(0, 0). { \u0026quot;type\u0026quot;: \u0026quot;Point\u0026quot;, \u0026quot;coordinates\u0026quot;: [0.0, 0.0] } // 点(0, 0)和(1, 1). { \u0026quot;type\u0026quot;: \u0026quot;MultiPoint\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [0.0, 0.0], [1.0, 1.0] ] } // 点(0, 0)到(1, 1)的连线. { \u0026quot;type\u0026quot;: \u0026quot;LineString\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [0.0, 0.0], [1.0, 1.0] ] } // 加上(2, 2)到(3, 3)的连线. { \u0026quot;type\u0026quot;: \u0026quot;MultiLineString\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ [0.0, 0.0], [1.0, 1.0] ], [ [2.0, 2.0], [3.0, 3.0] ] ] } // 带洞的正方形. { \u0026quot;type\u0026quot;: \u0026quot;Polygon\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ [0.0, 0.0], [3.0, 0.0], [3.0, 3.0], [0.0, 3.0], [0.0, 0.0] ], [ [1.0, 1.0], [1.0, 2.0], [2.0, 2.0], [2.0, 1.0], [1.0, 1.0] ] ] } // 在右边再加一个正方形. { \u0026quot;type\u0026quot;: \u0026quot;MultiPolygon\u0026quot;, \u0026quot;coordinates\u0026quot;: [ [ [ [0.0, 0.0], [3.0, 0.0], [3.0, 3.0], [0.0, 3.0], [0.0, 0.0] ], [ [1.0, 1.0], [1.0, 2.0], [2.0, 2.0], [2.0, 1.0], [1.0, 1.0] ] ], [ [ [5.0, 0.0], [8.0, 0.0], [8.0, 3.0], [5.0, 3.0], [5.0, 0.0] ] ] ] }  这里再啰嗦点，用表格总结一下 shapefile 和 GeoJSON 的概念对应关系\n   shapefile GeoJSON     shape geometry   attributes properties   Points coordinates   Point Point   MultiPoint MultiPoint   PolyLine LineString or MultiLineString   Polygon Polygon or MultiPolygon    PyShp 通过 __geo_interface__ 接口实现了 shapefile 到 GeoJSON 的格式转换：\n reader.__geo_interface__ 返回 FeatureCollection。 shapeRec.__geo_interface__ 返回 Feature。 shape.__geo_interface__ 返回几何对象。  稍微提一下这个接口对于多边形的处理：首先根据绕行方向将所有环分为外环和内环两种，然后为每个内环找到唯一的父亲外环，如果找不到则删除孤儿内环。因为使用的纯 Python 算法较为简单，所以并不能保证转换结果严格正确，甚至还可能转换失败，不过足够一般使用。PyShp 2.2 中还会额外把环的绕行方向颠倒过来，以符合 GeoJSON 新规范的要求，不过这一行为在 2.3 中又取消掉了（见 cartopy issue#2012）。下面将 bou2_4p.shp 文件转为 GeoJSON 格式\nimport json with shapefile.Reader('./data/bou2_4/bou2_4p.shp', encoding='gbk') as reader: geoj = reader.__geo_interface__ # 中文Windows环境下, open默认采用GBK编码. with open('./data/bou2_4/bou2_4p.json', 'w', encoding='utf-8') as f: json.dump(geoj, f, indent=4, ensure_ascii=False)  生成的 GeoJSON 文件的结构可以用下图直观展示\n图中只展示了北京市 Feature 里的详细内容，并且略去了 coordinates 中的数值。\n最后提一下 GeoJSON 相对于 shapefile 的优点：\n 以文本而非二进制形式存储，可读性更强。 引入了 MultiLineString 和 MultiPolygon 几何对象，便于程序解析坐标。 记录了多边形内外环的从属关系，便于后续用 Shapely 包处理。 GeoJSON 仅由单个 JSON 文件组成。  用 Shapely 操作多边形 Shapely 是一个基于 GEOS 库的计算几何包，能够对平面直角坐标系中的几何对象进行操作。Shapely 真要细讲的话需要新开一篇文章，这里只能快速展示与主题相关的部分。Shapely 主要实现了以下几种几何对象：Point、LineString、LinearRing 和 Polygon，同时还有 MultiPoint、MultiLineString 和 MultiPolygon。可见与 GeoJSON 的规范高度一致。shapely.geometry.shape 函数能够将 GeoJSON 的几何对象，或者实现了 __geo_interface__ 接口的对象（例如 PyShp 的 shape 对象）自动转换为 Shapely 中的几何对象。依旧以 bou2_4p.shp 文件为例\nimport shapely.geometry as sgeom from shapely.ops import unary_union # 提取出京津冀的形状. # 其中河北省由多个多边形组成. hebei = [] with shapefile.Reader('./data/bou2_4/bou2_4p.shp', encoding='gbk') as reader: for shapeRec in reader.iterShapeRecords(): name = shapeRec.record['NAME'] if name == '北京市': beijing = sgeom.shape(shapeRec.shape) elif name == '天津市': tianjin = sgeom.shape(shapeRec.shape) elif name == '河北省': hebei.append(sgeom.shape(shapeRec.shape)) # 含Polygon的列表构造MultiPolygon hebei = sgeom.MultiPolygon(hebei) # 合并三个几何对象. jingjinji = unary_union([beijing, tianjin, hebei])  Polygon 对象由外环 Polygon.exterior 和多个内环 Polygon.interiors 组成，其中 exterior 是 LinearRing 对象，而 interiors 是一系列 LinearRing 构成的序列。环的坐标通过 LinearRing.coords 属性访问，coords 同样也是一种序列，可以通过 list(coords) 或 coords[:] 转换为熟悉的 xy 坐标列表的形式。多个 Polygon 对象可以构造 MultiPolygon 对象，其成员通过 MultiPolygon.geoms 属性访问，环及其坐标的获取则通过 geoms[0].exterior 和 geoms[0].exterior.coords 实现。\n因为 bou2_4p.shp 中北京市和天津市都只由单个多边形表示，而河北省由 9 个多边形构成，所以代码中用 MultiPolygon 表示河北省。Polygon.union 或 MultiPolygon.union 方法可以用来合并两个多边形，但运算效率不是很高。所以这里使用更高效的 shapely.ops.unary_union 函数进行合并操作，得到京津冀地区的形状（含海岛）。当然没图说个锤子，下面改写前文的 polygon_to_path 函数，用 Matplotlib 绘制 Shapely 的多边形\ndef polygon_to_path(polygon): '''将Polygon或MultiPolygon转为Path.''' if hasattr(polygon, 'geoms'): polygons = polygon.geoms else: polygons = [polygon] # 空多边形需要占位. if polygon.is_empty: return mpath.Path([(0, 0)]) # 用多边形含有的所有环的顶点构造Path. vertices, codes = [], [] for polygon in polygons: for ring in [polygon.exterior] + polygon.interiors[:]: vertices += ring.coords[:] codes += ring_codes(len(ring.coords)) path = mpath.Path(vertices, codes) return path  画图部分为\nfig, axes = plt.subplots(1, 2) # 第一张子图分别画京津冀三地. for i, geom in enumerate([beijing, tianjin, hebei]): color = plt.cm.tab10(i) path = polygon_to_path(geom) patch = mpatches.PathPatch(path, fc=color, ec='k', lw=1) axes[0].add_patch(patch) axes[0].set_title('Before Union') # 第二张子图画合并后的京津冀. path = polygon_to_path(jingjinji) patch = mpatches.PathPatch(path, fc='lightgrey', ec='k', lw=1) axes[1].add_patch(patch) axes[1].set_title('After Union') # 调整比例和显示范围. for ax in axes: ax.set_aspect('equal') ax.autoscale_view() plt.show()  可以看到第二张图中三个地区间的边界在合并后消失了。\n上面的演示只是 Shapely 丰富功能中的冰山一角，想了解更多用法请参阅官网的 User Manual。\n用 Cartopy 绘制 shapefile 如果把本文的目的，即理解 Cartopy 是怎么画省界的，比作打 Boss 的话，那么前面对各种格式和包的学习就相当于是疯狂练级，现在终于到了能数值碾压 Boss 的阶段。重新回顾一下本文开头的代码（基于 Cartopy 0.20.3），首先是读取 shapefile 文件的部分\nimport cartopy.io.shapereader as shpreader filepath = './data/bou2_4/bou2_4p.shp' reader = shpreader.Reader(filepath) geoms = reader.geometries()  cartopy.io.shapereader 模块提供了读取 shapefile 的 Reader 类，其定义为\nif _HAS_FIONA: Reader = FionaReader else: Reader = BasicReader  即 Python 环境里如果安装了 Fiona 包就以 Fiona 为后端，通过 FionaReader 进行读取；否则以 PyShp 为后端，通过 BasicReader 进行读取。因为 Cartopy 在安装时肯定会依赖 PyShp，所以这里就以 BasicReader 为例继续讲解。其类定义为\nclass BasicReader: def __init__(self, filename): # Validate the filename/shapefile self._reader = reader = shapefile.Reader(filename) if reader.shp is None or reader.shx is None or reader.dbf is None: raise ValueError(\u0026quot;Incomplete shapefile definition \u0026quot; \u0026quot;in '%s'.\u0026quot; % filename) self._fields = self._reader.fields def close(self): return self._reader.close() def __len__(self): return len(self._reader) def geometries(self): '''Return an iterator of shapely geometries from the shapefile.''' for shape in self._reader.iterShapes(): # Skip the shape that can not be represented as geometry. if shape.shapeType != shapefile.NULL: yield sgeom.shape(shape) def records(self): '''Return an iterator of :class:`~Record` instances.''' # Ignore the \u0026quot;DeletionFlag\u0026quot; field which always comes first fields = self._reader.fields[1:] for shape_record in self._reader.iterShapeRecords(): attributes = shape_record.record.as_dict() yield Record(shape_record.shape, attributes, fields)  可以看出 BasicReader 是对 PyShp 的 Reader 类的包装和简化。geometries 方法与 iterShapes 方法类似，都是返回惰性的迭代器，区别在于 Cartopy 返回的是 Shapely 中的几何对象。records 方法与 iterShapeRecords 类似，返回的是同时含有形状和属性的记录。Record.geometry 表示 Shapely 几何对象，Record.attributes 以字典表示属性。不过 BasicReader 的一个问题是，没有选择字符编码的参数，所以若 shapefile 中的属性采用的是非 UTF-8 的编码（例如 GBK）时，调用 records 方法就会报错。解决方法是修改 Cartopy 的源码，添上 encoding 参数。\n本节开始不再使用 bou2_4p.shp，而是采用 ChinaAdminDivisonSHP 项目提供的省界文件，数据来源是高德 Web 服务 API 中的行政区域查询。相比 bou2_4p 来说符合最新的行政区划，省界轮廓更加精细，并且一个省仅对应一条记录，即省的形状用 MultiPolygon 表示。另外这一文件用的是 UTF-8 编码，Cartopy 读取时不会有编码问题。下面是 Cartopy 版筛选河北省记录的例子\nfilepath = './data/ChinaAdminDivisonSHP/2. Province/province' reader = shpreader.BasicReader(filepath) for record in reader.records(): if record.attributes['pr_name'] == '河北省': break  接下来要解说的是这一句\nax.add_geometries(geoms, crs, lw=0.5, fc='none')  add_geometries 顾名思义是往 GeoAxes 上添加几何形状，函数签名为\nadd_geometries(self, geoms, crs, **kwargs)  geoms 是 Shapely 几何对象的序列，不过事实证明迭代器也行。crs 指定几何对象所处的坐标系，考虑到 shapefile 文件的投影多为等经纬度投影，指定 crs=ccrs.PlateCarree() 即可。kwargs 是传给 matplotlib.collections.PathCollection 的参数，用来指定画路径的效果，例如 facecolor、edgecolor、linewidth 和 alpha 等。PathCollection 的作用是把一系列路径对象聚合成单个 Artist，相比逐个绘制 PathPatch 来说效率更高。add_geometries 的源码涉及多个模块，下面仅展示精简后的核心部分\nfrom matplotlib.collections import PathCollection import cartopy.feature as cfeature import cartopy.mpl.patch as cpatch def add_geometries(ax, geoms, crs, **kwargs): '''Add the given shapely geometries (in the given crs) to the axes.''' # 只选取落入地图显示范围中的几何对象进行绘制. feature = cfeature.ShapelyFeature(geoms, crs, **kwargs) extent = ax.get_extent(crs) geoms = feature.intersecting_geometries(extent) paths = [] for geom in geoms: # 将几何对象投影到地图坐标系中. if ax.projection != crs: geom = ax.projection.project_geometry(geom, crs) paths.extend(cpatch.geos_to_path(geom)) # 构造Collection对象并绘制. transform = ax.projection._as_mpl_transform(ax) c = PathCollection(paths, transform=transform, **kwargs) ax.add_collection(c)  首先是将几何对象的序列转为 ShapelyFeature 对象，利用其 intersecting_geometries 方法过滤掉没有落入地图显示范围内的几何对象，这样在画图时就能略去看不到的部分，避免浪费时间。然后代表地图投影坐标系统的 ax.projection 有一个 project_geometry 方法，能够将输入的几何对象从 crs 坐标系变换到 ax.projection 坐标系，返回新的几何对象。geos_to_path 函数与前文我们实现的 polygon_to_path 函数功能一样（返回结果略有差别），是把几何对象转为 Matplotlib 的路径对象，以便之后画图。这里 _as_mpl_transform 的作用是把 CRS 对象转为 Transform 对象。最后是构造 PathCollection，并把准备好的 kwargs 画图参数也输入进去，然后添加到 GeoAxes 上。\n然而 add_geometries 也存在一些问题：\n 是 GeoAxes 专属的方法，普通的 Axes 没有。 geoms 可以是点或线几何，但点画出来看不到，线最好设置 fc='none'。 geoms 中如果有几何对象被过滤掉了，那么上色顺序会跟 facecolors 或 array 参数的顺序不符。 返回值不含 PathCollection 的信息，不方便作为 mappable 对象传给 colorbar。  为了改进这些问题，下面手工实现一个版本\ndef add_polygons(ax, polygons, **kwargs): '''将多边形添加到Axes上. GeoAxes可以通过transform参数指定投影.''' paths = [polygon_to_path(polygon) for polygon in polygons] pc = PathCollection(paths, **kwargs) ax.add_collection(pc) return pc  函数改名为 add_polygons，强调是专门用来画多边形的。点可以用 ax.scatter 画，线可以用 ax.plot 画，并不是非得用 Path。代码中为了简单去掉了 intersecting_geometries 的功能，甚至还去掉了 project_geometry 的部分，因为我发现在 **kwargs 中传入 transform=crs 便能正确处理投影变换。也正是因为去掉了这两部分，现在 ax 可以是普通的 Axes。最后增加了返回值 pc，方便传给 colorbar。下面利用该函数画省界试试\nfilepath = './data/ChinaAdminDivisonSHP/2. Province/province' reader = shpreader.BasicReader(filepath) provinces = reader.geometries() crs = ccrs.PlateCarree() fig = plt.figure() ax = fig.add_subplot(111, projection=crs) ax.set_extent([70, 140, 10, 60], crs) add_polygons( ax, provinces, fc='lightgrey', ec='k', lw=0.5, transform=crs ) plt.show()  简单应用 为落入多边形的网格点生成掩膜（mask）数组，之后掩膜数组可以用来将多边形外的数据点设为缺测\nfrom shapely.vectorized import contains mask = contains(polygon, lon, lat) data[~mask] = np.nan  最直接的思路当然是为每个网格点构造 Point 对象，然后利用 Polygon.contains(Point) 来判断点是否落入多边形内，但这种方法速度极慢。后来我在看 regionmask 包的源码时发现了矢量化版本的 contains 函数，效率对于日常数据处理来说绰绰有余。另外还可以通过递归分割来优化循环法，可见 Cartopy 系列：利用多边形生成掩膜数组 一文。现成的其它选择还有 regionmask、salem、rasterio 等包（python绘图 | salem一招解决所有可视化中的掩膜(Mask)问题）。\n对填色图、风矢量等画图结果进行白化，将多边形轮廓外面的部分设成白色，仅在轮廓内部显示绘图结果\ndef clip_by_polygon(artist, polygon): '''利用多边形裁剪画图结果.''' ax = artist.axes path = polygon_to_path(polygon) if hasattr(artist, 'collections'): for collection in artist.collections: collection.set_clip_path(path, ax.transData) else: artist.set_clip_path(path, ax.transData) cf = ax.contourf(lon, lat, data, levels=10) clip_by_polygon(cf, polygon)  显然前面的 contains 函数能通过设置 NaN，在数据层面实现白化效果，缺点是当网格分辨率较粗时，填色图的边缘会出现明显的锯齿感。而利用 Matplotlib 的 Artist.set_clip_path 方法，以一个 Path 或 Patch 对象作为轮廓，裁剪掉 Artist 在轮廓外面的部分，就能在画图层面实现白化效果，观感更为平滑自然，缺点是对数据处理没有什么帮助。网上非常流行的 maskout 包（实际上是一个模块）的原理便是后者（Python完美白化、提高白化效率 等）。而 maskout 的问题在于把 shapefile 文件的读取和白化功能都放在了 shp2clip 函数里，并且假定读者的文件的字段排列与原作者的文件相同，如果不同，那你就要手动修改 shp2clip 函数内的语句。并且下一次换用另一种 shapefile 文件时就又需要修改。\n这里把白化部分的功能单独拿出来进行讲解：pcolor、pcolormesh、imshow、quiver 和 scatter 方法返回的对象都是 matplotlib.collections.Collection 的子类，可以直接用 set_clip_path 方法进行剪切；而 contour 和 contourf 返回的结果是每一级的等值线（用 PathCollection 表示）构成的列表，保存在 collections 属性中，所以需要迭代其成员进行剪切。代码中出现的 polygon_to_path 函数在前文有定义。\n为了方便测试和复用代码，我写了一个 frykit 包，直接提供掩膜和白化相关的函数，并且加入了对坐标变换和填色图出界的处理（cartopy issue#2052）。另外 frykit 自带 ChinaAdminDivisonSHP 项目的 shapefile 文件，可以一行命令绘制中国国界、省界和市界，并且速度要比 add_geometries 快一些。更多说明请见 GitHub 页面，安装方法为\npip install frykit  依赖仅为 cartopy\u0026gt;=0.20.0。\n下面以 2020 年 6 月 21 日 12 时（CST）的 ERA5 地表 2 米温度场和 10 米风场数据来演示\nimport numpy as np import xarray as xr import matplotlib.pyplot as plt import matplotlib.colors as mcolors import cartopy.crs as ccrs import frykit.plot as fplt import frykit.shp as fshp # 地图范围和数据范围. map_extents = [78, 128, 15, 55] data_extents = [60, 150, 0, 60] lonmin, lonmax, latmin, latmax = data_extents # 读取并裁剪数据. ds = xr.load_dataset('./data/ERA5/era5.tuv.20200621.nc') ds = ds.sortby('latitude').sel( time='2020-06-21 04:00', longitude=slice(lonmin, lonmax), latitude=slice(latmin, latmax) ) t2m = ds.t2m.values - 273.15 u10 = ds.u10.values v10 = ds.v10.values lon, lat = np.meshgrid(ds.longitude, ds.latitude) # 制作国界和省界的掩膜数组. country = fshp.get_cn_shp(level='国') provinces = fshp.get_cn_shp(level='省') country_mask = fshp.polygon_to_mask(country, lon, lat) province_masks = [ fshp.polygon_to_mask(province, lon, lat) for province in provinces ] # 应用掩膜数组. t2m_masked = np.where(country_mask, t2m, np.nan) u10_masked = np.where(country_mask, u10, np.nan) v10_masked = np.where(country_mask, v10, np.nan) # 计算每个省的平均气温. t2m_mean = np.full(len(provinces), np.nan) for i, mask in enumerate(province_masks): if mask.any(): t2m_mean[i] = t2m[mask].mean() # 设置投影. map_crs = ccrs.LambertConformal( central_longitude=105, standard_parallels=(25, 47) ) data_crs = ccrs.PlateCarree() fig, axes = plt.subplots( 1, 3, figsize=(12, 6), subplot_kw={'projection': map_crs} ) fig.subplots_adjust(wspace=0.1) for ax in axes.flat: ax.set_extent(map_extents, crs=data_crs) # 准备cmap和norm. cmap = plt.cm.plasma vmin, vmax = -10, 35 norm = mcolors.Normalize(vmin, vmax) levels = np.linspace(vmin, vmax, 10) # 子图1绘制省平均气温. ax = axes[0] pc = fplt.add_polygons( ax, provinces, ec='k', lw=0.2, cmap=cmap, norm=norm, array=t2m_mean ) cbar = fig.colorbar( pc, ax=ax, ticks=levels, orientation='horizontal', pad=0.05, aspect=30, extend='both' ) cbar.set_label('Temperature (℃)', fontsize='small') cbar.ax.tick_params(labelsize='small') ax.set_title('Averaged by Provinces', fontsize='medium') # 子图2绘制掩膜后的气温场和风场. ax = axes[1] fplt.add_polygons(ax, provinces, fc='none', ec='k', lw=0.2) cf = ax.contourf( lon, lat, t2m_masked, levels, cmap=cmap, extend='both', transform=data_crs ) cbar = fig.colorbar( cf, ax=ax, ticks=levels, orientation='horizontal', pad=0.05, aspect=30, extend='both' ) cbar.set_label('Temperature (℃)', fontsize='small') cbar.ax.tick_params(labelsize='small') Q = ax.quiver( lon, lat, u10_masked, v10_masked, regrid_shape=25, transform=data_crs ) key_kwargs = {'labelsep': 0.05, 'fontproperties': {'size': 'x-small'}} fplt.add_quiver_legend(Q, U=10, width=0.15, height=0.12, key_kwargs=key_kwargs) ax.set_title('Masked by Country', fontsize='medium') # 子图3绘制气温场和风场后再剪切. ax = axes[2] fplt.add_polygons(ax, provinces, fc='none', ec='k', lw=0.2) cf = ax.contourf( lon, lat, t2m, levels, cmap=cmap, extend='both', transform=data_crs ) cbar = fig.colorbar( cf, ax=ax, ticks=levels, orientation='horizontal', pad=0.05, aspect=30, extend='both' ) cbar.set_label('Temperature (℃)', fontsize='small') cbar.ax.tick_params(labelsize='small') Q = ax.quiver( lon, lat, u10, v10, regrid_shape=25, transform=data_crs ) fplt.add_quiver_legend(Q, U=10, width=0.15, height=0.12, key_kwargs=key_kwargs) fplt.clip_by_polygon(cf, country) fplt.clip_by_polygon(Q, country) ax.set_title('Clipped by Country', fontsize='medium') fig.savefig('applications.png', dpi=300, bbox_inches='tight') plt.close(fig)  读者可以点击图片放大看看，第二张子图中填色图与国界间会有些参差不齐的空白，第三张子图中填色图则是严丝合缝地与国界贴在一起。\n结语 本文稍微梳理了一下用 Matplotlib 和 Cartopy 绘制 shapefile 时所需的前置知识和工具链，虽然不了解这些也能用网上现成的代码完成简单的绘图，但只有理解了整个过程，才能使我们在设计复杂图形和 debug 时游刃有余（精美图像一例：Plotting continents\u0026hellip; shapefiles and tif images with Cartopy）。当然对于赶时间的读者，我推荐使用 cnmaps 包，frykit 在很大程度上参考了 cnmaps 的 API。安装方法为\nconda install -c conda-forge cnmaps  然后一行代码画国界、省界、市区县，也不用自己准备 shapefile\nfrom cnmaps import get_adm_maps, draw_maps draw_maps(get_adm_maps(level='国')) draw_maps(get_adm_maps(level='省')) draw_maps(get_adm_maps(level='市'))  同样一行代码白化填色图、伪彩图\nfrom cnmaps import clip_pcolormesh_by_map, clip_pcolormesh_by_map clip_contours_by_map(cs, map_polygon) clip_pcolormesh_by_map(mesh, map_polygon)  另外还有直接导出 GeoDataFrame 等便利的功能，更多用法详见 cnmaps使用指南。本文内容较多，可能在一些地方存在错误，还请读者批评指正。另外如果本文中麻烦的 Python 操作可以用 ArcGIS 或 QGIS 等软件一键解决，也请多多介绍。\n参考链接 Wikipedia: Shapefile\nGeoJson规范（RFC 7946）全文翻译\nA Python Protocol for Geospatial Data\nmatplotlib.collections\nCartopy API reference\nPythonでのShapefile（.shp）操作まとめ\nRaster mask on regular grid from shapely Polygon\n气象绘图加强版（十二）——白化杂谈\n","date":"2022-06-20","permalink":"https://zhajiman.github.io/post/cartopy_shapefile/","tags":["cartopy","matplotlib","shapely"],"title":"Cartopy 系列：探索 shapefile"},{"content":"前段时间重看自己的文章时发现公式渲染、图片的放大缩小和代码高亮等功能都失效了，按 F12 发现原因是引自 cdn.jsdelivr.net 的字体资源、CSS 和 JS 文件都无法访问，挂梯子后页面恢复正常。\njsDelivr 是一款开源的免费公共 CDN，可以加速对 npm、GitHub 和 WordPress 上面静态资源的访问。通过 jsDelivr 引用网站所需的 CSS 和 JS 文件，可以避免直接向服务器请求资源，利用 CDN 加速网站的访问。然而，可能是 jsDelivr 提供的加速功能被一些用户拿来分发不和谐的内容等原因，2021 年 12 月 20 日，jsDelivr 在大陆的 CDN 节点被关闭，ICP 备案被注销，2022 年 4 月 28 日遭到 DNS 污染，自此大陆无法正常访问 jsDelivr，导致大批网站工作失常。jsDelivr 进出大陆的始末详见 【杂谈】jsDelivr域名遭到DNS污染。\n据说很多人的博客因为缺失 CSS 文件而排版错乱，我使用的 Fuji 主题倒没有出现那么严重的错误，但公式失效还是令人非常恼火，这里就来解决一下这个问题。\n我搜到的解决方法有三种：\n 使用 cdn.jsdelivr.net 未受污染的子域：  fastly.jsdelivr.net，由 Fastly 提供 gcore.jsdelivr.net，由 G-Core 提供 testingcf.jsdelivr.net，由 CloudFlare 提供   使用国内的静态库：  cdn.staticfile.org，七牛云和掘金的静态资源库 cdn.bytedance.com，字节跳动静态资源公共库 cdn.baomitu.com，360 前端静态资源库   将需要的静态资源下载到本地  第一种只需将博客主题的 HTML 文件中 jsDelivr 链接里的 cdn 替换为子域名即可；第二种需要在这些国内网站上搜索 JS 库的名字，然后复制搜索结果给出的链接，再替换掉对应的 jsDelivr 链接；第三种是替换为本地路径。为了方便和稳定，我使用的是国内的 cdn.staticfile.org。\n在 VSCode 中搜索站点 themes 目录下含 cdn 的链接，收集得到\n# KaTex 相关\rhttps://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css\rhttps://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js\rhttps://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js\r# 搜索相关\rhttps://cdn.jsdelivr.net/npm/art-template@4.13.2/lib/template-web.min.js\rhttps://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js\r# 页面相关\rhttps://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js\rhttps://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\rhttps://cdn.jsdelivr.net/npm/prismjs@1.23.0/components/prism-core.min.js\rhttps://cdn.jsdelivr.net/npm/prismjs@1.23.0/plugins/autoloader/prism-autoloader.min.js\r 省略了 APlayer、Google Analytics、Disqus 和字体的链接，前三者我用不到，而字体在 staticfile.org 上没搜到，就用备用字体算了。将上述链接修改为\n# KaTex 相关\rhttps://cdn.staticfile.org/KaTeX/0.15.6/katex.min.css\rhttps://cdn.staticfile.org/KaTeX/0.15.6/katex.min.js\rhttps://cdn.staticfile.org/KaTeX/0.15.6/contrib/auto-render.min.js\r# 搜索相关\rhttps://cdn.staticfile.org/art-template/4.13.2/lib/template-web.min.js\rhttps://cdn.staticfile.org/fuse.js/6.6.2/fuse.min.js\r# 页面相关\rhttps://cdn.staticfile.org/medium-zoom/1.0.6/medium-zoom.min.js\rhttps://cdn.staticfile.org/lazysizes/5.3.2/lazysizes.min.js\rhttps://cdn.staticfile.org/prism/1.28.0/components/prism-core.min.js\rhttps://cdn.staticfile.org/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js\r 注意 KaTex 还要去 官网 把对应版本文件的哈希值复制过来，替换原来的 integrity 属性。至此博客又能在不挂梯子的情况下正常显示✌。如果有更好的方法（例如字体方面的）还请读者指点。\n参考资料 jsDelivr Wikipedia\njsDelivr大面积失效，个人站点该怎么办？\n","date":"2022-05-28","permalink":"https://zhajiman.github.io/post/jsdelivr_problem/","tags":["hugo","net"],"title":"解决博客 jsDelivr 资源无法访问的问题"},{"content":"此内容受密码保护。如需查阅，请在下列字段中输入您的密码。\n密码：\n","date":"2022-01-17","permalink":"https://zhajiman.github.io/post/fried_chicken_resources/","tags":["nonsense"],"title":"我所整理的各种炸鸡资源"},{"content":"前言 Matplotlib 的 pcolor 函数能够绘制由一个个四边形（quadrilateral）单元构成的网格数据的彩色图像，相比绘制等值填色图的 contourf 函数，不会产生过度的平滑效果，能忠实反映像元的数值大小，因而在科学可视化中也很常用。本文并不打算介绍该函数的种种，只想着重讨论网格数据的显示效果、shading 参数发挥的作用，以及 pcolor 和 pcolormesh 这对双胞胎间的差异。本文基于 Matplotlib 3.3.4。\n图解网格数据 pcolor 全名 pseudo color，即伪彩色。函数签名为\npcolor([X, Y], C, **kw)\r 其中 X 和 Y 分别是网格的横纵坐标，C 是网格单元内变量的数值。之所以称之为“伪”，是因为 pcolor 并不像 imshow 那样直接用 RGB(A) 数组表示颜色，而是将 C 的数值归一化之后，在一个颜色查找表中查找对应的颜色，进而用颜色差异表现数值大小（原理详见 Matplotlib 系列：colormap 的设置）。C 数组的形状为 (ny, nx)，X 和 Y 的形状要比 C 大上一圈，即 (ny + 1, nx + 1)，ny 在前表示纵坐标会随数组的行号变动，nx 在后表示横坐标会随数组的列号变动。pcolor 对网格数据的显示效果如下图所示\n图中第一行是规则网格，即每个网格单元都是矩形（rectangle）。网格顶点用红点表示，X 和 Y 描述了顶点的横纵坐标；四个邻近的顶点围成一个矩形，C 描述了每个矩形内变量的值，pcolor 再根据这些值的大小为矩形涂上相应的颜色——这便是 pcolor 的作用。\n显然矩形的边长数总是比间隔数大一，这解释了 X、Y 和 C 在数组形状上的差异。并且正如前面所述，X 的值只会随列号变动，Y 的值只会随行号变动。另外必须注意，数组的起点，即行号和列号都为 0 的元素在数组的左上角，但在画出来的图中这个元素位于左下角，也就是说我们需要区分列号和纵坐标的正方向。\n图中第二行是不规则网格，每个网格单元都是平行四边形（parallelogram）。实际上任意四边形都行，因为数组里相邻四个元素的逻辑位置天然构成一个正方形，顶点经由 X 和 Y 的数值可以在 xy 空间映射为任意形状的四边形。注意到，此时 X 和 Y 的值会同时随行号和列号变动，维度 nx 和 ny 不再单纯对应于横坐标和纵坐标。\nX 和 Y 还可以是一维数组，此时 pcolor 会利用 X, Y = np.meshgrid(X, Y) 制作对应的规则网格。不给出 X 和 Y 时则会利用从零开始的简单计数制作网格，即\nX, Y = np.meshgrid(np.arange(ny + 1), np.arange(nx + 1))\r shading 参数 然而世上还有另一种极为常见的网格数据，如下图所示\n这里 C 的形状与 X 和 Y 完全相同，C 直接描述网格顶点处的变量。气候数值模式输出的格点化产品基本都是这种，例如全球的温压湿风等。由此带来的问题是：原先一个四边形对应一个变量值，相当于四边形中心有一个值；而现在一个四边形的四个顶点都有值，那该选哪个来代表四边形的中心呢？\n在 Matplotlib 3.2 及之前的版本里，pcolor 会偷偷抛弃 C 的最后一行和最后一列，即只使用 C[:-1, :-1]，从而将数据变成上一节的形式。这样画出来的图不仅会损失一点显示范围，还会因为强行用顶点描述四边形中心，使填色图向对角线方向偏移半个网格单元的长度。\n虽然听起来有些可怕，但对画图的实际影响其实不是很大，特别是当网格特别密的时候，少掉一行一列无足轻重，微小的偏移也很难看出来，不过说到底还是不太严谨。新版本的 Matplotlib 给出的解决方案是：推测出一张新的网格，其形状比 C 多出一行一列，并尽可能使每个数据点落在新网格单元的正中心。自 3.3 版本起，可以通过指定参数 shading='nearest' 开启这一功能，而原先丢掉数据的行为称作 'flat'。实现该功能的具体语句是（摘自 _axes.py）\ndef _interp_grid(X):\r# helper for below\rif np.shape(X)[1] \u0026gt; 1:\rdX = np.diff(X, axis=1)/2.\rif not (np.all(dX \u0026gt;= 0) or np.all(dX \u0026lt;= 0)):\r_api.warn_external(\rf\u0026quot;The input coordinates to {funcname} are \u0026quot;\r\u0026quot;interpreted as cell centers, but are not \u0026quot;\r\u0026quot;monotonically increasing or decreasing. \u0026quot;\r\u0026quot;This may lead to incorrectly calculated cell \u0026quot;\r\u0026quot;edges, in which case, please supply \u0026quot;\rf\u0026quot;explicit cell edges to {funcname}.\u0026quot;)\rX = np.hstack((X[:, [0]] - dX[:, [0]],\rX[:, :-1] + dX,\rX[:, [-1]] + dX[:, [-1]]))\relse:\r# This is just degenerate, but we can't reliably guess\r# a dX if there is just one value.\rX = np.hstack((X, X))\rreturn X\rX = _interp_grid(X)\rY = _interp_grid(Y)\rX = _interp_grid(X.T).T\rY = _interp_grid(Y.T).T\r 函数 _interp_grid 的功能是：计算数组 X 列与列之间的差分 dX，取 X 的第一列沿列方向偏移 -dX / 2，再取 X 沿列方向偏移 dX / 2，把两个结果并排堆叠成新的 X。旧 X 的形状为 (ny, nx)，那么新 X 的形状为 (ny, nx + 1)， 同时数值上正好错开差分的一半。反复调用该函数，即可产生在行方向和列方向上都扩展了的新 X 和 Y，形状变为 (ny + 1, nx + 1)。比起仔细研究这个函数，还是看个例子更直观\nimport numpy as np\rimport matplotlib.pyplot as plt\rimport matplotlib.colors as mcolors\rimport matplotlib.lines as mlines\rdef interp_grid(X, Y):\r'''插值扩展网格.'''\rX = _interp_grid(X)\rY = _interp_grid(Y)\rX = _interp_grid(X.T).T\rY = _interp_grid(Y.T).T\rreturn X, Y\rdef polar_to_xy(r, t):\r'''极坐标转xy.'''\rx = r * np.cos(np.deg2rad(t))\ry = r * np.sin(np.deg2rad(t))\rreturn x, y\r# 生成规则网格.\rx = np.arange(4)\ry = np.arange(4)\rX1, Y1 = np.meshgrid(x, y)\r# 生成平行四边形网格.\rX2 = X1 + 0.2 * Y1\rY2 = 0.2 * X1 + Y1\r# 利用极坐标生成不规则网格.\rr = np.linspace(2, 4, 5)\rt = np.linspace(0, 180, 5)\rT, R = np.meshgrid(t, r)\rX3, Y3 = polar_to_xy(R, T)\r# 收集网格.\rdata = [(X1, Y1), (X2, Y2), (X3, Y3)]\r# 透明cmap.\rwhite = (0, 0, 0, 0)\rcmap = mcolors.ListedColormap([white])\r# 三个子图对应三组网格.\rfig, axes = plt.subplots(1, 3, figsize=(15, 5))\rfor (X, Y), ax in zip(data, axes):\r# 随便生成一个C数组.\rC = np.arange(X.size).reshape(X.shape)\r# 画出flat的网格.\rax.pcolor(X, Y, C[:-1, :-1], shading='flat', cmap=cmap, ec='k', lw=1)\rax.scatter(X.flat, Y.flat, color='k')\r# 画出nearest的网格.\rax.pcolor(X, Y, C, shading='nearest', cmap=cmap, ec='C3', lw=1)\rX, Y = interp_grid(X, Y)\rax.scatter(X.flat, Y.flat, color='C3')\r# 手动生成图例.\rl1 = mlines.Line2D([], [], c='k', lw=1, marker='o', label=\u0026quot;'flat'\u0026quot;)\rl2 = mlines.Line2D([], [], c='C3', lw=1, marker='o', label=\u0026quot;'nearest'\u0026quot;)\rax.legend(handles=[l1, l2], loc='upper right')\rax.set_xlabel('x', fontsize='large')\raxes[0].set_ylabel('y', fontsize='large')\r# 调整坐标轴.\raxes[0].set_xlim(-1.5, 4.5)\raxes[0].set_ylim(-1.5, 4.5)\raxes[1].set_xlim(-2, 5)\raxes[1].set_ylim(-2, 5)\raxes[2].set_xlim(-6, 6)\raxes[2].set_ylim(-2, 5)\rplt.show()\r 三张图里画的都是当 X、Y 和 C 形状相同时，选取两种 shading 时的网格，区别在于第一张图画的是规则网格，而后两张画的是不规则网格。先说说第一张：\n shading='flat'：黑点同时是网格顶点和数据点，每个矩形单元的颜色由左下角黑点的值决定。 shading='nearest'：红色网格根据黑色网格及其间距插值得到，每个黑点正好处在矩形单元的中心，矩形颜色也由这个点的值决定。  这个推测新网格的策略在第一张和第二张图里都表现良好，但在第三张图里，生成的新网格并不能正确包裹数据点，甚至网格形态也有了不小的变化。原因在于，_interp_grid 函数有效的前提是，X 和 Y 在行方向的差分反映的就是纵坐标的差异，在列方向的差分反映的就是横坐标的差异。但上一节已经展示过，对于不规则网格来说，很可能横纵坐标会同时随行或列而变动，所以该函数可能产生预料之外的结果。并且相关代码里也明确表示，这种情况下会弹出 warning 信息警告用户。\n除了 'flat' 和 'nearest'，还可以指定 shading='gouraud'，表示采用计算机图形学中的 Gouraud 着色法，通过线性插值得到平滑的填色效果。不过要求 X、Y 和 C 的形状必须相同，同时只有 pcolormesh 才有这个选项。例如\n# 生成规则网格.\rx = np.arange(4)\ry = np.arange(4)\rX, Y = np.meshgrid(x, y)\rC = np.arange(X.size).reshape(X.shape)\r# 两张子图分别表示两种shading.\rfig, axes = plt.subplots(1, 2, figsize=(12, 5))\rshadings = ['nearest', 'gouraud']\rfor shading, ax in zip(shadings, axes):\rim = ax.pcolormesh(X, Y, C, shading=shading)\rax.scatter(X.flat, Y.flat, color='C3', label='vertices')\rax.legend(loc='upper right')\rax.set_xlim(-1, 4)\rax.set_ylim(-1, 4)\rax.set_xlabel('x', fontsize='large')\rax.set_title(f\u0026quot;shading='{shading}'\u0026quot;, fontsize='large')\raxes[0].set_ylabel('y', fontsize='large')\r# 设置共用的colorbar.\rcbar = fig.colorbar(im, ax=axes)\rcbar.set_label('c', fontsize='large')\rplt.show()\r 'gouraud' 设置下直接使用原有的网格进行填色，效果甚至比 contourf 还要平滑，当然对于离散的定性数据来说就不要选这个了。\n最后梳理一下 shading 参数的使用方法：\n  shading='flat'：Matplotlib 3.4 及之前是 pcolor 的默认参数。当 C 的形状与 X 和 Y 相同时，会自动抛弃最后一行和最后一列（3.3 与 3.4 会产生 warning），而从 3.5 开始会直接报错，要求 C 的形状必须比 X 和 Y 小一圈。\n  shading='nearest'：Matplotlib 3.3 开始引入，要求 C 的形状与 X 和 Y 相同。对于不规则网格可能产生错误的效果，建议仅对规则网格使用。\n  shading='auto'：Matplotlib 3.3 开始引入，3.5 开始变为 pcolor 的默认参数。顾名思义会自动根据 C 的形状决定使用 'flat' 还是 'nearest'。\n  shading='gouraud'：pcolormesh 独有，要求 C 的形状与 X 和 Y 相同。\n  pcolor 与 pcolormesh 的差别 Matplotlib 中有两种 pcolor 函数：pcolor 和 pcolormesh。前者返回 PolyCollection 对象，能够记录每个四边形单元的独立结构，因而支持坐标 X 和 Y 含有缺测值；后者返回 QuadMesh 对象，更强调网格整体，画图速度比 pcolor 更快，还专有 'gouraud' 选项，但不允许坐标含有缺测值。由于画图速度的优势，一般推荐使用 pcolormesh。坐标缺测的例子如下\n# 创建规则网格.\rx = np.arange(5)\ry = np.arange(5)\rX1, Y1 = np.meshgrid(x, y)\r# 复制一份有缺测的网格.\rX2, Y2 = X1.astype(float), Y1.astype(float)\rX2[2, 2] = np.nan\rY2[2, 2] = np.nan\rshape = X1[:-1, :-1].shape\rsize = shape[0] * shape[1]\rC = np.arange(size).reshape(shape)\rfig, axes = plt.subplots(1, 2, figsize=(12, 5))\rnorm = mcolors.Normalize(vmin=C.min(), vmax=C.max())\r# 两张子图分别画出pcolormesh和pcolor的结果.\rim = axes[0].pcolormesh(X1, Y1, C, shading='flat', ec='k', norm=norm)\rim = axes[1].pcolor(X2, Y2, C, shading='flat', ec='k', norm=norm)\rcbar = fig.colorbar(im, ax=axes)\rcbar.set_label('c', fontsize='large')\r# 标出顶点.\raxes[0].scatter(X1.flat, Y1.flat, color='k', label='good vertices')\raxes[1].scatter(X2.flat, Y2.flat, color='k', label='good vertices')\raxes[1].scatter(X1[2, 2], Y1[2, 2], color='m', label='nan vertices')\r# 设置坐标等.\rfor ax in axes:\rax.legend(loc='upper right')\rax.set_xlim(-1, 5)\rax.set_ylim(-1, 5)\rax.set_xlabel('x', fontsize='large')\raxes[0].set_ylabel('y', fontsize='large')\raxes[0].set_title('pcolormesh', fontsize='large')\raxes[1].set_title('pcolor', fontsize='large')\rplt.show()\r 左图和右图绘制的是相同的数据，区别在于左图使用 pcolormesh，右图使用 pcolor 且把网格中心的顶点设为缺测。结果是右图中与紫色顶点相连的四边形全都没画出来，即便这些四边形对应的数据点都是有值的。pcolor 和 pcolormesh 都能正确处理 C 含缺测的情况，默认缺测位置透明，效果可见 NumPy 系列：缺测值处理 的最后一节。\n结语 本来 Matplotlib 中的 pcolor 直接效仿了 MATLAB 中 pcolor 的行为，但近期 shading='nearest' 的引入使其有了更丰富的表现力。不过正如前面所展示的，推测新网格的策略对不规则网格效果欠佳，并且是否会影响下游的 Cartopy 地图包的效果也还是个未知数，也许相关的 API 日后还会再变动，烦请多加小心。\n参考链接 matplotlib.pyplot.pcolormesh\npcolormesh grids and shading\nMatplotlib Release notes\nMake pcolor(mesh) preserve all data\nENH: add shading=\u0026lsquo;nearest\u0026rsquo; and \u0026lsquo;auto\u0026rsquo; to pcolormesh\nFixing pcolormesh offsets in cartopy\n","date":"2022-01-15","permalink":"https://zhajiman.github.io/post/matplotlib_pcolor/","tags":["matplotlib"],"title":"Matplotlib 系列：网格数据与 pcolor"},{"content":"今天改程序时脑海里突然蹦出这个问题，更宽泛地说，是修饰词或者偏正结构的先后顺序，例如\n upper_ax 和 bottom_ax，ax_upper 和 ax_bottom。 start_date 和 end_date，date_start 和 date_end。  一旦开始疑惑，焦虑便随之而来：哪一种比较好呢？我之前的代码里好像两种写法都出现过，有没有什么现成的规范可以参考呢？越想越不痛快，所以赶紧上网找点前人经验来背书。意外的是，网上大部分文章都在讨论如何取有意义的变量名，而关于这个问题的寥寥无几，也许是因为太细节、太“语法”了？现归纳两篇我看过的帖子以供参考。\n首先在 stack overflow 上找到了一模一样的提问：是用 left_button 和 right_button，还是 button_left 和 button_right 更好呢？提问者自己觉得前者符合英文语序，读起来更加自然，而后者强调了变量的重点在于按钮，而左和右是额外的补充信息。有评论指出后者在 IDE 里更方便，因为你一键入 button，就会自动联想出所有带后缀的版本。这也挺符合人的联想过程，我们肯定是先想到“我要找按钮”，再明确具体要什么样的按钮。当然也有评论给出了经典的废话：与其纠结哪一种约定，任选一种并在项目里维持一致性最重要！好家伙，要是我如此豁达还会来搜这种鸡毛蒜皮的问题吗？\n接着我在 reddit 上找到了 相关的讨论串，里面有两个很有说服力的回复，稍微总结如下。\n网友 DarkSilkyNightmare 表示根据 ta 多年的经验，回答是：”永远把最大的单元放在前面。“例如在面向对象的 Python 里，所谓最大的单元通常是某个类。再具体点，如果想表示一个 file 有很多属性，那就采用 file_id 和 file_name 这种命名；如果想表示存在很多 id，并强调 file 的 id 不同于 thread 和 program 的，那就采用 id_thread 和 id_file 这种命名。类似地，如果你有一个含许多属性的 earth，就用 earth_radius；如果有一系列天体的半径，那么 radius_earth 和 radius_sun 之类的名字会更有意义。\n遵循这个规则可不是迂腐，反而会带来很多具体的好处。首先，这样命名可以提高代码的可读性，让对象所描述的内容更加清晰，并暗示程序的结构。比如说，看到 radius_earth 就会联想到可能存在其它 raidius_* 形式的变量，而看到 earth_radius 就会联想到其它 earth_* 形式表示地球属性的变量。其次，这可以让代码更容易归档和搜索。例如在代码文档工具中相似的变量会按字母顺序排在一起，显然形容词在前时会把顺序搞得一团糟。最后，这有助于我们使用工具扩展代码，例如想把 earth_radius 中的 earth 改成一个类，那么批量修改变量名时将 earth_ 替换成 earth. 即可。\n另一位网友 t_h_r_o_w_-_a_w_a_y 也给出了很深刻的见解。变量命名应该做到便于人类阅读和理解，而英语里形容词一般前置，所以很多人认为变量命名时这样做最为合理。但问题在于，合理与否的标准不应该是”因为某门语言就是这样规定的“，而应该是”大部分读者就是这样感觉的“。事实上，程序员并不都来自英语国家，只是当下他们必须用英语编程罢了，世界上约有一半的语言习惯形容词后置，例如法语、西班牙语、葡萄牙语、意大利语等（详见这篇 知乎回答）。考虑到读者来源的多样性，不能断言形容词后置就是不合规范的，兴许别人读起来更习惯呢。再激进点说，按前文提到的人的正常联想过程，恐怕英语才是真正不合理的那位吧！\n抛开这些主观意见，形容词后置的视觉效果是明显优于前置的。例如定义一系列物体的半径\nearth_radius = 243\rio_radius = 12\rpluto_radius = 30\ryour_mom_radius = 100000\rgliese_581g_radius = 500\r radius_earth = 243\rradius_io = 12\rradius_pluto = 30\rradius_your_mom = 100000\rradius_gliese_581g = 500\r 哪种更易读一目了然。\n本来只是想搜搜相关的规范，结果看到了从实用性和语言学角度的解释，算是意外之喜。最后列出一般的命名规范以供参考：\n PEP 423 \u0026ndash; Naming conventions and recipes related to packaging Google Python Style Guide（中文版 Python风格规范） What is the naming convention in Python for variable and function names? ","date":"2022-01-05","permalink":"https://zhajiman.github.io/post/python_position_of_adjective/","tags":["python"],"title":"Python 系列：变量命名时形容词应该放在名词前面还是后面？"},{"content":"本文研究一个小问题：如何将长度为 N 的列表等分为 n 份？该问题的示意图如下\nN 除以 n 的商为 size，余数为 rest，数值满足 0 \u0026lt;= rest \u0026lt; n or size（除法规则请见 Python 系列：除法运算符）。当 N 是 n 的倍数时，rest = 0 ，列表正好被等分为 n 份，每份含 size 个元素；而当 N 不是 n 的倍数时，rest \u0026gt; 0，按前面的分法会剩下 rest 个元素。对于后一种情况来说并不存在真正的等分，只能说希望尽量等分，问题的重点也落在了如何处理这 rest 个元素上。\n策略一是，若余数不为零，那么 size 顺势增大一位，这样一来肯定能涵盖剩下的元素。\ndef split_list_1(lst, n):\rsize, rest = divmod(len(lst), n)\rsize = size + 1 if rest else size\rfor i in range(n):\ryield lst[i*size:(i+1)*size]\r 这里用到的一个窍门是：虽然索引超出列表下标范围时会报错，但切片并不会，只是返回的元素会变少，或干脆返回空列表。下面进行测试\ndef test(N, n):\rlst = list(range(N))\rfor subset in split_list(lst, n):\rprint(subset)\r In : test(12, 3)\rOut:\r[0, 1, 2, 3]\r[4, 5, 6, 7]\r[8, 9, 10, 11]\rIn : test(12, 5)\rOut:\r[0, 1, 2]\r[3, 4, 5]\r[6, 7, 8]\r[9, 10, 11]\r[]\r 显然第二个结果不太对劲，虽然的确分成了 n 份，但最后一组里一个元素也没有，这很难称得上是等分。余数不为零的情况下的确会有一些分组“缺斤少两”，但我们还是希望组与组之间最多相差一个元素。为了达成这种均衡（balanced）的分组，下面介绍策略二：前 rest 组含 size + 1 个元素，后 n - rest 组含 size 个元素。如下图所示\ndef split_list(lst, n):\rsize, rest = divmod(len(lst), n)\rstart = 0\rfor i in range(n):\rstep = size + 1 if i \u0026lt; rest else size\rstop = start + step\ryield lst[start:stop]\rstart = stop\r In : test(12, 3)\rOut:\r[0, 1, 2, 3]\r[4, 5, 6, 7]\r[8, 9, 10, 11]\rIn : test(12, 5)\rOut:\r[0, 1, 2]\r[3, 4, 5]\r[6, 7]\r[8, 9]\r[10, 11]\r 这次的结果相比策略一更加整齐。当 n \u0026gt; N 时，该函数会用空列表补齐不够的分组。其实还有一个与策略二异曲同工，但仅需一行代码的算法\ndef split_list(lst, n):\rreturn (lst[i::n] for i in range(n))\r 理解其原理需要交换除数与被除数的位置：将列表分为 size 份，每份含 n 个元素，另外剩余 rest 个元素归为特殊的一组。第一次循环收集每组的第一个元素，第二次循环收集每组的第二个元素，依次类推，循环 n 次收集到的 n 个列表即为最终结果。rest 个元素会在前 rest 次循环里被收集完，所以后 n - rest 次循环要比前面的循环少一个元素——这与策略二的思路是一致的。测试结果为\nIn : test(12, 3)\rOut:\r[0, 3, 6, 9]\r[1, 4, 7, 10]\r[2, 5, 8, 11]\rIn : test(12, 5)\rOut:\r[0, 5, 10]\r[1, 6, 11]\r[2, 7]\r[3, 8]\r[4, 9]\r 每组的长度与策略二相同，但跳步索引使得组内元素并不连续，或许这就是简洁的代价吧。\n当然还可以直接调包。more_itertools 包的 divide 函数就可以实现该功能，源码的算法和策略二差不多，区别在于每个分组以迭代器的形式返回。此外这个包里还有按每组元素数进行分组的 chunked 函数，以及可以用缺测值补充长度的 grouper 函数，感兴趣的读者可以去自行查阅。\n参考链接 Python split list into n chunks\nHow do you split a list into evenly sized chunks?\nmore-itertools docs\n","date":"2022-01-04","permalink":"https://zhajiman.github.io/post/python_split_list/","tags":["python"],"title":"Python 系列：将列表等分为 n 份"},{"content":"前言 Matplotlib 中用箭头表示风场或电磁场等矢量场时需要用到 quiver 方法，据字典，quiver 一词的意思是颤动、颤抖或箭袋，貌似也就最后一个意思跟箭头搭得上边。相比于其它画图方法，quiver 的参数又多又容易混淆，所以本文将以图解的方式逐一介绍。这些参数按功能可分为三种：控制箭头位置和数值的、控制箭头长度和角度的，以及控制箭头尺寸和形状的。下面会按照这个分组顺序来解说。本文代码基于 Matplotlib 3.3.4。\n箭头的位置和数值 据文档，quiver 的函数签名为\nquiver([X, Y], U, V, [C], **kw)\r  X 和 Y 指定矢量及箭头的位置。 U 和 V 指定矢量的横纵分量。 C 数组的数值会通过 cmap 和 norm 映射为箭头的颜色（原理详见 Matplotlib 系列：colormap 的设置），例如可以取矢量长度 np.hypot(U, V)。如果只是想让所有箭头颜色相同，使用 color 参数即可。  quiver 既可以像 scatter 那样接受一维散点数据，画出任意位置的箭头，也可以像 pcolormesh 那样绘制二维网格数据。\npivots 参数可以指定 X 和 Y 的位置对应于箭头的尾部、中间，还是头部，默认 pivot = tail，即箭头从 X 和 Y 的位置出发。下面基于这个设置讲解箭头的长度和角度。\n箭头的长度和角度 箭头的长度和角度能直接反映矢量的强度和方向，所以控制这些量的参数无疑是最重要的。其中长度由 scale_units 和 scale 两个参数控制，角度由 angles 参数控制。对于一个分量为 (u, v) 的矢量来说，其在 uv 空间里的长度和角度分别为\nlen_vector = sqrt(u**2 + v**2)\rangle_vector = arctan(v / u)\r 箭头是画在 Axes 的 xy 空间里的，从矢量到箭头要经过两个空间之间的变换。首先介绍如何得到箭头长度\nlen_arrow = len_vector / scale [scale_units]\r 其中 scale 用于放缩数值，scale_units 决定箭头的长度单位。所谓单位即某个基准长度，需要参考图中已有的元素来进行设定。例如当 scale = 1 时，箭头长度等于矢量长度的数值乘上这个基准长度。scale_units 可取七种：'inches'、'dots'、'width'、'height'、'x'、'y' 和 'xy'。下图展示了前六种\n该图由 fig, ax = plt.subplots() 语句生成，默认 figsize = (6.4, 4.8)，dpi = 100，所以尺寸为 6.4 x 4.8 英寸，或 640 x 480 像素（英寸和像素的意义详见 Matplotlib 系列：导出高 DPI 的图片）。以 inches 为例，若 scale = 1，那么长度为 1 的矢量在图上对应于长度为 1 英寸的箭头，其它单位同理。图中未展示的 'xy' 单位比较特殊，后面讲到 angles 时再细说。\n七种单位中 inches 和 dots 显然是绝对单位，而剩下的均为相对于 Axes 的元素设定的单位。在 plt.show 弹出的交互式窗口内缩放 Axes 时，基于相对单位的箭头长度会动态变化，而基于绝对单位的箭头长度则纹丝不动。无论选用哪种单位，若箭头过长或过短，都可以用 scale 参数缩放到合适的范围：scale 越小，箭头越长；scale 越大，箭头越短。\n接着来看如何得到箭头角度。控制箭头角度的 angles 有三种设置：一是把单个浮点数或数组传给 angles 参数，直接指定每个箭头的角度，此时矢量的 u 和 v 分量和箭头角度没有任何关系。二是令 angles = 'uv'，表示沿用矢量角度\nangle_arrow = angle_vector\r 三是令 angles = 'xy'，一般需要和 scale_units = 'xy' 联用，此时箭头等同于 xy 平面里 (x, y) 到 (x + u, y + v) 的连线箭头。例如当 xy 平面是空间位置，矢量表示位移时就适合用这个设置。下面示意 angles 的效果\n图中为了体现 uv 空间和 xy 空间的差异，特地设置 ax.set_aspect(0.5) ，于是网格单元的宽高比为 2:1。可以看到，angles = 'uv' 时，箭头角度就为 45°；angles = 'xy' 且 scale_units = 'xy' 时，箭头与网格单元的对角线刚好重合。这里未展示 angles 为定值的结果，是因为 scale_units = 'xy' 与之冲突，导致画不出箭头，也许是个 bug。\nscale_units 和 scale 默认为 None，表示 Matplotlib 会自动根据矢量长度的平均值，以及矢量的个数决定箭头的长度。angles 默认为 'uv'。一般我们只需要调整 scale_units 和 scale，而不需要改动 angles。\n值得一提的是，若通过 ax.set_aspect(1) 使 Axes 两个坐标轴的单位长度等长，那么 'x'、'y' 和 'xy' 三种长度单位的结果相同， 'uv' 和 'xy' 两种角度设置的结果也相同。\n箭头的尺寸和形状 类似于箭头长度与 scale_units 的关系，箭头尺寸的单位由 units 给出，同样可取七种：'inches'、'dots'、'width'、'height'、'x'、'y'、'xy'。此处 'xy' 的含义不同于上一节，仅指 Axes 对角线的单位长度。units 默认为 width。\n选好单位后首先需要设置的参数是 width，箭杆（shaft）的宽度就等于 width 的数值乘上单位对应的基准长度。之后其它形状参数——headwidth、headlength、headaxislength——均以箭杆的宽度为单位。下图描绘了这些参数代表的部分\nwidth 默认为 None，表示 Matplotlib 会自动决定箭杆宽度。而其它参数都有提前设好的值，例如 headwidth 默认为 3，表示箭镞（允许我用古文称呼箭头尖尖）的宽度总是箭杆的三倍。\n最后提一个神秘的地方，文档指出 units 不会影响箭头长度，但事实是在不给出 scale_units 时，units 会同时决定箭头长度和尺寸的单位。例如参考资料的最后一篇便展示了 units 对箭头长度的影响，我个人认为这是 Matplotlib 的设计失误。\n箭头的阈值 你可能会碰到箭头的尺寸不合预期、或箭头缩成了一个点的情形，这都是 minshaft 和 minlength 这两个阈值参数导致的。\nminshaft 以 headlength 为单位，默认为 1，当箭头长度小于 minshaft 代表的长度时，箭头整体尺寸会按箭头长度等比例缩小。\nminlength 以 width 为单位，默认为 1，当箭头长度小于 minlength 代表的长度时，箭头直接退化成以该长度为直径的六边形。\n选用默认值的场合，minshaft 是五倍 width 的长度，minlength 是单倍 width 的长度，当矢量长度越来越小时，对应的箭头一开始只缩短长度，后来尺寸也跟着缩小，最后直接缩成一个点（六边形）。如果没有这两个参数，那么特别短的矢量在图上仍然会挂着一个特别大的箭镞，既不美观，还可能影响我们的判断。下面改编一个 官网示例\n可以看到左图中间的短矢量与周围的长矢量通过尺寸差异被区分开来，而右边则很难辨认，中间的箭头还出现了空心情况。这两个阈值一般不需要改动，默认条件下就有不错的效果。\n箭头的图例 箭头的图例通过 quiverkey 方法添加，由一个箭头和文本标签构成。函数签名为\nquiverkey(Q, X, Y, U, label, **kwargs)\r 下面列举常用参数：\n Q：quiver 方法返回的 Quiver 对象，借此可以画出与 quiver 类似的箭头。 X 和 Y：图例的位置。虽然用大写字母表示，其实并不是数组。 U：箭头的长度，用矢量长度衡量。 label：标签的文本，一般填 U 的数值和矢量的单位。 coordinates：指定 X 和 Y 是什么坐标，可选 'axes'、'figure'、'data' 和 'inches'，默认为 'axes'。坐标间的差异请见文档的 Transformations Tutorial。 labelpos：标签相对于箭头的位置，可选 'N'、'S'、'E' 和 'W'。默认为北，即标签在箭头上面。 labelsep：标签与箭头间的距离，默认为 0.1 pt。 fontproperties：用于指定标签字体参数的字典。  Cartopy 系列：从入门到放弃 文末提供了一个示例，同时为了实现 NCL 那种箭头图例外面带个方框的风格，在图例后面还加了个矩形补丁。\nCartopy 中的 quiver Cartopy 的 GeoAxes 对 Axes 的 quiver 方法进行了装饰，使之能通过 transform 参数实现不同 CRS 间的坐标变换（详见 Cartopy 系列：对入门教程的补充）。注意所有投影的 GeoAxes 的 aspect_ratio 都为 1，所以正如本文开头提到的，scale_units 取 x、y 或 xy 时结果没区别，angles 取 uv 或 xy 结果也没有区别。尽管如此，考虑到各种投影坐标系的 x 范围和 y 范围通常都很怪，胆小的我还是会取 scale_units = 'inches'，angles = 'uv'。\n此外 Cartopy 还提供了一个非常便利的新参数 regrid_shape，可以将矢量场重新插值到投影坐标系中的规则网格上，以达到规整矢量位置或稀疏箭头密度的目的，而在 Axes 中这活儿需要通过手动插值或跳步索引来实现。regrid_shape 接收二元组或整数，前者指定 x 和 y 方向上的箭头个数，后者指定短边上的箭头个数，然后长边的个数通过地图范围的宽高比缩放得出。默认为 None，即不进行网格化。下面改编一个 官网示例\n两图中的风场基于 NorthPolarStereo 坐标里的规则网格，地图则基于 PlateCarree 坐标。上图未进行网格化，风箭头明显间距不一。下图指定 regrid_shape = 10 后，风场被 scipy.interpolate.griddata 函数线性插值到地图上形为 (16, 10) 的规则网格中，箭头因而清晰可辨。\n结语 文中未给出渐变色箭头的例子，读者可以参考 官网的 demo。另外矢量场除了用 quiver 画箭头表示，还可以用 streamplot 画流线表示，以后有机会再另行介绍。\n参考资料 matplotlib.axes.Axes.quiver\nmatplotlib.axes.Axes.quiverkey\ncartopy.mpl.geoaxes.GeoAxes.quiver\n【python】quiverの矢印の長さをうまく調整したい【matplotlib.pyplot.quiver】\n","date":"2021-12-18","permalink":"https://zhajiman.github.io/post/matplotlib_quiver/","tags":["matplotlib","cartopy"],"title":"Matplotlib 系列：图解 quiver"},{"content":"前言 笔者初次使用 MODIS 二级气溶胶产品时，一下就被密密麻麻一堆变量搞懵了：很多变量名字里带个 Optical_Depth，这我能猜到，就是气溶胶光学厚度，但各种 Corrected、Effective、Best、Average、Small、Large 的前后缀鬼知道是什么。看过的论文基本不说具体用的哪个变量，各种教程也不会告诉你这些亲戚间的差异，似乎这件事一点也不重要。本着 know your data 的心态，我在翻阅了 MODIS 的几个官网后总算从反演的原理中稍微体会到了这些前后缀的意义。现将学习经验总结归纳如下，希望能帮到和我一样疑惑的小伙伴。同时本文还会提供简单的 Python 示例代码。\n如果嫌正文太啰嗦，可以直接跳到文末的总结部分，那里直接给出了各个变量的使用建议。\n基本介绍 MODIS 全称 MODerate resolution Imaging Spectroradiometers（中分辨率成像光谱仪），基本信息为：\n  搭载平台：Terra（2000 年至今）和 Aqua（2002 年至今）。\n  轨道：太阳同步，上午 10:30 降轨通过赤道（Terra）或下午 1:30 升轨通过赤道（Aqua）。\n  波段：0.41 ~ 14.5 μm的 36 个波段（即可见光到远红外）。\n  刈幅：2330 km。\n  空间分辨率：250 m（bands 1 - 2）、500 m（bands 3 - 7）、1000 m（bands 8 - 36）。\n  上图为大气层顶（绿线）和海平面（红线）处的太阳入射光谱，以及 MODIS 的各个波段（橙线）。\nMODIS 的优势可以概括为两点：\n 多波段的辐射测量能揭示关于陆面、海洋和大气的丰富信息。 高空间分辨率、太阳同步轨道和宽广的刈幅能覆盖宽广的时空范围  气溶胶光学厚度（Aerosol Optical Depth，AOD）反映到达地表的太阳辐射受气溶胶的衰减程度，在无云的晴空下，太阳光度计可以通过直接测量太阳方向的辐射来计算 AOD。星载的 MODIS 与之不同，如上图所示，其镜头朝向地表，测量的是经地表反射和气溶胶散射后的太阳辐射。利用地表和气溶胶在不同波段的辐射性质差异，有机会从测量结果中分离出气溶胶的信号，进而推测气溶胶的物理性质，这便是 MODIS 反演气溶胶的思路。当然现实是全球的地表类型和气溶胶组成不尽相同，为了简化问题，需要根据观测资料提前对地表性质和气溶胶组成做出一些假设。目前 MODIS 反演气溶胶的业务算法有两种：\n 暗目标算法（Dark Target，DT）：洋面和植被覆盖的陆面在可见光波段因为反射率较低显得偏暗，被称作“暗目标”，而气溶胶在这一背景上显得偏亮，由此可以忽略或估算地表的信号，进而分离出气溶胶的信号。因为陆面的地表类型更为复杂，所以 DT 算法在实现上又细分为针对洋面的 DT-ocean 和针对陆面的 DT-land。 深蓝算法（Deep Blue，DB）：干旱和沙漠类型的地表即便在可见光波段也显得很亮，DT 算法成立的假设失效。而在近紫外的“深蓝”波段这些地表又变得偏暗，所以借助这一波段可以扩展反演的覆盖范围。目前深蓝算法仅应用于陆面。  MODIS 的二级气溶胶产品名为 MOD04_L2 和 MYD04_L2，其中 MOD 和 MYD 分别代表 Terra 和 Aqua 卫星。每个文件对应一段 5 分钟长度的 MODIS 轨道片段（称作 granule），星下点的空间分辨率为 10 x 10 km，共含 203 x 135 个像元。主要提供的变量为 0.55 μm 的 AOD 和细粒子比（Fine Mode Fraction，FMF），这些变量还会按算法分为 DT-ocean、DT-land 和 DB 等多个版本。MODIS 每次改进和更新算法后重新生产的产品集被称作一个 collection，目前的最新版是 2016 年发布的 C6.1，这一版本还引入了 3 x 3 km 高空间分辨率的 MOD04_3K 和 MYD04_3K 产品。\n后面将会以 C6.1 版本的 MYD04_L2 产品为例，介绍每种算法提供的主要变量，以及用 Python 进行读取的方法。选择 MYD 是因为 Terra 发射时间更早仪器衰减更大，Aqua 的产品可能更为可靠（参考 Dark Target FAQ）。同时因为 DT 算法是气溶胶产品的主要算法，并且其文档更为详尽，所以 DB 算法的部分会比较简略。\n读取准备 MYD04_L2 文件名的意义如下图所示\n文件格式是基于 HDF4 的 HDF-EOS2，因为自带经纬度坐标，所以不需要像陆面产品那样需要用 MRT 或 HEG 进行重投影，直接读了用就行。Python 中可以用 GDAL 或 pyhdf 包进行读取，这里使用后者。反演得到的参数以科学数据集（Scientific Data Set，SDS）的形式存储在 HDF 文件中，所谓 SDS 不过是带有元数据的数组罢了，而 pyhdf 的 SD 模块能提供关于 SDS 的 API。其使用方法详见 官方文档 和参考资料里的链接，不过写代码之前，可以先用 NASA 开发的 Panoply 软件直接打开 MYD04_L2 文件，查看 SDS 的元数据，并进行简单的可视化，如下面两张图所示\n以 Optical_Depth_Land_And_Ocean 变量为例，可以看到其维度是沿卫星轨道的 203 个像元和横向的 135 个像元，这样一个数组画在等经纬度地图上呈边缘带弧度的矩形区域，同时有很多像元因为缺测而没有标出颜色。值得注意的是，AOD 本应是 double 类型（对应于 np.float64）的数据，但在 Panoply 中显示为 short 类型（对应于 np.int16），这是因为卫星产品常常会利用偏移和缩放操作将浮点类型的数据转换为整数类型，牺牲一点精度以压缩存储空间。Panoply 在画图时会自动换算回去，但使用 pyhdf 时我们需要根据 add_offset 和 scale_factor 属性手动进行换算。HDF 文件的元数据中记录有换算公式\nfloat value = scale_factor * (stored integer - add_offset)  不过换算前需要小心 _FillValue 属性标记的缺测部分。用 Panoply 探索一番后，就可以动手实现一个读取 MYD04_L2文件的类\nfrom pyhdf.SD import SD, SDC import numpy as np import pandas as pd class ReaderMYD: '''读取MYD04_L2文件的类.''' def __init__(self, filepath): '''读取文件.''' self.sd = SD(filepath, SDC.READ) def __enter__(self): return self def search_sds(self, keyword): '''搜索SDS名,无视大小写.''' sdsnames = sorted(self.sd.datasets().keys()) for sdsname in sdsnames: if keyword.lower() in sdsname.lower(): print(sdsname) def read_lonlat(self): '''读取经纬度.''' lon = self.sd.select('Longitude')[:] lat = self.sd.select('Latitude')[:] return lon, lat def read_sds(self, sdsname, scale=True): '''读取SDS.''' sds = self.sd.select(sdsname) data = sds[:] # 按需进行缩放. if scale: attrs = sds.attributes() fill_value = attrs['_FillValue'] scale_factor = attrs['scale_factor'] add_offset = attrs['add_offset'] data = np.where( data == fill_value, np.nan, (data - add_offset) * scale_factor ) return data def read_time(self): '''读取沿swath的TAI时间,返回DatetimeIndex.''' second = self.sd.select('Scan_Start_Time')[:, 0] time = pd.to_datetime(second, unit='s', origin='1993-01-01') return time def close(self): '''关闭文件''' self.sd.end() def __exit__(self, *args): self.close()  这里使用 NaN 标记缺测值（参考 NumPy 系列：缺测值处理），用 __enter__ 和 __exit__ 实现上下文管理器（即可以使用 with 语句），读取的 TAI 时间与 UTC 时间仅相差数十秒，偷懒直接当作 UTC 时间用。\nDT-ocean 算法及其变量 DT-ocean 的基本思路是，忽略可见光波段洋面贡献的反射率，那么大气顶的反射率可以表示为细模态（fine mode）和粗模态（coarse mode）气溶胶产生的反射率之和。所谓细模态指有效半径在 0.1 ~ 0.25 μm 范围内的细粒子，粗模态指有效半径在 1.0 ~ 2.5 μm 范围内的粗粒子。利用辐射传输模式模拟各种细模态气溶胶和粗模态气溶胶组合产生的反射率，用模拟值去逼近 MODIS 的实测值，最后求解出总 AOD、气溶胶的组合，以及细模态气溶胶的贡献比例。\n具体反演过程基于查找表（Look-Up Table，LUT）法。首先根据以往的观测资料预定义四种典型的细模态气溶胶和五种典型的粗模态气溶胶，如下表所示\n表中给出了每种气溶胶的折射系数和谱分布参数，利用 Mie 散射模型可以计算气溶胶在多个波段的 AOD 性质，利用辐射传输模式又可以计算出气溶胶存在时的大气顶反射率、大气透过率、大气后向散射比等参数，这些预先计算好的信息便构成了 LUT。接下来从四种细模态气溶胶和五种粗模态气溶胶中各选一种构成二十对组合，任意一对组合的总反射率表示为 $$ \\rho_{\\lambda}^{LUT}(\\tau_{0.55}) = \\eta_{0.55}\\rho_{\\lambda}^{f}(\\tau_{0.55}) + (1 - \\eta_{0.55})\\rho_{\\lambda}^{c}(\\tau_{0.55}) $$ 其中 $\\rho_{\\lambda}^{f}$ 和 $\\rho_{\\lambda}^{c}$ 分别表示细模态和粗模态气溶胶的反射率，二者的值可以用 0.55 μm AOD $\\tau_{0.55}$ 在 LUT 中查询到。细粒子比 $\\eta_{0.55}$ 用于衡量细模态和粗模态气溶胶的贡献，值为 0 时表示全为细粒子，值为 1 时表示全为粗粒子。$\\rho_{\\lambda}^{LUT}$ 即模拟出的气溶胶的总反射率，已知 MODIS 在 0.55、0.65、0.86、1.24、1.63 和 2.11 μm 六个波段测得大气层顶反射率 $\\rho_{\\lambda}^{m}$，通过最小化 $\\rho_{\\lambda}^{LUT}$ 和 $\\rho_{\\lambda}^{m}$ 之间的拟合误差即可得到一对 $\\tau_{0.55}$ 和 $\\eta_{0.55}$ 的值——这便是反演的主要产品：0.55 μm 的气溶胶光学厚度和细粒子比。之后在 LUT 中很容易查询到其它波段 $\\tau$ 和 $\\eta$ 的值，这些值又可以用来得出其它变量，例如用 0.55 和 0.86 μm 的 $\\tau$ 计算 Ångström 指数（AE）。\n二十对气溶胶组合能反演出二十组结果，我们肯定要进行挑选，其中拟合误差最小的结果称作 best solution，而误差较小的所有组合平均之后的结果称作 average solution。于是最后的产品里含有 best 和 average 两种类型的反演结果，官方推荐使用后者。\n为了表示反演结果的可信度，每个像元被赋予了 QA（Quality Assuracne）标记，其中包含像元状态、云的影响、反演中遇到的问题等多种信息，不过我们一般只需要用到最基本的 QA Confidence（QAC）值：\n QAC = 3 表示结果非常好（very good）。 QAC = 2 表示结果很好（good）。 QAC = 1 表示结果有些勉强（marginal）。 QAC = 0 表示结果很差（bad）。  画图时为了数据的完整性可以不考虑 QA，但进行定量统计时官方建议仅采用 QAC \u0026gt; 0 的像元。\n简叙完必要的概念，下面介绍 DT-Ocean 的变量，并用颜色标出了比较重要的几个\nEffective_Optical_Depth_Average_Ocean：Average solution 得到的 0.47、0.55、0.65、0.86、1.24、1.63 和 2.11 μm 七个波段的 AOD。被云遮挡的部分、陆面部分等都默认缺测。虽然表中没提，但文件中存在已经提取好的 0.55 μm AOD Effective_Optical_Depth_0p55um_Ocean。你可能会问为什么有个 Effective 的前缀？我也不清楚，可能是自老版本产品传承下来的约定。\nOptical_Depth_Ratio_Small_Ocean_0.55micron：Average solution 得到的 0.55 μm 的细粒子比 FMF，表示细模态 AOD 与总 AOD 的比值，可以用来计算细模态 AOD 和粗模态 AOD $$ \\tau_{0.55}^{f} = \\eta_{0.55}\\tau_{0.55} \\newline \\tau_{0.55}^{c} = (1-\\eta_{0.55})\\tau_{0.55} $$ Optical_Depth_Small_Average_Ocean：Average Solution 得到的七个波段的细模态 AOD，相当于提前帮你准备好了细模态 AOD，不需要用 FMF 手动进行计算。\nOptical_Depth_Large_Average_Ocean：同上，提前准备好的粗模态 AOD。\nQuality_Assurance_Ocean：QA 标记，数据类型为 short，QA 相关的信息需要通过位运算从 bit 中提取（详见 MODIS Atmosphere QA Plan），例如 QAC 可以这样提取\nQAC = (Quality_Assurance_Ocean[:, :, 0] \u0026amp; 14) \u0026gt;\u0026gt; 1  这种操作还是挺麻烦的，所以文件中的 Land_Ocean_Quality_Flag 变量直接提供 0 ~ 3 的 QAC 值，其中还含有后面会介绍的 DT-land 的 QAC。例如筛去低质量的 AOD\nEffective_Optical_Depth_0p55um_Ocean[Land_Ocean_Quality_Flag == 0] = np.nan  下面以 2015 年 2 月 11 日扫过中国东南沿海地区的 MYD04_L2.A2015042.0535.061.2018047210001.hdf 文件为例，画出变量 0.55 μm 的 AOD 和 FMF。首先在 EOSDIS Worldview 网站在线查看当天 Aqua MODIS 的真彩色图\n可见沿海地区被“灰霾”覆盖，说明有气溶胶存在。接着在 Python 中用 Matplotlib 和 cartopy 包画图\nimport copy import numpy as np import matplotlib.pyplot as plt from matplotlib.ticker import MultipleLocator import cartopy.crs as ccrs from reader import ReaderMYD from map_funcs import set_map_extent_and_ticks # 读取经纬度,时间和AOD变量. filepath = './MYD04_L2.A2015042.0535.061.2018047210001.hdf' with ReaderMYD(filepath) as f: lon, lat = f.read_lonlat() time = f.read_time() aod = f.read_sds('Effective_Optical_Depth_0p55um_Ocean') fmf = f.read_sds('Optical_Depth_Ratio_Small_Ocean_0.55micron') # 设置地图. extents = [100, 130, 10, 40] proj = ccrs.PlateCarree() fig, axes = plt.subplots( 1, 2, figsize=(10, 6), subplot_kw={'projection': proj} ) for ax in axes: ax.coastlines(resolution='10m', lw=0.3) set_map_extent_and_ticks( ax, extents, xticks=np.arange(-180, 190, 10), yticks=np.arange(-90, 100, 10), nx=1, ny=1 ) ax.tick_params(labelsize='small') # 缺测设为灰色. cmap = copy.copy(plt.cm.jet) cmap.set_bad('lightgray') # 绘制AOD. im = axes[0].pcolormesh( lon, lat, aod[:-1, :-1], cmap=cmap, vmin=0, vmax=2, shading='flat', transform=proj ) cbar = fig.colorbar( im, ax=axes[0], pad=0.1, ticks=MultipleLocator(0.5), orientation='horizontal' ) cbar.set_label('AOD', fontsize='medium') cbar.ax.tick_params(labelsize='small') axes[0].set_title('Effective_Optical_Depth_0p55um_Ocean', fontsize='medium') # 绘制FMF im = axes[1].pcolormesh( lon, lat, fmf[:-1, :-1], cmap=cmap, vmin=0, vmax=1, shading='flat', transform=proj ) cbar = fig.colorbar( im, ax=axes[1], pad=0.1, ticks=MultipleLocator(0.2), orientation='horizontal' ) cbar.set_label('FMF', fontsize='medium') cbar.ax.tick_params(labelsize='small') axes[1].set_title( 'Optical_Depth_Ratio_Small_Ocean_0.55micron', fontsize='medium' ) # 用平均过境时间作为标题. time_str = time.mean().strftime('%Y-%m-%d %H:%M') fig.suptitle(time_str, fontsize='large') plt.show()  代码中 set_map_extent_and_ticks 函数的定义请见 Cartopy 系列：从入门到放弃 一文。图中 Aqua 卫星于 UTC 时间 05:37，即北京时间下午 13:37 过境，MODIS 在黄海反演出 1.0 左右的 AOD 和 0.8 以上的 FMF，暗示主要为粒径偏小的人为排放气溶胶。同时台湾北侧因为有云遮挡而全部缺测。\nDT-land 算法及其变量 DT-land 算法的基本思路与 DT-ocean 一致，但因为陆面在可见光和短波红外波段的反射率明显不为零，所以为了排除地表信号的影响，需要额外估算地表反射率。同时因为用到的观测波段变少了，采用的气溶胶模型也不同于 DT-ocean。\n首先，DT-land 算法主要使用 0.47、0.65 和 2.12 μm 三个波段的观测数据，波段太少难以像 DT-ocean 那种动态选取细模态和粗模态气溶胶进行组合。取而代之的是一种更为宽泛的表示方法：fine model 和 coarse model。一个 model 由多个模态组成，反映了多种气溶胶的综合效应，fine model 即细模态气溶胶占主导地位的 model，coarse model 同理。DT-land 算法中将气溶胶概括以下五种 model\n前四种是 fine model，第五种即沙尘是单独的 coarse model。其中 continental model 仅用于后面将会提到的地表偏亮情况时的反演。利用聚类分析等统计方法可以从 AERONET 站点的观测数据中得出全球不同区域各季节占主导地位的 fine model，之后的反演会参考这一结果预先选取合适的 fine model，再与沙尘 model 相组合。通过辐射传输模式计算的每种 model 产生的大气顶反射率等信息会被存入 LUT 中。\n其次，地表反射率的估计有赖于 VISvs2.12 关系。Kaufmann 等通过观测和模拟研究发现，在植被覆盖或深色土壤的地表，可见光与短波红外波段的地表反射率的比值近乎定值，例如 0.47vs2.12 约为 0.25，0.65vs2.12 约为 0.5，这被称作 VISvs2.12 关系。DT-land 使用的 VISvs2.12 关系用散射角和短波短波红外 NDVI（衡量地表绿度）进行了修正，若已知 2.12 μm 的地表反射率，便可以根据该关系计算出 0.47 和 0.65 μm 波段的值。\n对于暗地表，fine model、coarse model 和地表三者产生的大气顶反射率表示为 $$ \\rho_{\\lambda}^{*} = \\eta\\rho_{\\lambda}^{*f} + (1-\\eta)\\rho_{\\lambda}^{*c} $$ 其中 $\\rho_{\\lambda}^{*f}$ 是 fine model 的反射率 $\\rho_{\\lambda}^{f}$ 与地表反射率 $\\rho_{\\lambda}^{s}$ 的综合效果，其数值可以用 $\\tau_{0.55}$ 在 LUT 中查询得到，$\\rho_{\\lambda}^{*c}$ 同理。细粒子比 $\\eta$（或 $\\eta_{0.55}$） 用于衡量 fine model 和 coarse model 的贡献。需要注意，除了 $\\tau_{0.55}$ 和 $\\eta_{0.55}$，$\\rho_{\\lambda}^{s}$ 也是要求解的量，不同波段的值由 VISvs2.12 关系相关联。求解方程为 $$ \\rho_{0.47}^{*} - \\rho_{0.47}^{m} = 0 \\newline \\rho_{0.66}^{*} - \\rho_{0.66}^{m} = \\epsilon \\newline \\rho_{2.12}^{*} - \\rho_{2.12}^{m} = 0 $$ 即让 0.47 和 2.12 μm 波段的模拟值与实测值恰好相等，然后找出最小化 0.66 μm 波段误差 $\\epsilon$ 的 $\\tau_{0.55}$、$\\eta_{0.55}$ 和 $\\rho_{2.12}^{s}$——此即反演的主要产品：0.55 μm 的 AOD 和 FMF，以及 2.12 μm 的地表反射率。之后可以利用 LUT 查询其它波段的值，并计算一系列的衍生量。\n为了确保解的唯一性，$\\eta_{0.55}$ 仅取 -0.1 ~ 1.1 之间间隔为 0.1 的离散值，非物理的 -0.1 和 1.1 是为了在反演时容纳误差，反演结束后会被设为 0 和 1。另外当 $\\tau_{0.55} \u0026lt; 0.2$ 时，$\\eta_{0.55}$ 准确度非常差，所以直接设为缺测。\n对于不那么暗、稍微有点亮的地表也能进行反演，此时会设定 $\\eta_{0.55} = 1$，气溶胶模型仅由 continental model 构成，然后直接求解 0.47 和 2.12 μm 波段模拟值与实测值的方程。因为 VISvs2.12 关系在这种地表的误差会增大，导致反演结果不准，所以走这一流程的像元会被打上 QAC = 0 的标记。\nDT-land 一个很特殊的地方在于，允许反演的 AOD 存在负值。因为反演在 0 附近存在误差，如果把值截断在 0 反而会使数据有偏，所以统计意义上有负值会更好。不过值最低也只允许到 -0.05 罢了。\n因为陆面远比洋面复杂多变，所以 DT-land 的误差要比 DT-ocean 大。C5 版本的验证结果表示全球尺度上 DT-ocean AOD 的误差为 ±(0.03 + 5%)，DT-land AOD 的误差为 ±(0.05 + 15%)。验证还发现，陆面上的 FMF 和 AE 不太靠谱，仅有定性的价值，用其进行论证时需要留心。\nDT-land 的 QA 同 DT-ocean，官方推荐使用 QAC = 3 的像元进行定量研究，这要求就比 DT-ocean 高很多，也可以理解为是陆面反演误差大，需要更强的限制。\n因为 DT-land 的坑很多所以上面写的零零散散的。接着具体介绍变量\nCorrected_Optical_Depth_Land：0.47、0.55 和 0.66 μm 三个波段的 AOD。跟 DT-ocean 类似，有个意义不明的 Corrected 的前缀。\nOptical_Depth_Ratio_Small_Land：0.55 μm 的 FMF，数值是间隔为 0.1 的离散值。同时如前所述，因为陆面的 FMF 和 AE 不靠谱，所以最新的 C6.1 版本里陆面的 AE 和 fine model AOD 惨遭移除，如果有需求必须手动计算。\nQuality_Assurance_Land：QA 标记，推荐直接使用 Land_Ocean_Quality_Flag，例如\nCorrected_Optical_Depth_Land[:, Land_Ocean_Quality_Flag \u0026lt; 3] = np.nan  还是 DT-ocean 中那个例子，画出 0.55 μm 的 AOD 和 FMF\n可以看到江苏省区域 AOD 较大FMF 偏高。同时注意到西边 AOD 较低的区域 FMF 直接缺测。\nDT-ocean 和 DT-land 联合的变量 为了便于使用，MYD04_L2 提供两个把 DT-ocean 和 DT-land 结果合并的变量：\n Image_Optical_Depth_Land_And_Ocean：把 0.55 μm 的陆面 AOD 和 average solution 的洋面 AOD 直接拼在一起。因为没做任何质量控制，所以有值的像元很多，适合做图，这也呼应了其名字中的 image 一词。 Optical_Depth_Land_And_Ocean：类似于前者，但洋面要求 QAC \u0026gt; 0，陆面要求 QAC = 3。虽然会使很多像元缺测，但更适合用于定量研究。  以前面的例子做图\n这下终于能在地图上连续展示 AOD 的水平分布了。右图中陆面有值的像元数量有所缩水，但还可以接受。\nDB 算法及其变量 沙漠和干旱地区的地表反射率在可见光波段很高，但在 412 nm 的深蓝波段又变得很低，DB 算法便是利用这一信息实现了亮地表的反演，同时也适用于一般的植被覆盖区域。具体算法详见 Hsu 等的一系列论文，类似于 DT-land，同样需要选取气溶胶模型、构建 LUT、估算地表反射率、模拟大气层顶的反射率等。DB 算法主要基于 412、470 和 650 nm 波段的观测，反演的主要变量是 AOD（550 nm） 和 AE（亮地表对应 412/470，暗地表对应 470/670）。后者定义为 $$ \\alpha = \\frac{\\ln(\\tau_{\\lambda_1}/\\tau_{\\lambda_2})}{\\ln(\\lambda_1/\\lambda_2)} $$ 一般来说 AE 小于 1 暗示粗粒子，大于 1 暗示细粒子。下面这张图展示了常见气溶胶的 AOD 和 AE\nDB 同样有 QA 标记，与 DT 的区别在于 0 直接表示缺测。\n肯定会有人问，既然 DT 和 DB 都能反演陆面，那么选哪个好？据 Sayer 等的验证研究，全球陆面上二者反演的 AOD 及其与 AERONET 观测的一致性都很相近，虽然 DT 与 AERONET 的相关性更好，但 DB 可以反演的空间范围更广，并且在低 AOD 情况下的误差更小。也许在一些特定的场合某个算法表现会更好，但一般任选一个使用就行，详细的区别还请自行查阅。\nDB 相关的主要变量如下表所示\nDeep_Blue_Aerosol_Optical_Depth_550_Land：550 nm 的 AOD。\nDeep_Blue_Aerosol_Optical_Depth_550_Land_Best_Estimate：同上，但是仅选取 QA \u0026gt; 1 的值，官方将其描述为“大部分用户应该使用的量”。\nDeep_Blue_Aerosol_Optical_Depth_550_Land_QA_Flag：QA 标记，不需要解码直接使用即可。\nDeep_Blue_Angstrom_Exponent_Land：AE，亮地表对应于 412/470，暗地表对应于 470/670。\n接下来画出 AOD 和 AE 看看\nAOD 的水平分布与 DT-land 一致，但有值的像元增加了很多。山东和江苏区域的 AOD 值很高，甚至能超过 2.0，而 1.7 左右的 AE 暗示气溶胶粒径偏小。\nDT 和 DB 联合的变量 由前面的介绍，DT 算法适用于洋面和植被茂盛的陆面，而 DB 算法是唯一能反演亮表面上 AOD 的算法，将二者联合起来，是不是就能获得整个地表的结果呢？C6 版本便引入了这样的联合变量，目前为 Aqua 卫星专供（MYD04_L2），合并策略如下：\n 洋面选用 DT-ocean AOD。 对于陆面，当多年月平均的 NDVI \u0026gt; 0.3，即植被茂密时，选用 DT-land AOD；当 NDVI \u0026lt; 0.2，即植被稀疏时，选用 DB AOD；当 NDVI 处于中间范围时：  若 QA_DB \u0026gt; 1 且 QA_DT = 3，取两种 AOD 的平均值。 若有一方的 QA 不满足上一条，则选取置信度高的那一方。 若双方的 QA 都不达标，则设为缺测。    相关变量为：\n AOD_550_Dark_Target_Deep_Blue_Combined：顾名思义，DT 和 DB 合并得到的 0.55 μm AOD。 AOD_550_Dark_Target_Deep_Blue_Combined_QA_Flag：继承自 QA_DT 和 QA_DB 的 QA。 AOD_550_Dark_Target_Deep_Blue_Combined_Algorithm_Flag：像元具体采用了哪个算法，0 表示 DT，1 表示 DB，2 表示混合（即平均）。  虽然相关论文表示只有当 NDVI 处于中间值时才会对 AOD 的 QA 有要求，但对 C6.1 的产品进行测试和画图后发现实际上联合变量选取的都是高质量的 AOD：洋面要求 QA_DT \u0026gt; 0，陆面要求 QA_DT = 3 或 QA_DB \u0026gt; 1。所以联合变量自带的 QA 就显得非常鸡肋了。另外需要注意这个量的可靠性目前还没有得到系统性的验证。下面画出这两个量\n可以看出中国东南区域的合并 AOD 主要基于 DT，到了北边才开始使用 DB。\n格点化处理 本节以 2021 年 3 月 15 日中国发生的沙尘暴天气为例，演示如何把一系列 MYD04_L2 文件里的 AOD 数据格点化并画出来。AOD 就选取上一节介绍的 AOD_550_Dark_Target_Deep_Blue_Combined，在南方植被茂密的区域会选用成熟的 DT 算法，而在北方的干旱区域又能发挥 DB 算法的优势，并且只含高 QA 的结果。下图为当天的 Aqua MODIS 真彩色图\n中国北方一带明显可见浓厚的黄色沙尘，甚至还卷入了东边的气旋中，不难预测这些区域会有特别高的 AOD。下载 MYD04_L2 产品可以到 NASA 的 EARTHDATA 网站，选中这一天，经纬度范围设为 70°E ~ 140°E，10°N ~ 60°N，最后会筛选出十多个文件。代码和结果如下\nfrom pathlib import Path import numpy as np from scipy.stats import binned_statistic_2d import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap from matplotlib.ticker import MultipleLocator import cartopy.crs as ccrs from reader import ReaderMYD from map_funcs import add_Chinese_provinces, set_map_extent_and_ticks def grid_data(x, y, data, xbins, ybins): ''' 利用平均的方式格点化一维散点数据. 没有数据的格点记作NaN.若格点内的数据全部缺测会产生warning,可以无视. 为了便于画图,结果的维度为(ny, nx). ''' avg, ybins, xbins, _ = binned_statistic_2d( y, x, data, bins=[ybins, xbins], statistic=np.nanmean ) xc = (xbins[1:] + xbins[:-1]) / 2 yc = (ybins[1:] + ybins[:-1]) / 2 return xc, yc, avg def aod_cmap(): '''制作适用于AOD的cmap.''' rgb = np.loadtxt('./NEO_modis_aer_od.csv', delimiter=',') cmap = ListedColormap(rgb / 256) return cmap if __name__ == '__main__': # 读取MYD04_L2文件,收集所有granule的经纬度和AOD数据. dirpath = Path('./data') lon_all, lat_all, aod_all = [], [], [] for filepath in dirpath.iterdir(): with ReaderMYD(str(filepath)) as f: lon, lat = f.read_lonlat() aod = f.read_sds('AOD_550_Dark_Target_Deep_Blue_Combined') lon_all.append(lon.ravel()) lat_all.append(lat.ravel()) aod_all.append(aod.ravel()) # 连接为一维数组. lon_all = np.concatenate(lon_all) lat_all = np.concatenate(lat_all) aod_all = np.concatenate(aod_all) # 设定网格. extents = [70, 140, 10, 60] lonmin, lonmax, latmin, latmax = extents dlon, dlat = 1, 1 lon_bins = np.arange(lonmin, lonmax + 0.5 * dlon, dlon) lat_bins = np.arange(latmin, latmax + 0.5 * dlat, dlat) # 格点化. lon_grid, lat_grid, aod_grid = grid_data( lon_all, lat_all, aod_all, lon_bins, lat_bins ) # 设置地图. proj = ccrs.PlateCarree() fig = plt.figure() ax = fig.add_subplot(111, projection=proj) add_Chinese_provinces(ax, lw=0.3, ec='k', fc='none') ax.coastlines(resolution='10m', lw=0.3) set_map_extent_and_ticks( ax, extents, xticks=np.arange(-180, 190, 10), yticks=np.arange(-90, 100, 10), nx=1, ny=1 ) ax.tick_params(labelsize='x-small') # 画出格点化的AOD. cmap = aod_cmap() cmap.set_bad('lightgray') # 缺测设为灰色. im = ax.pcolormesh( lon_grid, lat_grid, aod_grid, cmap=cmap, vmin=0, vmax=3, shading='nearest', transform=proj ) cbar = fig.colorbar( im, ax=ax, ticks=MultipleLocator(0.5), extend='both', shrink=0.8, pad=0.1, aspect=30, orientation='horizontal' ) cbar.set_label('AOD', fontsize='small') cbar.ax.tick_params(labelsize='x-small') # 日期作为标题. ax.set_title('2021-03-15', fontsize='medium') # 保存图片. fig.savefig('gridded.png', dpi=200, bbox_inches='tight') plt.close(fig)  格点化采用计算落入格子中的数据点的平均值的方式，可以通过写循环数格子来实现，不过这里选择直接调用 scipy.stats.binned_statistic_2d 函数。该函数能统计一维散点数据在二维数据框（bin）中的统计值，在计算平均值时会正确地将空格点设为 NaN。此外为了美观，colormap 选用 Panoply Additional Color Tables 中的 NEO_modis_aer_od，颜色从米黄向深红过渡，个人觉得很适合展示 AOD。不过下载到的 colormap 是 Adobe 的 ACT 格式，需要用这篇 Stack Exchange 回答 中的代码转换为 CSV 格式以方便读取。\n画出来的结果不出所料，一条长长的“沙龙”从新疆延伸到东北，AOD 超过了 3.0，这算是非常高的值了，暗示会给沿途的地面带来强烈的污染。中国以北因为存在大片云系所以基本都缺测了。调查资料可知这次沙尘天气的两大成因：一是我国西北和蒙古国地区前期气温偏高、降水稀少，因而地表容易起沙；二是蒙古气旋带来的大风为沙尘的扬起和传输提供了动力条件。\n总结 MODIS 气溶胶产品提供的变量众多，抛开前文冗长的讲解，这里给出极简使用指南。\n想画 AOD 图：\n Image_Optical_Depth_Land_And_Ocean  想定量研究 AOD：\n Optical_Depth_Land_And_Ocean  专注于洋面上的气溶胶：\n Effective_Optical_Depth_0p55um_Ocean Optical_Depth_Ratio_Small_Ocean_0.55micron Land_Ocean_Quality_Flag  专注陆面上的气溶胶：\n Corrected_Optical_Depth_Land Optical_Depth_Ratio_Small_Land Land_Ocean_Quality_Flag  需要研究沙漠和干旱地区：\n Deep_Blue_Aerosol_Optical_Depth_550_Land Deep_Blue_Angstrom_Exponent_Land Deep_Blue_Aerosol_Optical_Depth_550_Land_QA_Flag  想结合暗目标算法和深蓝算法的结果：\n AOD_550_Dark_Target_Deep_Blue_Combined  如果你需要用到 C6.1 之前的产品，例如 C5 产品，很可能会发现许多变量名字不一样，甚至是找不到。不过，要是你理解了各个算法的基本概念，再多去官网搜搜，一定能推理并找出所需的变量。就算以后 MODIS 产品有了新版本，或者说要面对其它采用了同样算法的卫星产品（例如 VIIRS、NPP）也应该不会有问题。\n参考资料 MODIS 及其气溶胶产品的基本介绍：\nWikipedia: Moderate Resolution Imaging Spectroradiometer\nAtmosphere Discipline Team Imager Products: Aerosol (04_L2)\nThe Collection 6 MODIS aerosol products over land and ocean\nUnderstanding the Details of the MODIS Aerosol Product（某个 Training 的 PPT，我搜不到链接了）\nDARK TARGET AEROSOL PRODUCTS USER’S GUIDE\nDT 和 DB 算法小组的官网，详细介绍了算法和数据使用的建议：\nDark Target Website\nDeep Blue Website\n利用 Python 读取 MYD04_L2 文件的教程：\nHow to read a MODIS HDF4 file using python and pyhdf ?\nARSET - Data Analysis Tools for High Resolution Air Quality Satellite Datasets\nMORE PYTHON COMPREHENSIVE EXAMPLES FOR LAADS MOD PRODUCTS\n","date":"2021-12-04","permalink":"https://zhajiman.github.io/post/guide_to_modis_aerosol_product/","tags":["卫星","python"],"title":"MODIS 二级气溶胶产品指北（with Python）"},{"content":" 这是 @skotaro 在 2018 年发布的一篇关于 Matplotlib Artist 的通俗易懂的介绍，很适合作为官方教程的前置读物，故翻译于此，原文标题是 \u0026ldquo;Artist\u0026rdquo; in Matplotlib - something I wanted to know before spending tremendous hours on googling how-tos.。文中绝大部分链接都重定向到了最新版本的 Matplotlib 文档，还请读者注意。\n 毫无疑问 Python 里的 matplotlib 是个非常棒的可视化工具，但在 matplotlib 中慢慢调细节也是件很烦人的事。你很容易浪费好几个小时去找修改图里细节的方法，有时你连那个细节的名字也不知道的话，搜索起来会更加困难。就算你在 Stack Overflow 上找到了相关的提示，也可能再花几个小时根据需求来修改它。不过，只要了解了 matplotlib 图的具体组成，以及你可以对组件执行的操作，就能避开这些徒劳无益的工作。我想，我跟你们中的大多数人一样，做图时遇到的困难都是靠读 Stack Overflow 上那些 matplotlib 高手们的答案来解决的。最近我发现 官方的 Artist 对象教程 信息很丰富，有助于我们理解 matplotlib 的画图过程并节省调图时间1。本文里我会分享一些关于 matplotlib 里 Artist 对象的基本知识，以避免浪费数小时调图的情况出现。\n本文的目的 我并不打算写那种“想要这个效果时你得如何如何”的操作说明，而是想介绍 matplotlib 中 Artist 的基本概念，这有助于你挑选搜索时的关键词，并为遇到的同类问题想出解决方案。读完本文，你应该就能理解网上那些海量的程序片段了。本文同样适用于用 seaborn 和 pandas 画图的人——毕竟这两个包只是对 matplotlib 的封装罢了。\n内容 本文基本上是 我之前写的日文版文章 的英文版，内容主要基于 Artist tutorial 和 Usage Guide（原文发布时版本为 2.1.1）。\n目标读者 这样的 matplotlib 使用者：\n 有能力根据需求画图，但要把图改到适合出版或展示的水平总是会很吃力（并且会为离预期效果就差那么一点而感到恼火）。 成功在 Stack Overflow 上找到了确切的解决方案，但对其工作原理仍然一知半解，也无法举一反三到其它问题上。 找到了好几个关于问题的提示，但不确定要选哪个。  环境  Python 3.6 matplotlib 2.2  %matplotlib inline\rimport matplotlib.pyplot as plt\rimport numpy as np\r 因为我开启了 Jupyter notebook 的行内绘图，所以本文略去了 plt.show()。\n你需要注意的两种画图风格 在研究 Artist 对象之前，我想先提一下 plt.plot 和 ax.plot——或者说 Pyplot 和面向对象的 API——之间的差别。虽然官方推荐面向对象的 API 风格，但包括官方文档在内的很多地方还是存在许多 Pyplot 风格的例子和代码，甚至还有莫名其妙混用两种风格的，这显然会迷惑初学者。因为官方文档对此已经有过很好的注解，比如 A note on the Object-Oriented API vs. Pyplot 和 Coding Styles，所以我在这里只会简单解释一下。如果你需要关于这个话题的入门资料，我推荐官方教程：\n Tutorials \u0026gt; Introductory \u0026gt; The Lifecycle of a plot Tutorials \u0026gt; Introductory \u0026gt; Pyplot tutorial  面向对象的 API 接口 这是最为推荐的风格，一般以 fig, ax = plt.subplots() 或其它等价的语句开头，后跟 ax.plot、ax.imshow 等。实际上，这里的 fig 和 ax 就是 Artist。下面是几个最简单的例子：\nfig, ax = plt.subplots()\rax.plot(x, y)\r fig = plt.figure()\rax = fig.add_subplot(1,1,1)\rax.plot(x, y)\r 有些教程会用 fig = plt.gcf() 和 ax = plt.gca()，当你从 Pyplot 接口切换到面向对象接口时确实应该这么写，但有些纯 Pyplot 风格的代码里还写些无意义的 ax = plt.gca() ，这显然是无脑从面向对象代码里抄过来的。如果有意切换接口，那么使用 plt.gcf() 和 plt.gca() 并不是什么坏事。考虑到隐式切换可能会迷惑初学者，绝大部分情况下从一开始就显式地使用 plt.subplots 或 fig.add_subplot 就是最好的做法。\nPyplot 接口 这是一种 MATLAB 用户熟悉的画图风格，其中所有操作都是 plt.xxx 的形式：\n# https://matplotlib.org/stable/tutorials/introductory/pyplot.html\rdef f(t):\rreturn np.exp(-t) * np.cos(2*np.pi*t)\rt1 = np.arange(0.0, 5.0, 0.1)\rt2 = np.arange(0.0, 5.0, 0.02)\rplt.figure(1)\rplt.subplot(211)\rplt.plot(t1, f(t1), 'bo', t2, f(t2), 'k')\rplt.subplot(212)\rplt.plot(t2, np.cos(2*np.pi*t2), 'r--')\rplt.show()\r 刚开始的时候你可能会觉得这种风格非常简单，因为不需要考虑你正在操作哪个对象，而只需要知道你正处于哪个“状态”，因此这种风格又被称作“状态机”。这里“状态”的意思是目前你在哪张图（figure）和哪张子图（subplot）里。正如你在 Pyplot tutorial 里看到的，如果你的图不是很特别复杂的话，这种风格能给出不错的效果。虽然 Pyplot 接口提供了许多函数来设置图片，但你可能不到一会儿就会发现这些功能还不够用，具体时间取决于你想要的效果，也许不到几小时、几天、几个月就会这样（当然运气好的话你不会碰到问题）。到了这一阶段你就需要转到面向对象接口了，这也是我推荐从一开始就使用面向对象接口的原因之一。不过当你需要快速验证或只想画点草图时，Pyplot 还是有挺有用的。\nMatplotlib 的层级结构 在网上搜索几次后，你会注意到 matplotlib 有一个层级结构，由通常叫做 fig 和 ax 的东西组成。Matplotlib 1.5 的旧文档 里有张图能很好地解释这个：\n实际上，图中这三个组件是被称为“容器”的特殊 Artist（Tick 是第四种容器），我们后面还会再谈到容器。透过这种层级结构，前面举的简单例子会显得更加清晰：\nfig, ax = plt.subplots() # 创建 Figure 和属于 fig 的 Axes\r fig = plt.figure() # 创建 Figure\rax = fig.add_subplot(1,1,1) # 创建属于 fig 的 Axes\r 进一步查看 fig 和 ax 的属性能加深我们对层级结构的理解：\nfig = plt.figure()\rax = fig.add_subplot(1,1,1) # 创建一个空的绘图区域\rprint('fig.axes:', fig.axes)\rprint('ax.figure:', ax.figure)\rprint('ax.xaxis:', ax.xaxis)\rprint('ax.yaxis:', ax.yaxis)\rprint('ax.xaxis.axes:', ax.xaxis.axes)\rprint('ax.yaxis.axes:', ax.yaxis.axes)\rprint('ax.xaxis.figure:', ax.xaxis.figure)\rprint('ax.yaxis.figure:', ax.yaxis.figure)\rprint('fig.xaxis:', fig.xaxis)\r fig.axes: [\u0026lt;matplotlib.axes._subplots.AxesSubplot object at 0x1167b0630\u0026gt;]\rax.figure: Figure(432x288)\rax.xaxis: XAxis(54.000000,36.000000)\rax.yaxis: YAxis(54.000000,36.000000)\rax.xaxis.axes: AxesSubplot(0.125,0.125;0.775x0.755)\rax.yaxis.axes: AxesSubplot(0.125,0.125;0.775x0.755)\rax.xaxis.figure: Figure(432x288)\rax.yaxis.figure: Figure(432x288)\r--------------------------------------------------------------------------------\rAttributeError Traceback (most recent call last)\r\u0026lt;ipython-input-21-b9f2d5d9fe09\u0026gt; in \u0026lt;module\u0026gt;()\r9 print('ax.xaxis.figure:', ax.xaxis.figure)\r10 print('ax.yaxis.figure:', ax.yaxis.figure)\r--------\u0026gt; 11 print('fig.xaxis:', fig.xaxis)\rAttributeError: 'Figure' object has no attribute 'xaxis'\r 根据这些结果我们可以归纳以下几条关于 Figure、Axes 和 Axis 层级结构的规则：\n Figure 知道 Axes，但不知道 Axis。 Axes 同时知道 Figure 和 Axis。 Axis 同时知道 Axes 和 Figure。 Figure 可以容纳多个 Axes，因为 fig.axes 是一个由 Axes 组成的列表。 Axes 只能属于一个 Figure，因为 ax.figure 不是列表。 基于类似的理由，Axes 只能有一个 XAxis 和一个 YAxis。 XAxis 和 YAxis 只能属于一个 Axes，因而也只能属于一个 Figure。  图中一切皆为 Artist 目前 Usage Guide 里并没有放解释层级结构的图，而是放了一张名为”剖析一张图（Anatomy of a figure）“的示意图2，同样信息量十足，阐述了一张图所含的全部组件3。\n从代表数据的线条和点到 X 轴的小刻度和文本标签，图中每个组件都是一个 Artist 对象4。Artist 分为容器（container）和图元（primitive）两种类型。正如我在上一节写到的，matplotlib 层级结构的三个组件——Figure、Axes 和 Axis 都是容器，可以容纳更低一级的容器和复数个图元，例如由 ax.plot 创建的 Line2D、ax.scatter 创建的 PathCollection，或 ax.annotate 创建的 Text。事实上，连刻度线和刻度标签都是 Line2D 和 Text，并且隶属于第四种容器 Tick。\n容器有许多存放各种图元的“盒子”（技术层面上就是 Python 列表），例如刚实例化的 Axes 对象 ax 会含有一个空列表 ax.lines，常用的 ax.plot 命令会往这个列表里添加一个 Line2D 对象，并在后台静默地进行相关设置。\nx = np.linspace(0, 2*np.pi, 100)\rfig = plt.figure()\rax = fig.add_subplot(1,1,1)\rprint('ax.lines before plot:\\n', ax.lines) # 空的\rline1, = ax.plot(x, np.sin(x), label='1st plot') # 往 ax.lines 里加 Line2D\rprint('ax.lines after 1st plot:\\n', ax.lines)\rline2, = ax.plot(x, np.sin(x+np.pi/8), label='2nd plot') # 再加一个 Line2D\rprint('ax.lines after 2nd plot:\\n', ax.lines)\rax.legend()\rprint('line1:', line1)\rprint('line2:', line2)\r ax.lines before plot:\r[]\rax.lines after 1st plot:\r[\u0026lt;matplotlib.lines.Line2D object at 0x1171ca748\u0026gt;]\rax.lines after 2nd plot:\r[\u0026lt;matplotlib.lines.Line2D object at 0x1171ca748\u0026gt;, \u0026lt;matplotlib.lines.Line2D object at 0x117430550\u0026gt;]\rline1: Line2D(1st plot)\rline2: Line2D(2nd plot)\r 接下来概述一下这四种容器，表格摘自 Artist tutorial。\nFigure    Figure 属性 描述     fig.axes 含有 Axes 实例的列表（包括 Subplot）   fig.patch 用作 Figure 背景的 Rectangle 实例   fig.images 含有 FigureImages 补丁（patch）的列表——用于显示 raw pixel   fig.legends 含有 Figure Legend 实例的列表（区别于 Axes.legends）   fig.lines 含有 Figure Line2D 实例的列表（很少用到，详见 Axes.lines）   fig.patches 含有 Figure 补丁的列表（很少用到，详见 Axes.patches）   fig.texts 含有 Figure Text 实例的列表    复数名的属性是列表，而单数名的则代表单个对象。值得注意的是属于 Figure 的 Artist 都默认使用 Figure 坐标，它 可以通过 Transforms 转换为 Axes 或数据的坐标，不过这个话题就超出本文的范围了。\nfig.legend 和 ax.legend 通过 fig.legend 方法 可以添加图例（legend），而 fig.legends 就是用来装这些图例的“盒子”。你可能会说“这有什么用？我们已经有了 ax.legend 啊。”区别在于二者的作用域不同，ax.legend 只会从属于 ax 的 Artist 里收集标签（label），而 fig.legend 会收集 fig 旗下所有 Axes 里的标签。举个例子，当你用 ax.twinx 画图时，单纯调用 ax.legend 只会创建出两个独立的图例，这通常不是我们想要的效果，这时 fig.legend 就派上用场了。\nx = np.linspace(0, 2*np.pi, 100)\rfig = plt.figure()\rax = fig.add_subplot(111)\rax.plot(x, np.sin(x), label='sin(x)')\rax1 = ax.twinx()\rax1.plot(x, 2*np.cos(x), c='C1', label='2*cos(x)') # cf. 'CN' 形式的记号\r# https://matplotlib.org/stable/tutorials/colors/colors.html#cn-color-selection\rax.legend()\rax1.legend()\r 将两个图例合并在一起的经典技巧是，把两个 Axes 的图例句柄（handle）和标签组合起来：\n# 在另一个 notebook 里执行这部分以显示更新后的图像\rhandler, label = ax.get_legend_handles_labels()\rhandler1, label1 = ax1.get_legend_handles_labels()\rax.legend(handler+handler1, label+label1, loc='upper center', title='ax.legend')\r# ax1.legend 创建的图例仍然存在\rfig\r 这个需求可以通过不给参数直接调用 fig.legend 来轻松解决（自 2.1 版本 引入5）。图例的位置默认使用 Figure 坐标，想把图例放在绘图框里面时会不太方便，你可以指定 bbox_transform 关键字改用 Axes 坐标：\nfig.legend(loc='upper right', bbox_to_anchor=(1,1), bbox_transform=ax.transAxes, title='fig.legend\\nax.transAxes')\rfig\r Axes  matplotlib.axes.Axes 是 matplotlib 体系的核心。\n 这句话出自 Artist tutorial，说的非常正确，因为在 matplotlib 中数据可视化的重要部分都是由 Axes 的方法完成的。\n   Axes 属性 描述     ax.artists 含有 Artist 实例的列表   ax.patch 用作 Axes 背景的 Rectangle 实例   ax.collections 含有 collection 实例的列表   ax.images 含有 AxesImage 实例的列表   ax.legends 含有 Legend 实例的列表   ax.lines 含有 Line2D 实例的列表   ax.patches 含有 Patch 实例的列表   ax.texts 含有 Text 实例的列表   ax.xaxis matplotlib.axis.XAxis 实例   ax.yaxis matplotlib.axis.YAxis 实例    常用的 ax.plot 和 ax.scatter 等命令被称为”辅助方法（helper methods）“，它们会将相应的 Artist 放入合适的容器内，并执行其它一些杂务。\n   辅助方法 Artist 容器     ax.annotate Annotate ax.texts   ax.bar Rectangle ax.patches   ax.errorbar Line2D \u0026amp; Rectangle ax.lines \u0026amp; ax.patches   ax.fill Polygon ax.patches   ax.hist Rectangle ax.patches   ax.imshow AxesImage ax.images   ax.legend Legend ax.legends   ax.plot Line2D ax.lines   ax.scatter PathCollection ax.collections   ax.text Text ax.texts    下面这个例子展示了 ax.plot 和 ax.scatter 分别将 Line2D 和 PatchCollection 对象添加到对应列表里的过程：\nx = np.linspace(0, 2*np.pi, 100)\rfig = plt.figure()\rax = fig.add_subplot(1,1,1)\rprint('ax.lines before plot:\\n', ax.lines) # 空的 Axes.lines\rline1, = ax.plot(x, np.sin(x), label='1st plot') # 把 Line2D 加入 Axes.lines\rprint('ax.lines after 1st plot:\\n', ax.lines)\rline2, = ax.plot(x, np.sin(x+np.pi/8), label='2nd plot') # 加入另一条 Line2D print('ax.lines after 2nd plot:\\n', ax.lines)\rprint('ax.collections before scatter:\\n', ax.collections)\rscat = ax.scatter(x, np.random.rand(len(x)), label='scatter') # 把 PathCollection 加入 Axes.collections\rprint('ax.collections after scatter:\\n', ax.collections)\rax.legend()\rprint('line1:', line1)\rprint('line2:', line2)\rprint('scat:', scat)\rax.set_xlabel('x value')\rax.set_ylabel('y value')\r ax.lines before plot:\r[]\rax.lines after 1st plot:\r[\u0026lt;matplotlib.lines.Line2D object at 0x1181d16d8\u0026gt;]\rax.lines after 2nd plot:\r[\u0026lt;matplotlib.lines.Line2D object at 0x1181d16d8\u0026gt;, \u0026lt;matplotlib.lines.Line2D object at 0x1181d1e10\u0026gt;]\rax.collections before scatter:\r[]\rax.collections after scatter:\r[\u0026lt;matplotlib.collections.PathCollection object at 0x1181d74a8\u0026gt;]\rline1: Line2D(1st plot)\rline2: Line2D(2nd plot)\rscat: \u0026lt;matplotlib.collections.PathCollection object at 0x1181d74a8\u0026gt;\r 不建议重复使用已经画好的对象 在知道了画好的对象会被存在列表里之后，你也许会灵机一动，尝试复用 Axes.lines 中的这些对象，即把它们添加到另一个 Axes.lines 列表中，以提高画图速度。Artist tutorial 里明确指出不推荐这样做，因为辅助方法除了创建 Artist 外还会进行很多其它必要的操作。随便测试一下就会发现这个思路确实行不通：\nx = np.linspace(0, 2*np.pi, 100)\rfig = plt.figure()\rax1 = fig.add_subplot(2,1,1) # 上面的子图\rline, = ax1.plot(x, np.sin(x), label='ax1 line') # 创建一个 Line2D 对象\rax1.legend()\rax2 = fig.add_subplot(2,1,2) # 下面的子图\rax2.lines.append(line) # 尝试着把同一个 Line2D 对象用于另一个 Axes\r 就算是 add_line 方法也不行：\nax2.add_line(line)\r ValueError: Can not reset the axes. You are probably trying to re-use an artist in more than one Axes which is not supported\r 报错信息表明，无论一个 Artist 是容器还是图元，都不能被多个容器同时容纳，这点也与前面提过的，每个 Artist 的父容器是单个对象而非列表的事实相一致：\nprint('fig:', id(fig)) print('ax1:', id(ax1))\rprint('line.fig:', id(line.figure))\rprint('line.axes:', id(line.axes))\r fig: 4707121584\rax1: 4707121136\rline.fig: 4707121584\rline.axes: 4707121136\r 理论上如果你以合适的方式把所有必要的操作都做好了，应该就行得通，但这就完全偏离了只是想向列表追加一个对象的初心，这么麻烦的事还是别做了吧。\nAxis Axis 以 XAxis 和 YAxis 的形式出现，虽然它们只含有与刻度和标签相关的 Artist，但若想细调还总得上网搜搜该怎么做，有时这会耗掉你一个钟头的时间。我希望这一小节能帮你快速搞定这事。\nArtist tutorial 里 Axis 不像其它容器那样有表格，所以我自己做了张类似的：\n   Axis 属性 描述     Axis.label 用作坐标轴标签的 Text 实例   Axis.majorTicks 用作大刻度（major ticks）的 Tick 实例的列表   Axis.minorTicks 用作小刻度（minor ticks）的 Tick 实例的列表    在前面 Axes 容器的例子里我们用到了 ax.set_xlabel 和 ax.set_ylabel，你可能认为这两个方法设置的是 Axes 实例（ax）的 X 和 Y 标签，但其实它们设置的是 XAxis 和 YAxis 的 label 属性，即 ax.xaxis.label 和 ax.yaxis.label。\nxax = ax.xaxis\rprint('xax.label:', xax.label)\rprint('xax.majorTicks:\\n', xax.majorTicks) # 七个大刻度(从0到6)和两个因为出界而看不到的刻度\rprint('xax.minorTicks:\\n', xax.minorTicks) # 两个刻度出界了(在图外面)\r xax.label: Text(0.5,17.2,'x value')\rxax.majorTicks:\r[\u0026lt;matplotlib.axis.XTick object at 0x117ae4400\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x117941128\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x11732c940\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x1177d0470\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x1177d0390\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x1175058d0\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x1175050b8\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x117bf65c0\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x117bf6b00\u0026gt;]\rxax.minorTicks:\r[\u0026lt;matplotlib.axis.XTick object at 0x117ab5940\u0026gt;, \u0026lt;matplotlib.axis.XTick object at 0x117b540f0\u0026gt;]\r ax.set_xxx 方法是暂时性的 Axes 有很多形如 set_xxx 的辅助方法，可以修改 Axis 和 Tick 的属性和值。这些方法用起来非常方便，matplotlib 初学者遇到的大部分问题都可以借助其中一些方法来解决。需要注意 set_xxx 方法都是静态的，它们的修改结果并不会随之后的改动而更新。例如，你在第一次 plot 之后用 ax.set_xticks 把 X 刻度改得很合适，接下来第二次 plot 超出了第一次 plot 圈定的 X 范围，那么结果就会不合预期：\nx = np.linspace(0, 2*np.pi, 100)\rfig = plt.figure()\rax = fig.add_subplot(1,1,1)\rline1, = ax.plot(x, np.sin(x), label='') # X 范围: 0 to 2pi\rax.set_xticks([0, 0.5*np.pi, np.pi, 1.5*np.pi, 2*np.pi])\rline2, = ax.plot(1.5*x, np.sin(x), label='') # X 范围: 0 to 3pi\r Ticker 帮你通通搞定 如果你不用 set_xxx 方法修改刻度参数，每次画上内容时刻度和刻度标签（tick label）会自动进行相应的更新。这归功于 Ticker，或者更准确点，formatter 和 locator。它们对于设置刻度来说极其重要，但如果你平时只靠复制粘贴 Stack Overflow 上的答案来解决问题，恐怕你对它们知之甚少6。让我们看看前一个例子里具体发生了什么吧：\n 译注：formatter 和 locator 似乎没有通用的译名，所以这里不译。\n xax = ax.xaxis\ryax = ax.yaxis\rprint('xax.get_major_formatter()', xax.get_major_formatter())\rprint('yax.get_major_formatter()', yax.get_major_formatter())\rprint('xax.get_major_locator():', xax.get_major_locator())\rprint('yax.get_major_locator():', yax.get_major_locator())\r xax.get_major_formatter() \u0026lt;matplotlib.ticker.ScalarFormatter object at 0x118af4d68\u0026gt;\ryax.get_major_formatter() \u0026lt;matplotlib.ticker.ScalarFormatter object at 0x118862be0\u0026gt;\rxax.get_major_locator(): \u0026lt;matplotlib.ticker.FixedLocator object at 0x1188d5908\u0026gt;\ryax.get_major_locator(): \u0026lt;matplotlib.ticker.AutoLocator object at 0x118aed1d0\u0026gt;\r X 和 Y 轴都设置有 ScalarFormatter，因为这是默认的 formatter，并且我们也没有对其进行改动。另一方面，Y 轴设置的是默认的 AutoLocator，而 X 轴因为我们用 ax.set_xticks 改变了刻度的位置，现在被设置为 FixedLocator。顾名思义，FixedLocator 使用固定的刻度位置，即便之后画图区域变了也不会更新刻度位置。\n接着让我们用 ax.set_xticks 以外的方法来改变上个例子中的 Ticker：\nimport matplotlib.ticker as ticker # 想使用 Ticker 必须要这一句\rax.xaxis.set_major_locator(ticker.MultipleLocator(0.5*np.pi)) # 每隔 0.5*pi 确定一个刻度\rfig # 展示应用了新 locator 的 figure\r 再来看看 formatter：\n@ticker.FuncFormatter # FuncFormatter 可以用作装饰器\rdef major_formatter_radian(x, pos):\rreturn '{}$\\pi$'.format(x/np.pi) # 这可能不是显示弧度单位的刻度标签的最好方法\rax.xaxis.set_major_formatter(major_formatter_radian)\rfig\r 好了，可能你还有想调整的地方，但我觉得讲到这儿已经够清晰了。\n你可以在 matplotlib gallery 里学到更多：\nGallery \u0026gt; Tick formatters\nGallery \u0026gt; Tick locators\nax.plot 的 xunits 关键字 顺便一提，ax.plot 有个目前 还没有说明文档 的关键字 xunits，我自己是从来没用过，但你可以在 Gallery \u0026gt; Radian ticks 页面看到例子，更多关于 matplotlib.units.ConversionInterface 的内容请点 这里。\nimport numpy as np\rfrom basic_units import radians, degrees, cos\rfrom matplotlib.pyplot import figure, show\rx = [val*radians for val in np.arange(0, 15, 0.01)]\rfig = figure()\rfig.subplots_adjust(hspace=0.3)\rax = fig.add_subplot(211)\rline1, = ax.plot(x, cos(x), xunits=radians)\rax = fig.add_subplot(212)\rline2, = ax.plot(x, cos(x), xunits=degrees)\r Tick 终于，我们抵达了 matplotlib 层级结构的底部。Tick 是个很小的容器，主要容纳表示刻度的短线和表示刻度标签的文本。\n   Tick 属性 描述     Tick.tick1line Line2D 实例   Tick.tick2line Line2D 实例   Tick.gridline 用作网格的 Line2D 实例   Tick.label1 Text 实例   Tick.label2 Text 实例   Tick.gridOn 控制是否画出网格线的布尔量   Tick.tick1On 控制是否画出第一组刻度线的布尔量   Tick.tick2On 控制是否画出第二组刻度线的布尔量   Tick.label1On 控制是否画出第一组刻度标签的布尔量   Tick.label2On 控制是否画出第二组刻度标签的布尔量    类似于 Axis，Tick 同样以 XTick 和 YTick 的形式出现。第一组和第二组分别指上边和下边的 XTick，以及左边和右边的 YTick，不过第二组默认是隐藏的。\nxmajortick = ax.xaxis.get_major_ticks()[2] # 上一张图里每隔 0.5 pi 出现的刻度\rprint('xmajortick', xmajortick)\rprint('xmajortick.tick1line', xmajortick.tick1line)\rprint('xmajortick.tick2line', xmajortick.tick2line)\rprint('xmajortick.gridline', xmajortick.gridline)\rprint('xmajortick.label1', xmajortick.label1)\rprint('xmajortick.label2', xmajortick.label2)\rprint('xmajortick.gridOn', xmajortick.gridOn)\rprint('xmajortick.tick1On', xmajortick.tick1On)\rprint('xmajortick.tick2On', xmajortick.tick2On)\rprint('xmajortick.label1On', xmajortick.label1On)\rprint('xmajortick.label2On', xmajortick.label2On)\r xmajortick \u0026lt;matplotlib.axis.XTick object at 0x11eec0710\u0026gt;\rxmajortick.tick1line Line2D((1.5708,0))\rxmajortick.tick2line Line2D()\rxmajortick.gridline Line2D((0,0),(0,1))\rxmajortick.label1 Text(1.5708,0,'0.5$\\\\pi$')\rxmajortick.label2 Text(0,1,'0.5$\\\\pi$')\rxmajortick.gridOn False\rxmajortick.tick1On True\rxmajortick.tick2On False\rxmajortick.label1On True\rxmajortick.label2On False\r 得益于各种辅助方法、Ticker 和 Axes.tick_params，基本上我们不需要直接操作 Tick。\n是时候自定义你的默认样式了 来瞧瞧默认样式的一系列参数吧。\nTutorials \u0026gt; Customizing matplotlib \u0026gt; A sample matplotlibrc file\n我猜你现在应该能理解各个参数的作用，并且知道参数具体作用于哪个 Artist 了，这样一来以后搜索时可以节省大把时间7。除了通过创建 matplotlibrc 文件来自定义默认样式，你还可以直接在代码开头写上这种语句：\nplt.rcParams['lines.linewidth'] = 2\r 去看文档吧（又来了） 有些读者可能对 matplotlib 文档印象不好，我也承认，从那么长的文章列表里为你的问题找出一个合适的例子还挺难的。但其实文档自 2.1.0 版本以来改进了很多8，当你对比改进前后的同一页面时尤为明显。\n   2.1.0（2017 年 10 月） 2.0.2（2017 年 5 月）     Gallery, Tutorials Matplotlib Examples, Thumbnail gallery   Overview Overview    我推荐你看一眼 最新的 gallery 和 Tutorials，现在的效果真的很赏心悦目。\n 译注：神秘的是，2.1.0 开始 Examples 页面改名为 Gallery，而到了 3.5.0，又改回 Examples 了，但网址里还是写的 gallery。\n 感谢你读到这里，尽情享受 matplotlib 绘图（和网络搜索）吧 📈🤗📊\n封面图来自 Caleb Salomons on Unsplash\n  没错，如果你不是那种使用前连教程都不读的懒人，那么教程总会是信息丰富和大有裨益的。其实几年前我刚开始用 matplotlib 画图时好像就试过读 Artist 的文档，但可以确定的是，我当时心里肯定想着“好吧，这不是给我这种用户读的”（也有可能当时读的不是现在的官方教程）。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 制作这张图的示例代码在 https://matplotlib.org/stable/gallery/showcase/anatomy.html。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 当然还存在其它的 Artist，想一览总体概貌的读者可以从 这个页面 入手。点击每个 Artist 的名字能看到更多说明。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 技术上来说，在 matplotlib 里，艺术家（Artist）会把你美丽的数据绘制在画布（canvas）上。这修辞还蛮可爱的。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 以前版本里的 fig.legend 要比现在难用，因为必须显式给出图例句柄和标签作为参数（据 文档 2.0.2）。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 当你不满于 set_xxx 之类的方法，更进一步搜索刻度相关的设置时，将会遇到许多使用 formatter 和 locator 的程序片段——然后摸不着头脑，只能放弃在自己的问题里应用它们（其实几个月前的我就是这样的）。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 或者你可以像我一样用省下的时间继续钻研 matplotlib。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 关于改进文档有多困难，这儿有篇不错的资料可以读读：Matplotlib Lead Dev on Why He Can\u0026rsquo;t Fix the Docs | NumFOCUS\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n  ","date":"2021-11-23","permalink":"https://zhajiman.github.io/post/matplotlib_artist/","tags":["matplotlib"],"title":"Matplotlib 中的 Artist——你在浪费时间瞎百度之前应该知道的东西"},{"content":"之前我在 Cartopy 系列：从入门到放弃 一文中定义了这样一个函数\ndef set_map_extent_and_ticks(\rax, extent, xticks, yticks, nx=0, ny=0,\rxformatter=LongitudeFormatter(),\ryformatter=LatitudeFormatter()\r):\r...\r 其功能是限制 GeoAxes 的经纬度范围，并画出经纬度刻度。其中 LongitudeFormatter 和 LatitudeFormatter 是 Cartopy 定义的两个 Formatter 类，用于格式化经纬度刻度标签。Formatter 对象因为其属性可以任意修改，所以也可以算作可变对象（are user defined classes mutable）。What the f*ck Python! 中提到过\n Python中函数的默认可变参数并不是每次调用该函数时都会被初始化。相反，它们会使用最近分配的值作为默认值，除非明确地将可变对象传递给函数。\n 也就是说，多次调用 set_map_extent_and_ticks 时如果不指定 xformatter 和 yformatter，就会一直沿用第一次调用时创建的 Formatter 对象，这一点可以通过打印对象的 id 来验证。而 Formatter 作为一种 Matplotlib Artist，被重复使用时可能会产生错误的结果（早く知っておきたかったmatplotlibの基礎知識、あるいは見た目の調整が捗るArtistの話）。我就因为对不同投影的多个 GeoAxes 连续使用 set_map_extent_and_ticks，画出了错误的刻度。\n避免可变参数导致的错误的常见做法是将 None 指定为参数的默认值，在函数体内判断是否创建默认的可变对象。所以这个函数应该修改为\ndef set_map_extent_and_ticks(\rax, extent, xticks, yticks, nx=0, ny=0,\rxformatter=None, yformatter=None\r):\r...\r","date":"2021-11-14","permalink":"https://zhajiman.github.io/post/python_mutable_arguments/","tags":["python"],"title":"Python 系列：小心默认的可变参数"},{"content":"前言 几年前曾写过 Cartopy 系列：从入门到放弃，但现在来看还是遗漏了不少细节，比如初学者可能会遇到以下问题\n 经度是用 [-180°, 180°] 还是 [0°, 360°] 范围？ 为什么有时候设置的刻度显示不全？ 怎么截取跨越地图边界的区域，画图又怎么跨越边界？  本文将会用坐标变换的思想来解答以上问题，希望能给读者一些实用的启示。本来应该把这些内容写到入门教程里的，但可能会太长，所以现在单独成篇。文中的讨论主要针对最常用的 Plate Carrée 投影，其它投影需要读者自己测试。代码基于 Cartopy 0.18.0，虽然现在已经更新到 0.20.0 了，但基本思想是一致的。\n经度的循环性 经度的数值范围一般有两种表示：[-180°, 180°] 或 [0°, 360°]。前者表示以本初子午线（zero meridian）为中心，向西向东各 180°，再在对向子午线（antimeridian）处交汇；后者表示以本初子午线为起点向东 360°，又绕回了本初子午线。经度这种绕圈转的量很容易让人联想到时钟的表盘，本初子午线就对应于 0 时（实际上“子午”一词指的就是夜半和正午），[-180°, 180°] 范围对应于使用 AM 和 PM 标记的计时方式，[0°, 360°] 范围对应于二十四小时制。如下图所描绘的那样\n一个小区别是：表盘的指针是顺时针旋转的，而经度的“指针”从北极往下看的话，是自西向东，也就是逆时针旋转的。\n两个范围的经度在 [0°, 180°] 区间是等价的，大于 180° 的经度减去 360° 又可以换算到 [-180°, 0°] 范围内，例如 240° 就等价于 240° - 360° = -120°。在 Python 中可以通过下面的公式将 [0°, 360°] 范围的经度换算到 [-180°, 180°] 上\ndef convert_lon(lon):\r'''将经度换算到[-180, 180]范围内.'''\rreturn (lon + 180) % 360 - 180\rfor lon in range(-270, 450 + 90, 90):\rlon_new = convert(lon)\rprint(lon, '-\u0026gt;', lon_new)\r 结果为\n-270 -\u0026gt; 90\r-180 -\u0026gt; -180\r-90 -\u0026gt; -90\r0 -\u0026gt; 0\r90 -\u0026gt; 90\r180 -\u0026gt; -180\r270 -\u0026gt; -90\r360 -\u0026gt; 0\r450 -\u0026gt; 90\r 有趣的是，当经度超出了 [0°, 360°] 范围时上式依旧成立，例如 450° 表示从子午线出发绕地球一圈后再绕 90°，上面的结果中也恰好换算为 90°，同理带入 -240° 后换算成 120°。注意边界值 180° 被换算成了 -180°，不过考虑到这两个值对应于同一条经线，也还可以接受。所以只要借助这个公式，任意数值的经度都可以换算到 [-180°, 180°] 的范围内。\nCartopy 正好遵循这一特性，会自动换算我们给出的任意经度值（不过具体实现可能不同于 convert_lon 函数）。例如\nline_proj = ccrs.PlateCarree()\rax.plot([-60, 60], [0, 0], transform=line_proj)\rax.plot([300, 420], [0, 0], transform=line_proj)\r 两句 ax.plot 的画出来的效果是相同的，都画的是 [-60°, 60°] 之间的连线。但这并不意味着在 Cartopy 里经度只要换算过来合理，就可以随便设置了。例如对画图函数来说经度的大小顺序非常重要、对刻度设置来说因为存在 bug，效果也可能不同于预期。后面的小节会一一解说这些例外。\n理解坐标变换 地理坐标与投影坐标 地理坐标即经纬度，能够描述地球表面任意一点的位置；而投影坐标则是将地球球体投影到平面上得到的坐标。二者的数值和单位一般不同，但可以根据投影时用到的数学公式进行换算。画图用的源数据（站点位置、卫星像元网格、再分析网格等）一般基于地理坐标，而 Cartopy 地图（即 GeoAxes）因为处于屏幕这个平面上，自然是基于投影坐标的。\nCartopy 将坐标系称为“坐标参考系统”（coordinate reference system，CRS），并在 cartopy.crs 模块中定义了一系列表示 CRS 的类，其中也包括各种地图投影，比如 PlateCarree、Mercator、Mollweide、LambertConformal 类等。在创建 Axes 时将 CRS 对象传给 projection 参数，即可将 Axes 转为这个 CRS 代表的投影的 GeoAxes。例如下面这段代码分别创建了等经纬度投影和麦卡托投影的地图\nimport matplotlib.pyplot as plt\rimport cartopy.crs as ccrs\rproj1 = ccrs.PlateCarree()\rproj2 = ccrs.Mercator()\rfig = plt.figure()\rax1 = fig.add_subplot(211, projection=proj1)\rax2 = fig.add_subplot(212, projection=proj2)\r 下面以最常用的 PlateCarree 类为例讲解地理坐标和投影坐标的关系。PlateCarree 类有一个初始化参数 central_longitude，能够指定画全球地图（通过 ax.set_global 方法）时正中间的经度，默认值为 0，即全球地图默认会把本初子午线放在画面中心。若指定 central_longitude = 180，则全球地图会以对向子午线为中心，图这里就不放了。除这个功能以外，central_longitude 还会影响到 PlateCarree 坐标与地理坐标间的关系。PlateCarree 是一个标准的笛卡尔坐标系，其横坐标 x 与经度 lon 满足关系\nx = convert_lon(lon - central_longitude)\r 即经度减去 central_longitude 后再换算到 [-180°, 180°] 范围即可，显然 x 可以视作关于中央经度的相对经度。继续沿用上一节的表盘比喻，将二者的关系形象地表示为下图\n图中黑色表盘为经度 lon，将其逆时针旋转 central_longitude 度后即得到代表 x 的蓝色表盘。PlateCarree 的纵坐标 y 则与纬度 lat 直接对应，纬度是多少纵坐标就是多少。很容易注意到，当 central_longitude = 0 时，横坐标与经度直接对应，纵坐标与经度直接对应，即 PlateCarree 坐标正好等价于地理坐标。我们后面还会频繁用到这一点。\n举个例子，对投影 proj = ccrs.PlateCarree(central_longitude=180) 来说，地理坐标 (-160, 30) 对应于投影坐标 (20, 30)。这可以通过 Matplotlib 的 plt.show 函数创建的交互式界面得到直观验证\nMatplotlib 里若把鼠标指针放在 Axes 的图像上，窗口右上角就会显示指针位置的坐标。Cartopy 的 GeoAxes 增强了这一功能，还会在坐标后面的括号里显示对应的地理坐标。如上图所示，投影坐标 (20.32, 30.05) 对应的地理坐标为 (159.677419, 30.048387)。注意图中是纬度在前经度在后，且两种坐标对小数部分的显示有所不同，所以看起来像是有误差。探索一番还能发现，全球地图里 x 的范围为 [-180°, 180°]，y 的范围为 [-90°, 90°]，地图中央，也就是 central_longitude 所在位置的 x 总为 0°。Matplotlib 的这一功能对日常 debug 来说非常实用。\n此外 CRS 对象的 transform_points 方法能直接进行不同坐标系统间的坐标换算。例如\nimport numpy as np\rproj1 = ccrs.PlateCarree(central_longitude=0)\rproj2 = ccrs.PlateCarree(central_longitude=180)\rnpt = 5\rlon1 = np.linspace(-180, 180, npt)\rlat1 = np.linspace(-90, 90, npt)\rpos2 = proj2.transform_points(proj1, lon1, lat1)\rlon2 = pos2[:, 0]\rlat2 = pos2[:, 1]\rfor i in range(npt):\rprint(f'({lon1[i]}, {lat1[i]})', '-\u0026gt;', f'({lon2[i]}, {lat2[i]})')\r 其中 proj1 的中央经度为 0，如前所述，其投影坐标 lon1 和 lat1 正好代表经纬度。利用 proj1.transform_points 方法即可将 lon1 和 lat1 换算为 proj2 里的坐标 lon2 和 lat2。结果为\n(-180.0, -90.0) -\u0026gt; (0.0, -90.0)\r(-90.0, -45.0) -\u0026gt; (90.0, -45.0)\r(0.0, 0.0) -\u0026gt; (-180.0, 0.0)\r(90.0, 45.0) -\u0026gt; (-90.0, 45.0)\r(180.0, 90.0) -\u0026gt; (0.0, 90.0)\r 明显 lon2 相当于 lon1 减去了 180°，而 lat2 和 lat1 完全一致。在需要手动变换坐标的场合这个方法会派上用场。\n总结一下：PlateCarree 投影将地球投影到了平面笛卡尔坐标系里，横坐标相当于经度向右位移（逆时针旋转）了 central_longitude 度，纵坐标依然对应于纬度。PlateCarree 坐标与地理坐标的关系非常简单，但如果对于兰伯特、UTM 那种复杂的投影，坐标间的关系就不会这么直观了，甚至 x 和 y 的单位都不会是度，读者可以用前面提到的 Matplotlib 的交互式界面自行探索。\ncrs 和 transform 参数 由上一节的解说，Cartopy 官方文档里着重强调的 crs 和 transform 参数就很好理解了。\nGeoAxes 不仅工作在投影坐标系，其设置刻度的 set_xticks 和 set_yticks 方法、截取区域的 set_extent 方法，乃至各种绘图的 plot、contourf、pcolormesh 等方法等，都默认我们给出的数据也是基于投影坐标系的。所以需要提前把数据的地理坐标换算为地图的投影坐标，再把数据添加到地图上。例如下面这段代码\nmap_proj = ccrs.PlateCarree(central_longitude=180)\rfig = plt.figure()\rax = fig.add_subplot(111, projection=map_proj)\rax.set_xticks([0, 90])\r set_xticks 方法会在地图 x = 0 和 x = 90 的位置画出刻度——注意是 x 而不是经度！如果我们需要的是 lon = 0 和 lon = 90 处的刻度，就需要手动换算一下（根据上一节 x 和 lon 的关系式）\nax.set_xticks([-180, -90])\r PlateCarree 这样简单的投影还比较容易手动换算，如果是更复杂的兰伯特投影之类的，就需要利用 CRS 对象的 transform_points 方法了。但 Cartopy 能够通过 crs 和 transform 参数省略掉这一换算过程：通过将 CRS 对象传给设置刻度时的 crs 参数，或绘制图像时的 transform 参数，能够告知 Cartopy 你的数据基于这个 CRS 坐标系，之后 Cartopy 在内部会根据这一信息将你的数据换算到 GeoAxes 所处的坐标系中。因为我们的数据一般都基于地理坐标，所以我们常把等价于地理坐标系的 ccrs.PlateCarree() 对象传给 crs 和 transform 参数。例如上面在 lon = 0 处和 lon = 90 处标出刻度的写法可以改为\ntick_proj = ccrs.PlateCarree()\rax.set_xticks([0, 90], crs=tick_proj)\r 类似地，画出地理坐标 (0, 30) 和 (90, 30) 间的连线\nline_proj = ccrs.PlateCarree()\rax.plot([0, 90], [30, 30], transform=line_proj)\r 所以只要用好 crs 参数和 transform 参数，就可以忽略坐标转换的细节，统一使用地理坐标来描述和操作地图了。可能有人会指出，当地图投影 map_proj = ccrs.PlateCarree() 时 crs 和 transform 参数都可以省去，这确实没错，不过正如 Python 之禅说的，“显式胜于隐式”，显式地指定这些参数有助于明确坐标间的关系。\nGeodetic 坐标 前面说 ccrs.PlateCarree() 等价于地理坐标系是不严谨的，因为真正的地理坐标系定义在球面上，两点间的最短连线（测地线）是过这两点的大圆的劣弧；而 PlateCarree 坐标系定义在平面上，两点间的最短连线是过两点的直线。cartopy.crs 模块里的 Geodetic 类便能表示真正的地理坐标系，用于指定单点位置时其效果与 PlateCarree 无异，但在画两点间连线时将 Geodetic 对象传给 transform 参数，便能让连线变成球面上的测地线。例如\nx = [116, 286]\ry = [39, 40]\rax.plot(x, y, 'o-', transform=ccrs.PlateCarree(), label='PlateCarree')\rax.plot(x, y, 'o-', transform=ccrs.Geodetic(), label='Geodetic')\rax.legend()\r 虽然乍一看橙线比蓝线长，但投影回球面后，橙线才是两点间的最短连线。Geodetic 是一种 CRS，但不属于地图投影，所以不能用于 GeoAxes 的创建。平时画图时除非对测地线或大圆有需求，一般使用 PlateCarree 坐标即可，实际上，目前 Geodetic 对象还不能用作 contourf、pcolormesh 等画图函数的 transform 参数，可能是 Matplotlib 还无法实现曲线网格的填色吧。\n关于刻度设置 LongitudeFormatter 和 LatitudeFormatter 单纯使用 set_xticks 设置刻度后，刻度会以 x 的值作为刻度标签（ticklabel），而 x 的值很可能与经度不相等。这时就需要使用 Cartopy 提供的经纬度专用的 Formatter，将刻度标签表现为正确的地理坐标的形式。例如\nimport cartopy.feature as cfeature\rfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\r# 分别指定`GeoAxes`所处的投影和刻度所处的投影.\rmap_proj = ccrs.PlateCarree(central_longitude=180)\rtick_proj = ccrs.PlateCarree(central_longitude=0)\rfig, axes = plt.subplots(\rnrows=2, ncols=1, figsize=(6, 8),\rsubplot_kw={'projection': map_proj}\r)\r# 两个ax设置相同的刻度.\rfor ax in axes:\rax.set_global()\rax.add_feature(cfeature.LAND)\rax.add_feature(cfeature.OCEAN)\rax.set_xticks(np.linspace(-180, 180, 7), crs=tick_proj)\rax.set_yticks(np.linspace(-90, 90, 5), crs=tick_proj)\raxes[0].set_title('Ticks Added')\raxes[1].set_title('Formatter Added')\r# 为第二个ax使用Formatter.\raxes[1].xaxis.set_major_formatter(LongitudeFormatter())\raxes[1].yaxis.set_major_formatter(LatitudeFormatter())\rplt.show()\r 可以看到上图中的刻度标签显示的是 x 的值，下图中 Formatter 通过读取 GeoAxes 的投影信息，将刻度值换算为经纬度，并追加了度数和方向的符号。LongitudeFormatter 和 LatitudeFormatter 还提供丰富的参数来修改刻度的显示效果，不过一般来说默认设置就够用了。另外这两个 Formatter 还可以用于普通的 Axes，会将 Axes 的坐标视为地理坐标。\nset_xticks 和 gridlines 的 bug set_xticks 方法存在 bug：当省略 crs 参数，或提供的 CRS 对象与 GeoAxes 的投影等价（源码里通过 == 判断）时，会跳过坐标变换的环节，直接使用你提供的刻度。例如\nmap_proj = ccrs.PlateCarree()\rfig = plt.figure()\rax = fig.add_subplot(111, projection=map_proj)\rax.set_global()\rax.set_xticks(np.linspace(0, 360, 7), crs=map_proj)\rax.set_yticks(np.linspace(-90, 90, 5), crs=map_proj)\rax.xaxis.set_major_formatter(LongitudeFormatter())\rax.yaxis.set_major_formatter(LatitudeFormatter())\r 本来 set_xticks 里大于 180° 的刻度需要先换算到 [-180°, 180°] 范围内，现在这一环节被跳过了，大于 180° 的刻度直接标在了地图外面。弥补方法是，刻度改用 np.linspace(-180, 180, 7) 即可，或者当 crs 参数与 map_proj 不同时，错误也会自动消失。\n画网格的 gridlines 方法存在类似的问题：超出 [-180°, 180°] 范围的经度刻度直接画不出来，就算 crs 参数不同于 map_proj 也没用。例如\nmap_proj = ccrs.PlateCarree(central_longitude=180)\rtick_proj = ccrs.PlateCarree()\rfig = plt.figure()\rax = fig.add_subplot(111, projection=map_proj)\rax.set_global()\rax.gridlines(\rcrs=tick_proj, draw_labels=True,\rxlocs=np.linspace(0, 360, 7),\rylocs=np.linspace(-90, 90, 5),\rcolor='k', linestyle='--'\r)\r 可以看到西半球的经度网格线没画出来，并且调用 fig.savefig 保存图片时若 dpi 不为默认的 150，连纬度的标签也会莫名其妙消失（另见 issues 1794）。Bug 具体原因我也不清楚，感兴趣的读者可以自己探究一下。弥补方法是一样的，xlocs 改用 np.linspace(-180, 180, 7) 即可。\n跨越边界的 plot 本节探讨通过 plot 方法绘制两点间连线时，在什么情况下会跨越边界相连。测试程序如下\nmap_proj = ccrs.PlateCarree()\rtick_proj = ccrs.PlateCarree()\rfig, axes = plt.subplots(\rnrows=2, ncols=2, figsize=(10, 6),\rsubplot_kw={'projection': map_proj}\r)\rfig.subplots_adjust(wspace=0.3)\r# 填色和设置刻度.\rfor ax in axes.flat:\rax.set_global()\rax.add_feature(cfeature.LAND)\rax.add_feature(cfeature.OCEAN)\rax.set_xticks(np.linspace(-180, 180, 7), crs=tick_proj)\rax.set_yticks(np.linspace(-90, 90, 5), crs=tick_proj)\rax.xaxis.set_major_formatter(LongitudeFormatter())\rax.yaxis.set_major_formatter(LatitudeFormatter())\rdef draw_line(ax, p1, p2):\r'''画出点p1和p2之间的连线,并标注在标题上.'''\rx0, y0 = p1\rx1, y1 = p2\rline_proj = ccrs.PlateCarree()\rax.plot([x0, x1], [y0, y1], 'o-', c='C3', transform=line_proj)\rax.text(\rx0, y0 + 15, 'start', ha='center', va='center',\rtransform=line_proj\r)\rax.text(\rx1, y1 + 15, 'end', ha='center', va='center',\rtransform=line_proj\r)\rax.set_title(f'From {p1} to {p2}')\rdraw_line(axes[0, 0], (120, 60), (240, -60))\rdraw_line(axes[0, 1], (240, -60), (120, 60))\rdraw_line(axes[1, 0], (120, 60), (-120, -60))\rdraw_line(axes[1, 1], (-120, -60), (120, 60))\rplt.show()\r 从测试结果可以归纳出：设起点的坐标为 (x0, y0)，终点的坐标为 (x1, y1)，接着比较 x0 和 x1 的绝对大小，当 x0 \u0026lt; x1 时，会从起点出发自西向东绘制；当 x0 \u0026gt; x1 时，会从起点出发自东向西绘制。例如左上角的图中，起点的经度数值小于终点，所以向东绘制，且中途穿越了地图边界；右上角的图将起点和终点颠倒后，变为从起点出发向西绘制；左下角和右下角的图同理，但不穿越地图边界。借助这一特性，我们可以预测并控制两点间的连线是走“内圈”（不穿越边界），还是走“外圈”（穿越边界）。\n这点不仅限于 plot 方法，contourf、pcolormesh、imshow 等其它绘图方法，乃至截取区域用的 set_extent 方法均遵循这一特性。\n跨越边界的 set_extent 上一节提到 set_extent 方法会根据 x0 和 x1 的大小关系决定绕圈方向，但实际上想要成功截取还需要范围不能跨过边界。例如\nfrom matplotlib.patches import Rectangle\rclon1 = 0\rclon2 = 180\rmap_proj1 = ccrs.PlateCarree(central_longitude=clon1)\rmap_proj2 = ccrs.PlateCarree(central_longitude=clon2)\rdata_proj = ccrs.PlateCarree()\rextent = [120, 240, 20, 80]\rlonmin, lonmax, latmin, latmax = extent\r# 第一行和第二行子图的central_longitude不同.\rfig = plt.figure(figsize=(12, 8))\rax1 = fig.add_subplot(221, projection=map_proj1)\rax2 = fig.add_subplot(222, projection=map_proj1)\rax3 = fig.add_subplot(223, projection=map_proj2)\rax4 = fig.add_subplot(224, projection=map_proj2)\rfig.subplots_adjust(hspace=-0.1)\rfor ax in [ax1, ax3]:\rax.set_global()\rax.set_xticks(np.linspace(-180, 180, 7), crs=data_proj)\rax.set_yticks(np.linspace(-90, 90, 5), crs=data_proj)\r# 用patch标出extent范围.\rpatch = Rectangle(\r(lonmin, latmin), lonmax - lonmin, latmax - latmin,\rfc='C3', alpha=0.4, transform=data_proj\r)\rax.add_patch(patch)\rfor ax in [ax2, ax4]:\rax.set_xticks(np.linspace(lonmin, lonmax, 7), crs=data_proj)\rax.set_yticks(np.linspace(latmin, latmax, 4), crs=data_proj)\r# 截取区域\rax.set_extent(extent, crs=data_proj)\r# 填色和添加formatter.\rfor ax in [ax1, ax2, ax3, ax4]:\rax.add_feature(cfeature.LAND)\rax.add_feature(cfeature.OCEAN)\rax.xaxis.set_major_formatter(LongitudeFormatter())\rax.yaxis.set_major_formatter(LatitudeFormatter())\r# 设置标题.\rax1.set_title(f'central_longitude={clon1}°')\rax3.set_title(f'central_longitude={clon2}°')\rax2.set_title('set_extent failed')\rax4.set_title('set_extent success')\rplt.show()\r 截取范围为经度 [120°, 240°]，纬度 [20°, 80°]。第一排图片 central_longitude = 0，红色方块标识出了截取范围，可以看到这张图中截取范围跨越了地图边界（180°），然后右边对纬度的截取成功了，但对经度的截取失败了——经度范围仍然是 [-180°, 180°]，所以地图变成了长条状。第二排图片 central_longitude = 180，此时地图边界变为 0°，截取范围因此没有跨越边界，然后右边得到了正确的截取结果。\n由此引出了 central_longitude 的又一作用：控制地图边界，以保证 set_extent 生效。额外再提一点，使用 set_extent 截取完后，若再调用 set_xticks 和 set_yticks 画超出截取范围的刻度时，会强制拓宽当前地图的范围。所以建议先设置刻度，再进行截取（这点对 set_global 也是一样的）。\nGeoAxes 的大小 Matplotlib 中 Axes 横纵坐标单位长度的比例称作 aspect_ratio，通常会自动根据 figsize、rect、xlim、ylim 等参数动态变化。也可以利用 set_aspect 方法设定固定的值，例如 ax.set_aspect(1) 会使图片上一个单位的 x 和一个单位的 y 代表的物理长度（英寸或像素）相等。\n之所以要提这一点，是因为所有投影的 GeoAxes 的 aspect_ratio 都固定为 1。试想一下，如果地图的 aspect_ratio 会随其它参数发生变化，或者可以任意赋值，那么就相当于地图的投影被改变了。例如等经纬度投影的地图单位经度和单位纬度必须等长，否则就会名不副实。\n不过固定的 aspect_ratio 也会带来一个问题：使用 fig.add_axes 创建 GeoAxes 时，虽然 rect 参数已经指定了 GeoAxes 的边界形状，但 GeoAxes 为了满足 aspect_ratio = 1 的条件，其形状很可能会发生变动，导致其大小不合我们的预期。下面用代码进行演示\nfrom matplotlib.transforms import Bbox\rproj = ccrs.PlateCarree()\rfig = plt.figure()\rrect = [0.2, 0.2, 0.6, 0.6]\raxpos1 = Bbox.from_bounds(*rect)\rax = fig.add_axes(rect, projection=proj)\rax.set_global()\rax.add_feature(cfeature.LAND)\rax.add_feature(cfeature.OCEAN)\raxpos2 = ax.get_position()\r# 画出rect的方框.\rpatch = mpatch.Rectangle(\r(axpos1.x0, axpos1.y0), axpos1.width, axpos1.height,\rec='C3', fc='none', transform=fig.transFigure\r)\rfig.patches.append(patch)\rfig.text(\raxpos1.x0, axpos1.y0 + axpos1.height, 'Expected Box',\rc='C3', va='bottom'\r)\r# 画出地图的方框.\rpatch = mpatch.Rectangle(\r(axpos2.x0, axpos2.y0), axpos2.width, axpos2.height,\rec='C0', fc='none', transform=fig.transFigure\r)\rfig.patches.append(patch)\rfig.text(\raxpos2.x0 + axpos2.width, axpos2.y0 + axpos2.height,\r'Actual Box', c='C0', ha='right', va='bottom'\r)\rprint('Expected Box:', axpos1)\rprint('Actual Box:', axpos2)\rplt.show()\r 打印结果为\nExpected Box: Bbox(x0=0.2, y0=0.2, x1=0.8, y1=0.8)\rActual Box: Bbox(x0=0.2, y0=0.30000000000000004, x1=0.8, y1=0.7000000000000002)\r 可以看到地图的实际方框维持中心位置和宽度不变，但对恒定比例的要求使其高度缩短了。实际上，若通过 set_extent 方法截取区域，还可能出现实际方框高度不变、宽度缩短的情况，这里就不放图片了。总之是想说明，PlateCarree 投影的 GeoAxes 常常出现会出现高度或宽度短于预期的情况。其实际大小位置可以通过 get_position 方法获取，之后可以用于绘制等高或等宽的 colorbar 等（例子可见 Python 绘制 CALIPSO L2 VFM 产品）。\n强行把地图填到 rect 指示的空间里也不是不行，只需要设置\nax.set_aspect('auto')\r 地图就会自动填满预期方框，不过这样一来投影便称不上等经纬度了。\n结语 文中很多经验都是笔者试出来的，Cartopy 的官方文档并没有详细解说，所以这些经验可能存在不严谨或错误的地方，还请读者在评论区指出。\n参考链接 Cartopy API reference\nLongitude conversion 0~360 to -180~180\npreventing spurious horizontal lines for ungridded pcolor(mesh) data\nForce aspect ratio for a map\n","date":"2021-11-06","permalink":"https://zhajiman.github.io/post/cartopy_appendix/","tags":["cartopy","matplotlib"],"title":"Cartopy 系列：对入门教程的补充"},{"content":"定义 命名空间 命名空间（namespace）：官方说法是从名称到对象的映射，实际上就是保存变量名与变量值绑定关系的一个空间。赋值语句会将绑定关系写入命名空间，而引用变量时则会根据变量名在命名空间中查询出对应的值。并且大部分的命名空间都是利用 Python 的字典来实现的（例外如类的 __slots__ 属性）。程序中出现在全局的变量构成一个命名空间，Python 内置的函数和异常类也有它们自己的命名空间，每次定义函数或类时也会创建专属于它们的命名空间。命名空间之间相互独立，同名的变量可以存在于不同的命名空间中，例如两个函数内部可以使用同名的局部变量，这有助于我们在不引发冲突的同时合理复用变量名。\n作用域 作用域（scope）：官方说法是 Python 程序中能直接访问一个命名空间的文本区域。听起来有点抽象，实际上就是指出程序中哪些区域的文本归哪个命名空间管理，例如函数的作用域显然就是函数体（定义函数的所有语句），全局作用域就是从头到尾整个程序。但并不是说出现在一个作用域中的变量就一定属于该作用域（的命名空间）：若在该区域内通过赋值语句等操作创建（或修改）了该变量的绑定关系后，那它就属于该作用域；否则它就属于其它作用域，在当前区域引用它需要根据特定的规则向其它作用域进行查询。例如常见的在函数中引用全局变量。本文的一个重点就是要来仔细说说这一规则。\nLEGB 规则 引用变量时，按 L -\u0026gt; E -\u0026gt; G -\u0026gt; B 的顺序在不同作用域中查询：\n L（Local）：局部作用域，比如函数或方法内部。 E（Enclosing）：外层作用域，比如一个闭包函数的外层函数部分。 G（Global）：全局作用域，比如当前运行的文件或导入的模块的内部。 B（Built-in）：Python 的内置函数等存在的作用域。  举个例子，若在函数中引用某变量，首先会在函数的局部作用域中查询该变量是否存在，查不到就到外层函数（如果存在的话）的作用域里去查，再查不到就接着去全局和内置作用域，如果都查不到就会抛出 NameError 异常了。下面再以一张图为例一步步进行解说。\n这段程序的运行结果是\nfunc_arg in global: 1\rfunc_arg in func: 2\rinner_var in inner_func: 2\router_var in inner_func: 1\r 首先，程序在启动时就已经全部处于内置作用域中（图中肉色部分）。然后程序的每一句被解释器执行：函数名 func 和 outer_func 通过 def 语句分别绑定给了两个函数对象，其绑定关系写入了全局作用域的命名空间中（图中绿色部分）。__main__ 是全局作用域中预定义的变量，在本例中值为 'main'，变量名 func_arg 通过赋值语句绑定给了整数 1。因为全局作用域中并不存在名为 print 的函数，所以会到内置作用域中查询，因为 print 正好是内置函数所以顺利地找到了——即 G -\u0026gt; B 的查询顺序。print 函数的参数中出现了 func_arg，全局作用域中就有，所以打印出了整数 1。\n接着到了调用函数的部分。我们都知道，函数被调用时会把形式参数（func_arg）绑定给传入的实际参数（即整数 2），所以 func 的命名空间中出现了 func_arg（图中第一个蓝色部分），并且这个 func_arg 与全局作用域中的 func_arg 毫无干系。然后又是按 L -\u0026gt; G -\u0026gt; B 的顺序在内置作用域中找到 print 函数，打印出整数 2。\n主程序的最后一句是调用存在嵌套的函数 outer_func。outer_func 的函数体被执行，其中变量名 outer_var 被绑定给整数 1，函数名 inner_func 被绑定给嵌套定义的函数对象，之后它们出现在 outer_func 的命名空间中（图中第二个蓝色部分）。outer_func 函数体的最后一句是调用刚刚定义好的 inner_func 函数，inner_func 的函数体同样也是一个局部作用域（图中黄色部分），但因为被定义在 outer_func 内，所以 outer_func 的局部作用域同时也是 inner_func 的外部作用域。因此 inner_func 中调用 print 时发生了 L -\u0026gt; E -\u0026gt; G -\u0026gt; B 的搜索过程。在 inner_func 中调用 outer_var 也发生了 L -\u0026gt; E 的查询过程。\n简单总结一下：作用域就好比花花绿绿的便利贴，最底下两张大的便利贴分别是内置作用域和全局作用域。定义新函数时会在这两张纸的基础上一层一层往上盖小便利贴，因而不同函数栈会摞成一个个纸堆。引用变量时则会从当前便利贴出发，一层一层往下查询，最远查到底层的内置作用域；不过往上查询是不允许的，所以外层函数无法引用内层函数的变量。根据这一规则，不同函数栈之间也是互不相通的。下图是对这一比喻的立体化展示\nnonlocal 和 global 语句 考虑下面这个函数\ndef outer_func():\router_var = 1\rdef inner_func():\router_var = 2\rprint('outer_var before inner_func:', outer_var)\rinner_func()\rprint('outer_var after inner_func:', outer_var)\r 运行结果为\nouter_var before inner_func: 1\router_var after inner_func: 1\r 明明函数 inner_func 对变量 outer_var 进行了修改，但修改效果似乎没有体现在外层。这是因为 outer_var = 2 这个赋值语句只是在 inner_func 的作用域中新定义了一个绑定关系，这里的 outer_var 和外层的 outer_var 实际上分别属于不同的两个命名空间，除了变量名恰好相同以外并没有任何联系。这一行为还可以解读成，作用域外层的变量总是“只读”的——你可以根据 LEGB 规则引用外层变量的值，但若想通过赋值语句等操作改变其绑定关系，则只会在当前作用域里创建同名变量而已。\n若把 inner_func 中的赋值语句改为自增\ndef inner_func():\router_var += 1\r 运行却发现会抛出 UnboundLocalError 异常。这里自增语句 outer_var += 1 等价于赋值语句 outer_var = outer_var + 1，我们可能会认为，等号右边会通过引用外层 outer_var 的值计算出整数 2，然后再在当前作用域中创建同名的绑定关系，程序应该能正常运行才对。但实际情况是，函数在被定义时，若函数体内存在关于某变量的绑定语句，那么这个变量就一定会被解析到函数自己的作用域中，不会再向外查询——哪怕函数还没被调用、该语句还没被执行。所以当 inner_func 看到自己的语句块中出现了自增语句时，就认定 outer_var 肯定是自己的局部变量（local），但真当运行到 outer_var + 1 的表达式时，却发现局部作用域中查不到它，所以自然产生了 UnboundLocalError 异常：该局部变量还没有绑定关系就被引用了，命名空间里查不到它啊。\n如果真想修改外部作用域里的绑定关系，就需要用 nonlocal 和 global 语句显式声明某变量所处的作用域，同时获得修改其绑定关系的权限。nonlocal 会把变量名解析到离当前局部作用域最近的非全局的外层作用域中，例如上面的 inner_func 可以修改为\ndef inner_func():\rnonlocal outer_var\router_var = 'abc'\r 运行结果为\nouter_var before inner_func: 1\router_var after inner_func: abc\r 可以看到通过 nonlocal 声明 inner_func 里的 outer_var 就是外层那个 outer_var，便可以在 inner_var 里修改 outer_var 的绑定关系。global 同理，不过顾名思义会把变量名解析到全局作用域，例如\nN = 10\rdef func():\rglobal N\rN += 10\rif __name__ == '__main__':\rprint('N before func:', N)\rfunc()\rprint('N after func:', N)\r 运行结果为\nN before func: 10\rN after func: 20\r 如果去掉 global 的语句的话，同样会抛出 UnboundLocalError 异常。\n需要注意，这一节针对的都是不可变（immutable）对象，若外层作用域的变量是可变（mutable）对象，例如列表、字典等，那么即便不用 nonlocal 和 global 语句，我们也能用赋值语句直接修改其元素，利用自增语句进行原地的连接操作。\n模块的作用域 每个模块都有其专属的命名空间和全局作用域，模块内变量的引用同样服从 LEGB 规则。事实上，主程序也不过是特殊的 __main__ 模块的一部分而已。通过 import 语句可以把主程序里的变量名绑定给其它模块里的对象，以实现跨模块的引用。例如\nimport math\rfrom math import sqrt\r 第一句会将 math 模块作为一个对象绑定到主程序里的 math 变量名上，接着以 math.func 的形式调用模块里的函数即可。而第二句等价于\nimport math as _\rsqrt = _.sqrt\rdel(_)\r 相当于把 math.sqrt 函数直接绑定到主程序里的 sqrt 变量名上。因此可以想到，直接修改 sqrt 的绑定关系并不会影响到 math.sqrt。下面还是再以图片为例\n内置作用域上有两个全局作用域（图中绿色部分），左边是主程序的，而右边是自定义的 mod 模块的。本来这两个作用域互相独立，但通过 from mod import exp 语句将右边的 exp 函数导入到了左边，所以现在左边也能调用 exp。注意，虽然现在 exp 属于主程序的全局作用域，但 exp 指向的函数对象直接定义在 mod.py 文件中，其内部的变量依然工作在 mod 模块的全局作用域里（例如函数中用到了定义在 mod 里的全局变量 e，不会说导入到主程序中就找不到 e 了）。\n类的作用域 类的说明要稍微麻烦些，所以这里直接通过例子来展示\n运行结果为\nKate : meow\r 首先，类只有当其定义里的语句被全部执行后才能生效（显然函数不是这样）。当程序刚进入类定义时会创建类专属的命名空间，之后定义里的绑定关系将会被记录到这个命名空间中。如图中蓝色部分所示，绑定了一个类变量 sound 和两个类函数 __init__ 和 call，同时这两个函数因为第一个参数是 self，所以之后还能作为实例的方法被调用。定义执行完毕后会创建一个类对象，并将其绑定到与类名同名的名称上去（此处是 Cat）。\n直接调用类对象可以创建一个空的实例对象 c，它也有自己独立的命名空间。我们可以通过 c.attr 的形式引用类相关的变量。若引用的是实例变量，那么会直接查询实例自己的命名空间；若引用的是类变量，那么会跳到实例所属的类的命名空间中去查找；若引用的是方法，则会跳到实例所属的类的命名空间中查找同名的函数，并将实例对象自身作为 self 参数传入。\n再回过头来看具体的程序，Cat 类在被直接调用时会自动调用 __init__ 方法（如果存在的话），同时将 c 和接收的其它参数一并传给 __init__。__init__ 的作用是给实例一个初始状态，可以看到函数定义里以 self.name = name 等赋值语句向 c 的命名空间中写入了实例变量的绑定关系。之后主程序中调用 c.call()，等价于 Cat.call(c)，call 的函数定义中 self.sound 又等价于 Cat.sound。\n类与函数的一个重要差别是，函数里嵌套定义的函数可以按 L -\u0026gt; E 的顺序引用外层函数的变量，但类里定义的函数并不能引用类变量，例如本例中 call 函数里直接引用 sound 会抛出 NameError 异常。虽然类也有命名空间和作用域，但内层函数在向外层查询时会跳过类的作用域，用图上的内容来说，就是蓝色层对于黄色层是“透明”的。不过，因为绿色层里有类对象的绑定关系，所以可以用 Cat.attr 的形式迂回引用类属性。\n总结一下：类的作用域不同于一般函数的作用域，类里的函数不能直接访问类属性，但可以委托实例对象（self）去访问类变量和类方法，或直接用类名访问所有类属性。如果存在继承，那么上面提到的委托操作会递归地向父类进行查询，这里篇幅有限就不再详谈了。\nPS：如果你尝试以下代码\nN = 10\rprint('N before class:', N)\rclass A:\rN += 10\rprint('N in class:', N)\rprint('N after class:', N)\r 运行结果为\nN before class: 10\rN in class: 20\rN after class: 10\r Emmm……对于函数会报 UnboundLocalError 错误，但对类就成功运行了。所以也有人说其实类只有命名空间而没有作用域，感兴趣的读者可以参考最后一个参考链接。\n参考链接 The Python Tutorial: 9. Classes\nPython3 命名空间和作用域\nPython是一种纯粹的语言\nPython的类定义有没有建立新的作用域？\n","date":"2021-10-23","permalink":"https://zhajiman.github.io/post/python_namespace_scope/","tags":["python"],"title":"Python 系列：命名空间和作用域"},{"content":"在 Python 3 中关于除法的运算符有三种：\n /：表示精确的真除法。魔法方法为 __truediv__。 //：表示地板除。魔法方法为 __floordiv__。 %：表示求模。魔法方法为 __mod__。  / 无需介绍。其中 // 被称为地板除是因为其结果等价于对 / 的结果向下取整。设操作数 m 和 n 是整数，于是有关系\nm // n = floor(m / n)\r 即便 m 或 n 是负数时，这一关系依然成立。例如\nIn : 5 // 2\rOut: 2\rIn : -5 // 2\rOut: -3\rIn : 5 // -2\rOut: -3\rIn: -5 // -2\rOut: 2\r % 的结果与 // 的结果密切相关，它们一定满足\nq = m // n\rr = m % n\rq * n + r = m\r 所以 % 的结果可以通过 r = m - q * n 计算得到。例如 -5 % 2 就等于 1。Python 中的 divmod 函数能够同时返回 // 和 % 的结果，方便我们观察结果。例如\nIn : divmod(-5, 2)\rOUt: (-3, 1)\r 再扩展一下，即便 m 或 n 是浮点数，结果依然遵循上面的计算流程，不过此时 // 和 % 的结果都会变成浮点型。例如\nIn : divmod(5.5, 2)\rOut: (2.0, 1.5)\rIn : divmod(-5.5, 2)\rOut: (-3.0, 0.5)\r 此外可以观察到，在地板除的定义下，除数 n 和模 r 总是同号的。\n其它语言中 /、//（如果有的话）和 % 行为可能跟 Python 不同，使用时需要多加小心。\n","date":"2021-10-19","permalink":"https://zhajiman.github.io/post/python_divide/","tags":["python"],"title":"Python 系列：除法运算符"},{"content":"Vim 是一个拥有魔力的文本编辑器——这并不是比喻，而是说你在 Vim 中真的可以念咒语来操纵文本。看看我们的键盘，在 normal 模式下几乎每个键都有特定的功能，尤其是其中的数字和 26 个字母。如果把每个键都看作 Vim 这门语言中的单词，那么只要依据特定的语法，通过连续击键来遣词造句，就能施展操作文本的魔法。并且 Vim 语言的语法简单到用一句话就能描述：\nverb + noun\r 下面就来简单讲讲魔法的基本法。\n语法规则 Vim 的语法翻译过来就是，对什么文本（名词 noun）做什么操作（动词 verb）。其中动词指的是 Vim 中被称为 operator 的命令，例如小写的 d 键就是表示删除的 operator。但是单独按下 d 并不会起效，我们还需要指定动词的作用对象。Vim 中关于光标移动的命令被称为 motion，例如我们熟知的 hjkl 就是表示上下左右的 motion，w 是表示跳到下一个单词开头的 motion。Motion 作为名词使用时指代光标移动范围内的文本，所以句子\noperator + motion\r 就表示对 motion 移动范围内的文本执行 operator 的操作。例如组合 dw 就表示删除当前光标到下一个单词开头前的文本。不同于英语，Vim 语法中动词和名词前都可以加上数字，以表示重复动词或名词。例如 2w 表示跳跃到下下个单词开头，那么 d2w 就表示一次性删除两个接下来的单词；同时 2dw 表示删除下一个单词的操作执行两次；同理，2d2w 就表示删除 2 * 2 = 4 个单词。于是句子可以补充成\n[count] operator + [count] motion\r 其中 count 是大于 0 的整数，方括号表示可有可无。\n除了 motion，还有一类被称作 text object 的命令能作为名词。顾名思义，text object 表示具有某种结构的一段文本对象，具体形式为\ntext-object = modifier + object\r 其中 object 是具体的文本对象，modifier 是对其范围的一点补充修饰。例如 ap 就是一个 text object，其中对象 p 表示段落，修饰词 a 表示在整个段落范围的基础上，再包含段落前或段落后的空行。不同于 motion，text object 并不能单独使用，而是必须放在 operator 之后才能发挥作用。于是组合 dap 就表示删除一整个段落及与之相邻的空行。同样可以总结为句子\n[count] operator + [count] text-object\r 相比于 motion 的句子，这个句子不用关心光标的具体位置，只要我们的光标落入了文本对象的范围内，Vim 会自动找出文本对象的起始范围进行操作。\n至此 Vim 的语法基本上就讲完了，没错就这么点内容，但其中蕴含的思想是很值得玩味的。一般的文本编辑器只能提供非常原子化的操作：光标只能上下左右移动，字符只能单个单个增删。但 Vim 将具体的操作、光标的移动模式和结构化的文本分别抽象为 operator、motion 和 text object，再将它们映射到单个按键上，并按语法赋予其相互组合的能力，使编辑文本的逻辑能用简单的命令序列具象化地表达出来。这种操作哲学是一般的文本编辑器所欠缺的。\n正如学英语不能只学语法不背单词，Vim 里我们也需要掌握动词和名词才能正常造句，更别说实践过程中的许多迷惑点都是源于对词汇性质的不了解。所以下面继续来介绍常用的词汇。\n常用的 operator Vim 共有 16 个 operator，但最常用的无非以下几个：\n d：取自 delete，表示删除。例如 dw 表示删除当前光标到下一个词之前的内容。 c：取自 change，表示替换，相当于 d 之后自动进入 insert 模式。例如 cw 效果同 dw，但删除完毕后会进入 insert 模式以便马上输入新的替换文本。 y：取自 yank，表示复制到寄存器中。例如 yw 表示复制当前光标到下一个词之前的内容。因为词与词之前可能有空格或标点，所以 yw 会把这些多余的间隔也复制进去。 gu：把文本变成小写（lowercase）。这是一个两个键组成的 operator，例如 guiw 能把一个词变成全小写（其中文本对象 iw 会在后面讲解）。 gU：把文本变成大写（uppercase）。例如 gUiw 能把一个词变成全大写。 \u0026gt;：向右缩进一个 tab 的距离。默认作用于行，所以即便是 \u0026gt;w 也会使整行向右缩进。一个比较有用的例子是 \u0026gt;ip 或 \u0026gt;ap，表示使整个段落向右缩进。 \u0026lt;：向左缩进一个 tab 的距离。用法同 \u0026gt;。  单个 operator 后面必须接一个名词才能起作用。但当 operator 的按键被重复两次时，就可以省去名词，此时表示作用于光标所处的这一行。例如 dd 表示删除当前行，yy 表示复制当前行，\u0026gt;\u0026gt; 表示当前行向右缩进。此外也可以加上重复次数，例如 3dd 表示删除从当前行开始往下共 3 行。\n作为对 y 的补充，提一下并非 operator 的粘贴命令 p：小写的 p 表示在当前光标左边（当前行上面）粘贴字符（行），而大写的 P 表示在当前光标右边（当前行下面）粘贴内容（行）。\n常用的 motion Motion 有两个非常重要的属性需要预先说明一下。\n首先，若 motion 的移动发生在行与行之间，就称其是 linewise 的；若移动发生在字符间，就称其是 characterwise 的。例如 j 和 k 就是 linewise 的，而 w 显然是 characterwise 的。\n其次，motion 还拥有一个能影响到其作用范围的开闭性。以一个 characterwise motion 为例，若 operator + motion 组合的作用范围不包含 motion 移动范围的右边界，则称这个 motion 是 exclusive 的，反之则称为 inclusive 的。对 linewise motion 同理，根据句子的作用范围是否包含 motion 移动范围的下边界（即最后一行）来决定开闭性，不过一般 linewise motion 都是 inclusive 的。例如常用的 w 就是一个 exclusive motion，单独使用它会将光标跳到下个词的第一个字符处，但 dw 却会点到为止，刚刚好删除到那个字符之前。再比如 j 和 k 都是 linewise motion，dj 会删除当前行和下一行，dk 会删除当前行和上一行。\n这里恐怕有点绕，所以用图展示一下\n其中绿色方块是 block 形式的光标，单向箭头是 motion 的起止点，花括号指示句子的作用范围。可见对于 exclusive 的 motion 来说，移动的起止点围成的范围和句子的作用范围总是相差一个右边界字符；而对 inclusive 的 motion 来说，两种范围是相同的。\nVim 中 motion 相当多，不信可以看看本文头图中的绿色按键有多少。这里仅介绍常用的几个：\n hjkl：上下左右移动，其中 j 和 k 是 linewise 和 inclusive 的，而 h 和 l 是 characterwise 和 exclusive 的。所以 dl 只会删除当前光标处的字符，等价于 x；而 dh 会删除当前光标左边的一个字符。 w 和 W：跳到下一个词的第一个字符处，是 exclusive 的。大小写的区别在于，小写形式作用于 word，大写形式作用于 WORD（其中文本对象 word 和 WORD 会在后面讲解）。 b 和 B：跳到上一个词的第一个字符处，是 exclusive 的。 e 和 E：跳到下一个词的最后一个字符处，是 inclusive 的。 ge 和 gE：跳到上一个词的最后一个字符处，是 inclusive 的。 0、^ 和 $：0 表示移动到本行的第一列，^ 表示移动到本行第一个非空白字符处，而 $ 表示移动到本行的最后一列。其中 0 和 ^ 是 exclusive 的，而 $ 是 inclusive 的。且只有 $ 前可以加数字，表示移动到从当前行开始下面第 n 行的末尾。 f 和 F：取自 find，在本行搜索指定的字符并将光标移动过去。以当前光标为起点，小写的 f 表示向后搜索，大写的 F 表示向前搜索，前者是 inclusive 的，但后者却是 exclusive 的。f 后必须接目标字符，例如 fa 会跳到当前光标后第一次出现字符 a 的位置，而 2fa 则会跳到第二次出现的位置。若没有找到，则光标不会发生移动。 t 和 T：取自 till，基本同 f 和 F，但会恰好停在搜索结果前。例如 ta 会跳到 fa 终点的前面一格，所以何时使用 f 或 t 取决于我们对边界的处理。 ;：重复上一个 f、F、t 或 T 的移动。例如当本行有三个 a 字母时，fa 会使光标跳到第一个 a 上，此时按下 ; 便相当于重复了 fa 的操作，跳到第二个 a 上，再按又会跳到最后一个 a 上。 ,：类似于 ;，不过是按反方向移动。还是三个 a 的例子，按 , 会跳回上一个 a 的位置。 { 和 }：跳到上一个/下一个段落边界（即空行），是 exclusive 的。 G：若前面加数字，表示跳到指定行；若不加数字则表示跳到最后一行，且是 linewise 和 inclusive 的。例如 dG 表示删除当前行到最后一行的全部内容，d2G 表示删除当前行到第二行的全部内容。 gg：加数字时的行为同 G，但不加数字时则表示跳到第一行。例如 dgg 表示删除当前行到第一行的全部内容，等价于 d1G 和 d1gg。  常用的 text object 第一节提过\ntext-object = modifier + object\r 其中修饰词实际上只有两个：i 和 a，字面义分别是单词 inner 和冠词 a，但具体效果需要结合 object 来看。所以现在来介绍常用的 object：\n word：Vim 中把由字母、数字或下划线等非空白字符构成的字符序列称为 word，word 之间由空白字符（空格、制表和换行）或标点符号分隔。在命令中用 w 表示。iw 仅表示一个 word 含有的所有字符，而 aw 还会额外包含前后的空白字符，并且当前后都有空白时则只包含后面的空白。若光标的起始位置就是在 word 前后的空白上，aw 的范围又会发生变化——这里就不细讲了，烦请读者自己尝试一下。 WORD：条件更宽松的 word，只要是非空白字符的序列都能算是一个词。例如 apple,banana 算是两个 word，但只能算一个 WORD。在命令中用大写的 W 表示。 paragraph：即视觉上行与行相连的整段文本，段落之间一般通过空行（可含空白字符）分隔。在命令中用 p 表示。ip 表示仅作用于段落的所有行，而 ap 类似于 aw，会额外包含前后的空行。 括号：表示括号圈起来的文本块（可以分行），圆括号、方括号和花括号等皆可。这里以圆括号为例，在命令中用 ( 或 ) 表示。i( 仅表示括号内的文本，而 a( 则会包含括号本身。例如 di( 和 ci( 就是非常实用的两个组合命令。 引号：表示引号圈起来的文本，单引号和双引号皆可，可惜只限于本行。以双引号为例，i\u0026quot; 仅表示引号内的文本，而 a\u0026quot; 则会包含引号本身以及引号前后的空白。同样 di\u0026quot; 和 ci\u0026quot; 非常便于修改程序中字符串的内容。  结语 看到这里，你应该能一窥 Vim 的魔力了吧——赋予模糊不清的操作以名字，再按韵律吟唱这些名字，魔法就会出现。如果再加上 . 命令和宏的配方，更是能让魔法自动生出更多魔法，可惜我也只是刚入门的学徒，以后有机会再来介绍更多。文中存在的不妥之处还请读者多多指出。\n参考链接 VIM 中文帮助：有关移动的命令\nVim Grammar\nLearn-Vim Ch04. Vim Grammar\nVim终极指南：所思即所得\n","date":"2021-10-17","permalink":"https://zhajiman.github.io/post/vim_grammar/","tags":["vim"],"title":"Vim 的语法"},{"content":"前言 这几天要用 NumPy 生成随机数，所以去查了一下 np.random 模块的官方文档，却惊讶地发现里面介绍的用法跟我的记忆有很大出入：例如以前用 np.random.rand 便能生成 [0, 1) 之间均匀分布的随机数，现在文档里记载的却是面向对象风格的写法（创建随机数生成器再调用方法……）。调查一番后发现原来这一改动发生于 NumPy 1.17 版本（2020 年 1 月），并且网上对此的中文介绍也比较少，所以现撰文简单介绍一下该模块在改动前后的两套用法。\n原理 先概括一下计算机生成随机数的原理，方便后面理解程序的行为。我们先给定一个用整数表示的随机种子（seed），然后计算机会根据特定的算法（平方取中、线性同余等……）对这个种子不断进行计算，得到一串数字序列。由于输入是确定的，算法的步骤也是完全固定的，所以结果也是唯一确定的——即一个种子对应一个序列。这个序列虽然是完全确定的，但它本身与真实世界中随机过程产生的序列很相似，序列中的每个数字像是随机出现的，且分布接近于均匀分布。于是我们便把这个算法生成的“伪随机序列”当作随机序列来用，再根据需求通过数学变换把均匀分布的随机序列变换为其它概率分布的随机序列。\n不过这一做法的缺陷是，若种子不变，那么每次生成的随机序列总是一模一样的，甚至还可以从序列的排列规律中反推出种子的值。为了避免这种情况，可以用操作系统的时间戳或熵池（系统收集的各个设备的环境噪音）信息作为种子，以保证每次运行都产生不同的结果。\n更详细的解说请参考 混乱中的秩序——计算机中的伪随机数序列 这篇知乎专栏。我们将会看到，无论是旧版还是新版，numpy.random 模块都是按照这一节的流程来生成随机数的。\n旧版本 RandomState 虽然我们常用的是 np.random.rand 这样的函数命令，但要把用法讲清楚，还是需要从 RandomState 类开始。RandomState 是 np.random 模块中表示随机数生成器的类，内部采用 Mersenne Twister 算法的 MT19937 实现来生成伪随机序列（算法原理在前面提到的专栏中有介绍）。在创建对象时需要指定随机种子，然后通过调用方法来生成其它概率分布的随机数，例如\nimport numpy as np\rfrom numpy.random import RandomState\rseed = 0\rrs = RandomState(seed)\r# 生成3个[0,1)范围内均匀分布的随机数\rprint(rs.rand(3))\r# 生成3个服从标准正态分布的随机数\rprint(rs.randn(3))\r 种子可以是一个大于等于 0 的整数，也可以是这样的整数构成的一维序列。无论种子是哪种形式，只要每次给定相同的种子，那么随机数生成器都会生成相同的随机序列，调用方法时会不断从这个序列中抽取数字来进行变换，进而生成相同的随机数。例如\n# 生成三个[0,10]范围内的随机整数\rrs1 = RandomState(1)\rprint('seed=1:', rs1.randint(0, 11, 6))\rrs2 = RandomState(1)\rprint('seed=1:', rs2.randint(0, 11, 3), rs2.randint(0, 11, 3))\rrs3 = RandomState(2)\rprint('seed=2:', rs3.randint(0, 11, 6))\r 结果为\nseed=1: [5 8 9 5 0 0]\rseed=1: [5 8 9] [5 0 0]\rseed=2: [8 8 6 2 8 7]\r 可以看到当种子都为 1 时，两个不同的 RandomState 对象生成的随机数相同（尽管 rs2 调用了两次方法）；但当种子为 2 时，结果便发生了变化。下面再举一个用时间戳作为种子的例子\nimport time\rseed = int(time.time())\rrs = RandomState(seed)\rfor _ in range(3):\rprint(rs.randint(0, 11, 3))\r 注意不要把设置种子的语句写在循环里，因为取整后的时间戳的间隔只有 1 秒，而循环一次的速度一般远快于 1 秒，这就导致循环内一直使用同一个种子，最后产生三组一模一样的随机数。其实，在创建 RandomState 对象时如果不给出种子（即默认的 seed=None），那么程序会自动利用熵池和时间信息来确定种子的值。所以总结一下就是，如果你需要程序结果是可复现的（reproducible），那么使用固定种子即可；如果你需要每次都使用不同的随机数，那么大胆写上 rs = RandomState() 即可。\n下面用表格总结一下 RandomState 对象常用的方法\n   方法 效果     rand 生成 [0, 1) 范围内均匀分布的浮点随机数。本质是 random_sample 包装后的版本。   randint 生成 [low, high) 范围内离散均匀分布的整型随机数。   randn 生成服从标准正态分布的随机样本。对于更一般的正态分布可以使用 normal 方法。   choice 对给定的一维数组进行随机抽样。    调用函数 对我们更为熟悉的可能是直接调用函数的用法，例如\nnp.random.seed(1)\rprint(np.random.rand(3))\rprint(np.random.randint(0, 11, 3))\rprint(np.random.randn(3))\r 大家很容易看出其用法与上一节大差不差，所以就不详细解说了。联系在于，首次调用函数时，NumPy 会偷偷在全局创建一个 RandomState 对象，然后用这个对象来生成随机数，作为这些函数的返回值。所以调用函数只是一种偷懒（handy）的用法罢了。这种用法的缺点很明显，如果代码中有地方改动了种子，会影响全局的随机数结果，更别说在并行时还可能出现同时修改种子的情况。尽管有着明显的缺点，但在 np.random 模块大改之前，官方文档和各路教程都主推这一用法，我们在使用时需要多加小心。\n新版本 1.17 版本前 np.random 中存在面向对象和调用函数两种用法，而 1.17 版本后则统一使用新的面向对象式的用法，并在功能和性能方面作出了很多改进，下面便来一一解说。首先新版本为了能支持使用不同的随机数生成算法，将原先的 RandomState 细分为两个类：BitGenerator 和 Generator。前者通过随机数生成算法产生随机序列，后者则对随机序列进行变换。例如\n# MT19937和PCG64都是内置的BitGenerator\rfrom numpy.random import MT19937, PCG64, Generator\r# BitGenerator接收seed为参数\rseed = 1\rrng1 = Generator(MT19937(seed))\rrng2 = Generator(PCG64(seed))\r# 生成3个[0, 10]范围的整数\rprint(rng1.integers(0, 10, 3, endpoint=True))\rprint(rng2.integers(0, 10, 3, endpoint=True))\r 结果为\n[2 9 8]\r[5 5 8]\r 新用法的模式与 RandomState 非常类似，但 RandomState 只支持 Mersenne Twister 算法，而新用法通过更换 BitGenerator 对象可以换用不同的随机数生成算法。可以看到尽管种子相同，但不同算法的结果是不一样的。一般来说我们不需要自己选取算法，使用默认的随机数生成器即可。例如\nfrom numpy.random import default_rng\r# 等价于 rng = Generator(PCG64())\r# 不给定种子时,自动根据熵池或时间戳选取种子\rrng = default_rng()\rprint(rng.integers(0, 11, 3, endpoint=True))\r 默认生成器使用 2014 年提出的 PCG 算法，其性能与统计特性要比 1997 年提出的 Mersenne Twister 算法提高不少。下面用表格总结一下 Generator 对象常用的方法\n   方法 效果     random 生成 [0, 1) 范围内均匀分布的浮点随机数。类似于标准库的 random.random 。   integers 生成 [low, high) 范围内内离散均匀分布的整型随机数。相比 randint，增加了指定区间是否闭合的 endpoint 参数。   standard_normal 生成服从标准正态分布的随机样本。对于更一般的正态分布可以使用 normal 方法。   choice 对给定的多维数组进行随机抽样。    可以看到 Generator 的方法名相比 RandomState 更符合直觉，功能上也作出了改进。虽然现在官方推荐新版本的用法，但出于兼容性的考虑，旧版本的用法也依然可以使用。值得注意的是，即便使用相同的随机数生成算法和相同的种子，新版本与旧版本产生的随机数也不会相同，例如\nimport numpy as np\rfrom numpy.random import RandomState, MT19937, Generator\rseed = 1\rrs = RandomState(seed)\rrng = Generator(MT19937(seed))\rdecimals = 2\rprint('RandomState:', np.around(rs.rand(3), decimals))\rprint('Generator:', np.around(rng.random(3), decimals))\r 结果为\nRandomState: [0.42 0.72 0. ]\rGenerator: [0.24 0.73 0.56]\r 这是因为 Generator 在接受种子后还会在内部自动通过 SeedSequence 类对种子进行进一步的处理，利用新的散列算法将用户给出的低质量种子转化成高质量种子，以提高生成的随机数的质量。例如对于 Mersenne Twister 算法，如果给出相邻的两个整数种子，那么生成的两串随机序列将会有很大的相似性——即两串序列不够独立。而新引入的 SeedSequence 类就能让相邻的种子对应于迥然的两个生成器状态。同时 SeedSequence 类还有助于在并行生成随机数时为每个子进程设置相互独立的状态，有需求的读者请参考官方文档 Parallel Random Number Generation，这里就不多加介绍了。当然，即便种子经过了更复杂的处理，原理中提到的种子能决定随机数结果的规则依旧是不变的。\n基本用法的介绍就这些，新旧版本的其它差别在官网也有总结（What’s New or Different），希望本文能对读者有所帮助。\n参考链接 NumPy: Random sampling\nNumPy: Legacy Random Generation\nnumpy-random函数\nNumPy Random Seed, Explained\nnumpy.randomのGeneratorをためしてみる\nGood practices with numpy random number generators\n随机数大家都会用，但是你知道生成随机数的算法吗？\nWhat numbers that I can put in numpy.random.seed()?\n","date":"2021-09-21","permalink":"https://zhajiman.github.io/post/numpy_random/","tags":["numpy"],"title":"Numpy 系列：random 模块的变化"},{"content":"最近越发老年痴呆，连自己写的 Vim 配置的作用都忘光了，所以在本文记录并解说一下我常用的配置以便查阅。这里的配置非常简单，仅用以强化基本的使用体验。由于我同时工作在能联网的 PC 和内网的服务器上，所以也会分开介绍如何在这两种环境下安装插件。文中 Vim 版本分别是 8.1（PC）和 7.4（服务器）。\n基本配置 首先介绍 Vim 自带的基本配置，配置文件的路径是 ~/.vim/vimrc。关闭对 vi 的兼容，并保证退格键能正常使用\n\u0026quot; 关闭对vi的兼容\rset nocompatible\r\u0026quot; 设置backspace键功能\rset backspace=eol,start,indent\r 设置行的显示\n\u0026quot; 显示行号\rset number\r\u0026quot; 高亮显示当前行\rset cursorline\r\u0026quot; 让一行的内容不换行\rset nowrap\r\u0026quot; 距窗口边缘还有多少行时滚动窗口\rset scrolloff=8\r\u0026quot; 显示标尺,提示一行代码不要超过80个字符\rset ruler\rset colorcolumn=80\r 设置缩进。Vim 默认使用宽度为 8 的 tab，而我一般写 Python，需要用 4 个空格替代 tab。这里参考 Useful VIM Settings for working with Python 的设置。关于这些选项的意义可以参考 Vim 的帮助文档或 Secrets of tabs in vim\n\u0026quot; tab设为4个空格\rset tabstop=4\rset shiftwidth=4\rset softtabstop=4\rset expandtab\rset smarttab\r\u0026quot; 新一行与上一行的缩进一致\rset autoindent\r 显示相匹配的括号，并增强搜索功能。\n\u0026quot; 显示括号匹配\rset showmatch\r\u0026quot; 高亮查找匹配\rset hlsearch\r\u0026quot; 增量式搜索\rset incsearch\r\u0026quot; 不区分大小写,除非含有大写字母\rset ignorecase\rset smartcase\r 开启语法高亮并设置配色。这里使用的是 onedark.vim 配色方案（后面会介绍如何安装）\n\u0026quot; 开启语法高亮\rsyntax on\r\u0026quot; 代码颜色主题\rset t_Co=256\rcolorscheme onedark\r 增强命令部分的显示和补全\n\u0026quot; 在右下角显示部分命令\rset showcmd\r\u0026quot; 命令可以用tab补全,并设置匹配规则\rset wildmenu\rset wildmode=list:longest,full\r 显示 tab 和行尾多余的字符\n\u0026quot; 显示tab和行尾多余的空格\rset list\rset listchars=tab:\u0026gt;·,trail:·\r 切换 buffer 时 Vim 总会提醒你将当前 buffer 的改动写入文件。打开 hidden 能允许我们将未保存的 buffer 放到后台。水平分屏和垂直分屏操作分别默认在上边和左边打开一个新 window，这不太符合我的习惯，所以改为在下边和右边创建\n\u0026quot; 允许隐藏未保存的buffer\rset hidden\r\u0026quot; 设置分屏时的位置\rset splitright\rset splitbelow\r 检测文件类型、设置 Vim 内部的字符编码为 utf-8，对文件的解码参考 用vim打开后中文乱码怎么办？ 中马宏菩的回答，防止中文出现乱码\n\u0026quot; 检测文件类型\rfiletype on\r\u0026quot; 文件编码\rset encoding=utf-8\rset fileencodings=ucs-bom,utf-8,utf-16,gbk,big5,gb18030,latin1\r\u0026quot; 没有保存或文件只读时弹出确认\rset confirm\r 设置历史记录条数，并禁用自动备份（理由我忘了……）\n\u0026quot; 记录历史记录的条数\rset history=1000\rset undolevels=1000\r\u0026quot; 禁用自动备份\rset nobackup\rset nowritebackup\rset noswapfile\r 偶尔要用到 NCL 语言，需要相关的高亮提示，所以在 NCL: Editor enhancements for use with NCL scripts 网址下载 ncl3.vim 文件并重命名为 ncl.vim，把文件放入 ~/.vim/syntax 目录中，再修改 .vimrc 的配置。其中还为 commentary.vim 插件设置了 NCL 的注释\n\u0026quot; NCL高亮设置\rau BufRead,BufNewFile *.ncl set filetype=ncl\rau! Syntax newlang source $VIM/ncl.vim\r\u0026quot; NCL注释设置\rautocmd FileType ncl setlocal commentstring=;%s\r 插件配置 PC PC 上使用 vim-plug 插件来管理其它插件，利用它能非常简单地安装、更新和移除插件，关于它的安装和使用方法详见其 GitHub 页面。在 vimrc 文件的开头添加\ncall plug#begin('~/.vim/plugged')\rPlug 'joshdick/onedark.vim'\rPlug 'vim-airline/vim-airline'\rPlug 'tpope/vim-commentary'\rPlug 'kshenoy/vim-signature'\rPlug 'mhinz/vim-startify'\rPlug 'junegunn/fzf'\rPlug 'junegunn/fzf.vim'\rcall plug#end()\r 保存后再执行命令 PlugInstall 即可将 call 语句块中提到的插件下载并安装到 ~/.vim/plugged 目录下。这里用到的插件有：\n onedark.vim：一个暗配色方案。 vim-airline：更好看的状态栏。 commentary.vim：引入注释命令。 vim-signature：显示出 mark 标记。 vim-startify：给 vim 整个开屏页面。 fzf 和 fzf.vim：引入模糊搜索功能。  如果总是下载失败，可以考虑给 Git 设置代理。\n每个插件都可以再进行单独配置，这里我只改动了 vim-startify：在 Vim 的启动界面显示最近打开过的 15 个文件，并添加 ~/.bashrc 和 ~/.vim/vimrc 两个文件到收藏夹\n\u0026quot; vim-startify的设置\rlet g:startify_files_number = 15\rlet g:startify_lists = [\r\\ {'type': 'files', 'header': [' Recent Files']},\r\\ {'type': 'bookmarks', 'header': [' Bookmarks']}\r\\ ]\rlet g:startify_bookmarks = [\r\\ {'b': '~/.bashrc'},\r\\ {'v': '~/.vim/vimrc'}\r\\ ]\r 服务器 对于不能联网的服务器，依据 vim-plug 作者的建议（issue #808），用 pathogen.vim 插件代替 vim-plug。不同于 vim-plug，pathogen.vim 并不能帮你下载插件，它的功能只是将其它插件的路径添加到 Vim 的 runtimepath 中，使 Vim 能在工作时找到其它插件罢了。首先在 GitHub 上下载 pathogen.vim 文件并移动到服务器的 ~/.vim/autoload 目录下，再手动下载其它插件的仓库，解压并重命名，移动到服务器的 ~/.vim/bundle 目录下，最后在 vimrc 文件的开头添加\n\u0026quot; 把插件加入runtimepath\rexecute pathogen#infect()\r 我们所需的插件即可生效。如果你服务器上的 Vim 版本是 8，那么连 pathogen.vim 也不需要，直接使用原生的 pack 语句块即可，我没用过所以就不解说了。\n快捷键配置 为了不与 normal 模式下已有的大量快捷键发生冲突，所以这里用 \u0026lt;Leader\u0026gt; 键作为自定义快捷键的起手式。关于 \u0026lt;Leader\u0026gt; 键的解说可见 How to Use the Vim \u0026lt;leader\u0026gt; Key，这里使用趁手的空格键作为 \u0026lt;Leader\u0026gt; 键。自定义快捷键可以解决以下痛点\n 每次搜索产生的高亮需要通过 :nohlsearch（或简化的 :noh）命令取消，包括回车在内至少要按 5 个键。改成快捷键后就只需要按 2 下。 经常要用 fzf.vim 插件的 :Files 和 :Buffers 命令打开文件，设成快捷键更方便。 在分屏中移动光标的的默认快捷键是 \u0026lt;c-w\u0026gt; + hjkl，需要扭曲左手才能按到，非常费劲。用空格替代 \u0026lt;c-w\u0026gt; 后就舒服多了。 在一个 window 中来回切换 buffer 需要用到命令 :bn 和 :bp，这里简化到 \u0026lt; 和 \u0026gt; 所在的两个键上。  \u0026quot; leader键改为空格\rnnoremap \u0026lt;space\u0026gt; \u0026lt;nop\u0026gt;\rlet mapleader = \u0026quot; \u0026quot;\r\u0026quot; 关闭高亮\rnnoremap \u0026lt;leader\u0026gt;n :nohlsearch\u0026lt;cr\u0026gt;\r\u0026quot; 搜索文件\rnnoremap \u0026lt;leader\u0026gt;f :Files\u0026lt;cr\u0026gt;\rnnoremap \u0026lt;leader\u0026gt;b :Buffers\u0026lt;cr\u0026gt;\r\u0026quot; 设置在分屏间移动的快捷键\rnnoremap \u0026lt;leader\u0026gt;h \u0026lt;c-w\u0026gt;h\rnnoremap \u0026lt;leader\u0026gt;l \u0026lt;c-w\u0026gt;l\rnnoremap \u0026lt;leader\u0026gt;j \u0026lt;c-w\u0026gt;j\rnnoremap \u0026lt;leader\u0026gt;k \u0026lt;c-w\u0026gt;k\r\u0026quot; 设置移动分屏的快捷键\rnnoremap \u0026lt;leader\u0026gt;H \u0026lt;c-w\u0026gt;H\rnnoremap \u0026lt;leader\u0026gt;L \u0026lt;c-w\u0026gt;L\rnnoremap \u0026lt;leader\u0026gt;J \u0026lt;c-w\u0026gt;J\rnnoremap \u0026lt;leader\u0026gt;K \u0026lt;c-w\u0026gt;K\r\u0026quot; 设置移动buffer的快捷键\rnnoremap \u0026lt;leader\u0026gt;, :bprevious\u0026lt;cr\u0026gt;\rnnoremap \u0026lt;leader\u0026gt;. :bnext\u0026lt;cr\u0026gt;\r 其中映射新按键的语句 nnoremap 仅作用于 normal 模式，且不会发生递归映射。关于各种 map 的介绍请见 [Vim]vim的几种模式和按键映射。\n结语 我目前用到的配置就以上这些。可以说是很简陋了，自动补全、一键运行代码什么的统统没有。不过我觉得作为基本的文本编辑器已经够用了，如果读者有心得也可以传授我一下。\n参考链接 Vim 配置入门\n有哪些编程必备的 Vim 配置？\n上古神器Vim：从恶言相向到爱不释手 - 终极Vim教程01\niggredible/Learn-Vim\n","date":"2021-09-20","permalink":"https://zhajiman.github.io/post/vim_config/","tags":["vim"],"title":"简单的 Vim 配置"},{"content":"简介 连通域标记（connected component labelling）即找出二值图像中互相独立的各个连通域并加以标记，如下图所示（引自 MarcWang 的 Gist）\n可以看到图中有三个独立的区域，我们希望找到并用数字标记它们，以便计算各个区域的轮廓、外接形状、质心等参数。连通域标记最基本的两个算法是 Seed-Filling 算法和 Two-Pass 算法，下面便来分别介绍它们，并用 Python 加以实现。\n（2022-01-04 更新：修复了 seed_filling 重复追加相邻像素的问题，修改了其它代码的表述。）\nSeed-Filling 算法 直译即种子填充，以图像中的特征像素为种子，然后不断向其它连通区域蔓延，直至将一个连通域完全填满。示意动图如下（引自 icvpr 的博客）\n具体思路为：循环遍历图像中的每一个像素，如果某个像素是未被标记过的特征像素，那么用数字对其进行标记，并寻找与之相邻的未被标记过的特征像素，再对这些像素也进行标记，然后以同样的方法继续寻找与这些像素相邻的像素并加以标记……如此循环往复，直至将这些互相连通的特征像素都标记完毕，此即连通域 1。接着继续遍历图像像素，看能不能找到下一个连通域。下面的实现采用深度优先搜索（DFS）的策略：将与当前位置相邻的特征像素压入栈中，弹出栈顶的像素，再把与这个像素相邻的特征像素压入栈中，重复操作直至栈内像素清空。\nimport numpy as np\rdef seed_filling(image, diag=False):\r'''\r用Seed-Filling算法标记图片中的连通域.\rParameters\r----------\rimage : ndarray, shape (nrow, ncol)\r图片数组,零值表示背景,非零值表示特征.\rdiag : bool\r指定邻域是否包含四个对角.\rReturns\r-------\rlabelled : ndarray, shape (nrow, ncol), dtype int\r表示连通域标签的数组,0表示背景,从1开始表示标签.\rnlabel : int\r连通域的个数.\r'''\r# 用-1表示未被标记过的特征像素.\rimage = np.asarray(image, dtype=bool)\rnrow, ncol = image.shape\rlabelled = np.where(image, -1, 0)\r# 指定邻域的范围.\rif diag:\roffsets = [\r(-1, -1), (-1, 0), (-1, 1),(0, -1),\r(0, 1), (1, -1), (1, 0), (1, 1)\r]\relse:\roffsets = [(-1, 0), (0, -1), (0, 1), (1, 0)]\rdef get_neighbor_indices(row, col):\r'''获取(row, col)位置邻域的下标.'''\rfor (dx, dy) in offsets:\rx = row + dx\ry = col + dy\rif 0 \u0026lt;= x \u0026lt; nrow and 0 \u0026lt;= y \u0026lt; ncol:\ryield x, y\rlabel = 1\rfor row in range(nrow):\rfor col in range(ncol):\r# 跳过背景像素和已经标记过的特征像素.\rif labelled[row, col] != -1:\rcontinue\r# 标记当前位置和邻域内的特征像素.\rcurrent_indices = []\rlabelled[row, col] = label\rfor neighbor_index in get_neighbor_indices(row, col):\rif labelled[neighbor_index] == -1:\rlabelled[neighbor_index] = label\rcurrent_indices.append(neighbor_index)\r# 不断寻找与特征像素相邻的特征像素并加以标记,直至再找不到特征像素.\rwhile current_indices:\rcurrent_index = current_indices.pop()\rlabelled[current_index] = label\rfor neighbor_index in get_neighbor_indices(*current_index):\rif labelled[neighbor_index] == -1:\rlabelled[neighbor_index] = label\rcurrent_indices.append(neighbor_index)\rlabel += 1\rreturn labelled, label - 1\r Two-Pass 算法 顾名思义，是会对图像过两遍循环的算法。第一遍循环先粗略地对特征像素进行标记，第二遍循环中再根据不同标签之间的关系对第一遍的结果进行修正。示意动图如下（引自 icvpr 的博客）\n具体思路为\n 第一遍循环时，若一个特征像素周围全是背景像素，那它很可能是一个新的连通域，需要赋予其一个新标签。如果这个特征像素周围有其它特征像素，则说明它们之间互相连通，此时随便用它们中的一个旧标签值来标记当前像素即可，同时要用并查集记录这些像素的标签间的关系。 因为我们总是只利用了当前像素邻域的信息（考虑到循环方向是从左上到右下，邻域只需要包含当前像素的上一行和本行的左边），所以第一遍循环中找出的那些连通域可能会在邻域之外相连，导致同一个连通域内的像素含有不同的标签值。不过利用第一遍循环时获得的标签之间的关系（记录在并查集中），可以在第二遍循环中将同属一个集合（连通域）的不同标签修正为同一个标签。 经过第二遍循环的修正后，虽然假独立区域会被归并，但它所持有的标签值依旧存在，这就导致本应连续的标签值序列中有缺口（gap）。所以依据需求可以进行第三遍循环，去掉这些缺口，将标签值修正为连续的整数序列。  其中提到的并查集是一种处理不相交集合的数据结构，支持查询元素所属、合并两个集合的操作。利用它就能处理标签和连通域之间的从属关系。我是看 算法学习笔记(1) : 并查集 这篇知乎专栏学的。下面的实现中仅采用路径压缩的优化，合并两个元素时始终让大的根节点被合并到小的根节点上，以保证连通域标签值的排列顺序跟数组的循环方向一致。\nfrom scipy.stats import rankdata\rclass UnionFind:\r'''用列表实现简单的并查集.'''\rdef __init__(self, n):\r'''创建含有n个节点的并查集,每个元素指向自己.'''\rself.parents = list(range(n))\rdef find(self, i):\r'''递归查找第i个节点的根节点,同时压缩路径.'''\rparent = self.parents[i]\rif parent == i:\rreturn i\relse:\rroot = self.find(parent)\rself.parents[i] = root\rreturn root\rdef union(self, i, j):\r'''合并节点i和j所属的两个集合.保证大的根节点被合并到小的根节点上.'''\rroot_i = self.find(i)\rroot_j = self.find(j)\rif root_i \u0026lt; root_j:\rself.parents[root_j] = root_i\relif root_i \u0026gt; root_j:\rself.parents[root_i] = root_j\relse:\rreturn None\rdef two_pass(image, diag=False):\r'''\r用Two-Pass算法标记图片中的连通域.\rParameters\r----------\rimage : ndarray, shape (nrow, ncol)\r图片数组,零值表示背景,非零值表示特征.\rdiag : bool\r指定邻域是否包含四个对角.\rReturns\r-------\rlabelled : ndarray, shape (nrow, ncol), dtype int\r表示连通域标签的数组,0表示背景,从1开始表示标签.\rnlabel : int\r连通域的个数.\r'''\rimage = np.asarray(image, dtype=bool)\rnrow, ncol = image.shape\rlabelled = np.zeros_like(image, dtype=int)\ruf = UnionFind(image.size // 2)\r# 指定邻域的范围,相比seed-filling只有半边.\rif diag:\roffsets = [(-1, -1), (-1, 0), (-1, 1), (0, -1)]\relse:\roffsets = [(-1, 0), (0, -1)]\rdef get_neighbor_indices(row, col):\r'''获取(row, col)位置邻域的下标.'''\rfor (dx, dy) in offsets:\rx = row + dx\ry = col + dy\rif 0 \u0026lt;= x \u0026lt; nrow and 0 \u0026lt;= y \u0026lt; ncol:\ryield x, y\rlabel = 1\rfor row in range(nrow):\rfor col in range(ncol):\r# 跳过背景像素.\rif not image[row, col]:\rcontinue\r# 寻找邻域内特征像素的标签.\rfeature_labels = []\rfor neighbor_index in get_neighbor_indices(row, col):\rneighbor_label = labelled[neighbor_index]\rif neighbor_label \u0026gt; 0:\rfeature_labels.append(neighbor_label)\r# 当前位置取邻域内的标签,同时记录邻域内标签间的关系.\rif feature_labels:\rfirst_label = feature_labels[0]\rlabelled[row, col] = first_label\rfor feature_label in feature_labels[1:]:\ruf.union(first_label, feature_label)\r# 若邻域内没有特征像素,当前位置获得新标签.\relse:\rlabelled[row, col] = label\rlabel += 1\r# 获取所有集合的根节点,由大小排名得到标签值.\rroots = [uf.find(i) for i in range(label)]\rlabels = rankdata(roots, method='dense') - 1\r# 利用advanced indexing替代循环修正标签数组.\rlabelled = labels[labelled]\rreturn labelled, labelled.max()\r 其中对标签值进行重新排名的部分用到了 scipy.stats.rankdata 函数，自己写循环来实现也可以，但当标签值较多时效率会远低于这个函数。从代码来看，Two-Pass 算法比 Seed-Filling 算法更复杂一些，但因为不需要进行递归式的填充，所以理论上要比后者更快。\n其它方法 许多图像处理的包里有现成的函数，例如\n scipy.ndimage.label skimage.measure.label cv2.connectedComponets  具体信息和用法请查阅文档。顺便测一下各方法的速度，如下图所示（通过 IPython 的 %timeit 测得）\n显然调包要比手工实现快 100 倍，这是因为 scipy.ndimage.label 和 skimage.measure.label 使用了更高级的算法和 Cython 代码。因为我不懂 OpenCV，所以这里没有展示 cv2.connectedComponets 的结果。\n值得注意的是，虽然上一节说理论上 Two-Pass 算法比 Seed-Filling 快，但测试结果相差不大，这可能是由于纯 Python 实现体现不出二者的差异（毕竟完全没用到 NumPy 数组的向量性质），也可能是我代码写的太烂，还请懂行的读者指点一下。\n例子 以一个随机生成的 (50, 50) 的二值数组为例，展示 scipy.ndimage.label、seed_filling 和 two_pass 三者的效果，采用 8 邻域连通，如下图所示\n可以看到三种方法都找出了 17 个连通域，并且连标签顺序都一模一样（填色相同）。不过若 Two-Pass 法中的并查集采用其它合并策略，标签顺序就很可能发生变化。下面再以一个更复杂的 (800, 800) 大小的空露露图片为例\n将图片二值化后再进行连通域标记，可以看到おつるる的字样被区分成多个区域，猫猫和露露也都被识别了出来。代码如下\nimport numpy as np\rfrom PIL import Image\rfrom scipy import ndimage\rimport matplotlib.pyplot as plt\rimport matplotlib.colors as mcolors\rfrom connected_components import two_pass, seed_filling\rif __name__ == '__main__':\r# 将测试图片二值化.\rpicname = 'ruru.png'\rimage = Image.open(picname)\rimage = np.array(image.convert('L'))\rimage = ndimage.gaussian_filter(image, sigma=2)\rimage = np.where(image \u0026lt; 220, 1, 0)\r# 设置二值图像与分类图像所需的cmap.\rcmap1 = mcolors.ListedColormap(\r['white', 'black'])\rwhite = np.array([1, 1, 1])\rcmap2 = mcolors.ListedColormap(\rnp.vstack([white, plt.cm.tab20.colors])\r)\rfig, axes = plt.subplots(2, 2, figsize=(10, 10))\r# 关闭ticks的显示.\rfor ax in axes.flat:\rax.xaxis.set_visible(False)\rax.yaxis.set_visible(False)\r# 显示二值化的图像.\raxes[0, 0].imshow(image, cmap=cmap1, interpolation='nearest')\raxes[0, 0].set_title('Image', fontsize='large')\r# 显示scipy.ndimage.label的结果.\r# 注意imshow中需要指定interpolation为'nearest'或'none',否则结果有紫边.\rs = np.ones((3, 3), dtype=int)\rlabelled, nlabel = ndimage.label(image, structure=s)\raxes[0, 1].imshow(labelled, cmap=cmap2, interpolation='nearest')\raxes[0, 1].set_title(\rf'scipy.ndimage.label ({nlabel} labels)', fontsize='large'\r)\r# 显示Two-Pass算法的结果.\rlabelled, nlabel = two_pass(image, diag=True)\raxes[1, 0].imshow(labelled, cmap=cmap2, interpolation='nearest')\raxes[1, 0].set_title(f'Two-Pass ({nlabel} labels)', fontsize='large')\r# 显示Seed-Filling算法的结果.\rlabelled, nlabel = seed_filling(image, diag=True)\raxes[1, 1].imshow(labelled, cmap=cmap2, interpolation='nearest')\raxes[1, 1].set_title(f'Seed-Filling ({nlabel} labels)', fontsize='large')\rfig.savefig('image.png', dpi=200, bbox_inches='tight')\rplt.close(fig)\r 不只是邻接 虽然 scipy.ndimage.label 和 skimage.measure.label 要比手工实现更快，但它们都只支持 4 邻域和 8 邻域的连通规则，而手工实现还可以采用别的连通规则。例如，改动一下 seed_filling 中关于 offsets 的部分，使之能够表示以当前像素为原点，r 为半径的圆形邻域\noffsets = []\rfor i in range(-r, r + 1):\rk = r - abs(i)\rfor j in range(-k, k + 1):\roffsets.append((i, j))\roffsets.remove((0, 0)) # 去掉原点.\r 在某些情况下也许能派上用场。\n参考链接 网上很多教程抄了这篇，但里面 Two-Pass 算法的代码里不知道为什么没用并查集，可能会有问题。\nOpenCV_连通区域分析（Connected Component Analysis-Labeling）\n一篇英文的对 Two-Pass 算法的介绍，Github 上还带有 Python 实现。\nConnected Component Labelling\n代码参考了\n你都用 Python 来做什么？laiyonghao 的回答\n连通域的原理与Python实现\n","date":"2021-07-19","permalink":"https://zhajiman.github.io/post/connected_component_labelling/","tags":["图像处理"],"title":"二值图像的连通域标记"},{"content":"0. 前言 承接 Matplotlib 系列：colormap 的设置 一文，这次介绍 colorbar。所谓 colorbar 即主图旁一个长条状的小图，能够辅助表示主图中 colormap 的颜色组成和颜色与数值的对应关系。本文将会依次介绍 colorbar 的基本用法、如何设置刻度，以及怎么为组图添加 colorbar。代码基于 Matplotlib 3.3.4。\n1. colorbar 的基本用法 Colorbar 主要通过 figure.Figure.colorbar 方法绘制，先介绍常用的几个参数\n mappable：直译为“可映射的”，要求是 matplotlib.cm.ScalarMappable 对象，能够向 colorbar 提供数据与颜色间的映射关系（即 colormap 和 normalization 信息）。主图中使用 contourf、pcolormesh 和 imshow 等二维绘图函数时返回的对象都是 ScalarMappable 的子类。 cax：colorbar 本质上也是一种特殊的 Axes，我们为了在画布上决定其位置、形状和大小，可以事先画出一个空 Axes，然后将这个 Axes 提供给 cax 参数，那么这个空 Axes 就会变成 colorbar。 ax：有时我们懒得手动为 colorbar 准备好位置，那么可以用 ax 参数指定 colorbar 依附于哪个 Axes，接着 colorbar 会自动从这个 Axes 里“偷”一部分空间来作为自己的空间。 orientation：指定 colorbar 的朝向，默认为垂直方向。类似的参数还有 location。 extend：设置是否在 colorbar 两端额外标出归一化范围外的颜色。如果 colormap 有设置过 set_under 和 set_over，那么使用这两个颜色。 ticks：指定 colorbar 的刻度位置，可以接受数值序列或 Locator 对象。 format：指定 colorbar 的刻度标签的格式，可以接受格式字符串，例如 '%.3f'，或 Formatter 对象。 label：整个 colorbar 的标签，类似于 Axes 的 xlabel 和 ylabel。  此外 colorbar 还有些设置不能在初始化的时候一次性搞定，需要接着调用方法才能完成。\n1.1 单独绘制 colorbar 虽然 colorbar 一般依附于一张填色的主图，但其实只要给出 cmap 和 norm 就能决定 colorbar 了。下面给出单独绘制 colorbar 的例子\nimport copy\rimport numpy as np\rimport matplotlib.pyplot as plt\rimport matplotlib.cm as cm\rimport matplotlib.colors as mcolors\rfig, axes = plt.subplots(4, 1, figsize=(10, 5))\rfig.subplots_adjust(hspace=4)\r# 第一个colorbar使用线性的Normalize.\rcmap1 = copy.copy(cm.viridis)\rnorm1 = mcolors.Normalize(vmin=0, vmax=100)\rim1 = cm.ScalarMappable(norm=norm1, cmap=cmap1)\rcbar1 = fig.colorbar(\rim1, cax=axes[0], orientation='horizontal',\rticks=np.linspace(0, 100, 11),\rlabel='colorbar with Normalize'\r)\r# 第二个colorbar开启extend参数.\rcmap2 = copy.copy(cm.viridis)\rcmap2.set_under('black')\rcmap2.set_over('red')\rnorm2 = mcolors.Normalize(vmin=0, vmax=100)\rim2 = cm.ScalarMappable(norm=norm2, cmap=cmap2)\rcbar2 = fig.colorbar(\rim2, cax=axes[1], orientation='horizontal',\rextend='both', ticks=np.linspace(0, 100, 11),\rlabel='extended colorbar with Normalize'\r)\r# 第三个colorbar使用对数的LogNorm.\rcmap3 = copy.copy(cm.viridis)\rnorm3 = mcolors.LogNorm(vmin=1E0, vmax=1E3)\rim3 = cm.ScalarMappable(norm=norm3, cmap=cmap3)\r# 使用LogNorm时,colorbar会自动选取合适的Locator和Formatter.\rcbar3 = fig.colorbar(\rim3, cax=axes[2], orientation='horizontal',\rlabel='colorbar with LogNorm',\r)\r# 第四个colorbar使用BoundaryNorm.\rbins = [0, 1, 10, 20, 50, 100]\rnbin = len(bins) - 1\rcmap4 = cm.get_cmap('viridis', nbin)\rnorm4 = mcolors.BoundaryNorm(bins, nbin)\rim4 = cm.ScalarMappable(norm=norm4, cmap=cmap4)\r# 使用BoundaryNorm时,colorbar会自动按bins标出刻度.\rcbar4 = fig.colorbar(\rim4, cax=axes[3], orientation='horizontal',\rlabel='colorbar with BoundaryNorm'\r)\rplt.show()\r colorbar 使用的 cmap 和 norm 可以通过 cbar.cmap 和 cbar.norm 属性获取。\n1.2 向主图添加 colorbar 日常使用中一般不会单独画出 colorbar，而是将 colorbar 添加给一张主图。此时需要将主图中画填色图时返回的 ScalarMappable 对象传给 colorbar，并利用 cax 或 ax 参数指定 colorbar 的位置。下面是一个例子\nimport matplotlib.patches as mpatches\rimport matplotlib.transforms as mtransforms\rdef add_box(ax):\r'''用红框标出一个ax的范围.'''\raxpos = ax.get_position()\rrect = mpatches.Rectangle(\r(axpos.x0, axpos.y0), axpos.width, axpos.height,\rlw=3, ls='--', ec='r', fc='none', alpha=0.5,\rtransform=ax.figure.transFigure\r)\rax.patches.append(rect)\rdef add_right_cax(ax, pad, width):\r'''\r在一个ax右边追加与之等高的cax.\rpad是cax与ax的间距,width是cax的宽度.\r'''\raxpos = ax.get_position()\rcaxpos = mtransforms.Bbox.from_extents(\raxpos.x1 + pad,\raxpos.y0,\raxpos.x1 + pad + width,\raxpos.y1\r)\rcax = ax.figure.add_axes(caxpos)\rreturn cax\rdef test_data():\r'''生成测试数据.'''\rx = np.linspace(-3, 3, 200)\ry = np.linspace(-3, 3, 200)\rX, Y = np.meshgrid(x, y)\rZ = np.exp(-X**2) + np.exp(-Y**2)\r# 将Z缩放至[0, 100].\rZ = (Z - Z.min()) / (Z.max() - Z.min()) * 100\rreturn X, Y, Z\rX, Y, Z = test_data()\rfig, axes = plt.subplots(2, 2, figsize=(10, 10))\rfig.subplots_adjust(hspace=0.2, wspace=0.2)\r# 提前用红框圈出每个ax的范围,并关闭刻度显示.\rfor ax in axes.flat:\radd_box(ax)\rax.axis('off')\r# 第一个子图中不画出colorbar.\rim = axes[0, 0].pcolormesh(X, Y, Z, shading='nearest')\raxes[0, 0].set_title('without colorbar')\r# 第二个子图中画出依附于ax的垂直的colorbar.\rim = axes[0, 1].pcolormesh(X, Y, Z, shading='nearest')\rcbar = fig.colorbar(im, ax=axes[0, 1], orientation='vertical')\raxes[0, 1].set_title('add vertical colorbar to ax')\r# 第三个子图中画出依附于ax的水平的colorbar.\rim = axes[1, 0].pcolormesh(X, Y, Z, shading='nearest')\rcbar = fig.colorbar(im, ax=axes[1, 0], orientation='horizontal')\raxes[1, 0].set_title('add horizontal colorbar to ax')\r# 第三个子图中将垂直的colorbar画在cax上.\rim = axes[1, 1].pcolormesh(X, Y, Z, shading='nearest')\rcax = add_right_cax(axes[1, 1], pad=0.02, width=0.02)\rcbar = fig.colorbar(im, cax=cax)\raxes[1, 1].set_title('add vertical colorbar to cax')\rplt.show()\r 组图通过 plt.subplots 函数创建，这里用红色虚线方框圈出每个子图开始时的范围。然后第一个子图内画图但不添加 colorbar，可以看到其范围与红框重合；第二个子图内用 ax 参数指定 colorbar 依附于该子图，可以看到子图的水平范围被 colorbar 偷走了一部分，同理第三个子图的垂直范围被偷走了一部分；而第四个子图中因为手动在子图右边创建了一个新的 Axes 并指定为 cax，所以 colorbar 并没有挤占子图原有的空间。\n总之，向主图添加 colorbar 时，ax 参数用起来更方便，但会改变主图的范围；cax 参数需要提前为 colorbar 准备一个 Axes，但 colorbar 的摆放位置更灵活。\n2. 设置刻度 第 1 节中提到过，在初始化 colorbar 时通过 ticks 和 format 参数即可设置刻度。实际上，colorbar 在接受刻度的设置后，会将它们传给底层的 Axes 对象，利用 Axes 的方法来实现刻度的标注。所以为 colorbar 设置刻度有两种思路\n 利用 colorbar 提供的接口设置刻度，优点是简单直接，缺点是对于小刻度等参数无法进行细致的设定。 直接操作 colorbar 底层的 Axes，优点是设置更细致，缺点是可能会受 cbar.update_ticks 方法的干扰。  正因为这两种思路都行得通，所以你上网搜如何设置刻度时能找到五花八门的方法，下面便来一一辨析这些方法。\n另外需要提前说明一下，colorbar 不同于普通的 Axes，只会显示落入 cbar.vmin 和 cbar.vmax 这两个值范围内的刻度，而这两个值由 cbar.norm 的属性决定（例外会在后面提到）。\n2.1 ticks 和 format 参数 import matplotlib.ticker as mticker\rcmap = cm.viridis\rnorm = mcolors.Normalize(vmin=0, vmax=100)\rim = cm.ScalarMappable(norm=norm, cmap=cmap)\rlocator = mticker.MultipleLocator(10)\rformatter = mticker.StrMethodFormatter('{x:.1f}')\rcbar = fig.colorbar(\rim, cax=ax, orientation='horizontal',\rticks=locator, format=formatter\r)\rcbar.minorticks_on()\r 在初始化 colorbar 时直接指定 ticks 和 format 参数即可。\n小刻度则通过 minorticks_on 方法开启，可惜这个方法不提供任可供调节的参数，查看源码会发现，colorbar 是借助 matplotlib.ticker.AutoMinorLocator 实现小刻度的，其中小刻度的间隔数 n 被硬编码为默认值 None，所以小刻度的数目会根据大刻度的数值自动设为 3 个或 4 个，例如图中两个大刻度间就是 4 个小刻度。\n2.2 locator 和 formatter 属性 cbar = fig.colorbar(im, cax=ax, orientation='horizontal')\rcbar.locator = locator\rcbar.formatter = formatter\rcbar.minorticks_on()\rcbar.update_ticks()\r 图跟 2.1 节的一样。直接修改 locator 和 formatter 属性，接着调用 update_ticks 方法刷新刻度，将这两个属性传给底层的 Axes，从而使刻度生效。2.1 节中不需要刷新是因为初始化的最后会自动刷新。\n2.3 set_ticks 和 set_ticklabels 方法 ticks = np.linspace(0, 100, 11)\rticklabels = [formatter(tick) for tick in ticks]\rcbar = fig.colorbar(im, cax=ax, orientation='horizontal')\rcbar.set_ticks(ticks)\rcbar.set_ticklabels(ticklabels)\rcbar.minorticks_on()\r 图跟 2.1 节的一样。这个方法适用于手动给出刻度和与之匹配的刻度标签的情况。同时 set_ticks 和 set_ticklabels 方法都有一个布尔类型的 update_ticks 参数，效果同 2.2 节所述，因为默认为 True，所以可以不用管它。奇怪的是，set_ticks 方法还可以接受 Locator 对象，不过当 Locator 与刻度标签对不上时就会发出警告并产生错误的结果。\n也许你会联想到 Axes 设置刻度的方法，并进行这样的尝试\ncbar.ax.set_xticks(ticks)\rcbar.ax.set_xticklabels(ticklabels)\r 可惜这种方法行不通，也是会报警加出错。\n2.4 set_major_locator 和 set_major_formatter 方法 cbar = fig.colorbar(im, cax=ax, orientation='horizontal')\rcbar.ax.xaxis.set_major_locator(locator)\rcbar.ax.xaxis.set_minor_locator(mticker.AutoMinorLocator(2))\rcbar.ax.xaxis.set_major_formatter(formatter)\r# cbar.update_ticks()\r 图跟 2.1 节的一样。虽然 2.3 节中直接调用 set_xticks 和 set_xticklabels 会失败，但神秘的是调用 set_major_locator 和 set_major_formatter 却可以，你甚至可以用 set_minor_locator 来实现更细致的小刻度。这里因为 colorbar 是水平放置的，所以操作的是 xaxis，垂直方向换成 yaxis 即可。\n这种方法的缺点是，colorbar 的 locator 属性与 xaxis 的并不一致\nIn : print(cbar.locator)\rOut: \u0026lt;matplotlib.colorbar._ColorbarAutoLocator object at 0x000001B424E36AF0\u0026gt;\rIn : print(cbar.ax.xaxis.get_major_locator())\rOut: \u0026lt;matplotlib.ticker.MultipleLocator object at 0x000001B424E366A0\u0026gt;\r 尽管画出来的图是 MultipleLocator 的效果，但 cbar.locator 依旧保留初始化时的默认值，cbar.formatter 同理。如果此时执行 cbar.update_ticks()，就会将 cbar.ax.xaxis 的 locator 和 formatter 更新成 cbar.locator 和 cbar.formatter 的值——即变回默认效果。奇怪的是小刻度的 locator 并不受 update_ticks 的影响，小刻度依然得到保留。\n2.5 对数刻度 1.1 节中展示过，当传入的 mappable 的 norm 是 LogNorm 时，colorbar 会自动采取对数刻度和科学计数法的刻度标签，并开启小刻度。下面是一个不用科学计数法，并关掉小刻度的例子\nnorm = mcolors.LogNorm(vmin=1E0, vmax=1E3)\rim = cm.ScalarMappable(norm=norm, cmap=cmap)\rcbar = fig.colorbar(\rim, cax=ax, orientation='horizontal',\rformat=mticker.ScalarFormatter()\r)\rcbar.minorticks_off()\r 2.6 更多设置 如果想进一步设置刻度的参数（刻度长度、标签字体等），需要通过底层的 cbar.ax.tick_params 方法来实现。例如\ncbar.ax.tick_params(length=2, labelsize='x-small')\r 总结一下的话，colorbar 提供了设置刻度的接口，但做得还不够完善，以至于我们需要直接操作底层的 Axes。希望以后 Matplotlib 能对此加以改善。\n3. Contourf 中的 colorbar 把 pcolor、imshow 等函数的返回值传给 colorbar 时，colorbar 中会显示连续完整的 colormap；但若把 contourf 函数的返回值传给 colorbar 时，显示的就不再是完整的 colormap，而是等高线之间的填色（填色规则请见 Matplotlib 系列：colormap 的设置 第 3.1 节），下面是一个 pcolormesh 与 contourf 相对比的例子\nX, Y, Z = test_data()\rcmap = cm.viridis\rnorm = mcolors.Normalize(vmin=0, vmax=100)\rlevels = [10, 20, 40, 80]\rfig, axes = plt.subplots(1, 2, figsize=(10, 5))\rfor ax in axes:\rax.axis('off')\r# 第一张子图画pcolormesh.\rim = axes[0].pcolormesh(X, Y, Z, cmap=cmap, norm=norm, shading='nearest')\rcbar = fig.colorbar(im, ax=axes[0], extend='both')\raxes[0].set_title('pcolormesh')\r# 第二张子图画contourf.\rim = axes[1].contourf(\rX, Y, Z, levels=levels, cmap=cmap, norm=norm, extend='both'\r)\rcbar = fig.colorbar(im, ax=axes[1])\raxes[1].set_title('contourf')\rplt.show()\r 可以看到效果与上面描述的一致，colorbar 上颜色间的分界位置也与 levels 的数值大小相对应。第 2 节中提到过，colorbar 的显示范围由 cbar.vmin 和 cbar.vmax 决定，且这两个值与 cbar.norm.vmin 和 cbar.norm.vmax 相同——不过使用 contourf 的返回值作为 mappable 时则是例外，这里 cbar.vmin 和 cbar.vmax 由 levels 的边界决定。所以上图中 colorbar 的范围为 [10, 80]。\n另外若 contourf 中指定过 extend 参数，那么其返回值会带有 extend 的信息，初始化 colorbar 时就不应该再设定 extend 参数了。Matplotlib 3.3 以后同时使用 extend 参数的行为被废弃。\n4. 为组图添加 colorbar 4.1 为每个子图添加 最简单的方法是在绘制每个子图的 colorbar 时，将 ax 参数指定为子图的 Axes，缺点是会改变子图形状，不过可以之后用 ax.set_aspect 等方法进行调整。下面利用 1.2 节中的 add_right_cax 函数实现 cax 的版本\nX, Y, Z = test_data()\rcmap = cm.viridis\rnorm = mcolors.Normalize(vmin=0, vmax=100)\rfig, axes = plt.subplots(2, 2, figsize=(8, 8))\r# 调节子图间的宽度,以留出放colorbar的空间.\rfig.subplots_adjust(wspace=0.4)\rfor ax in axes.flat:\rax.axis('off')\rcax = add_right_cax(ax, pad=0.01, width=0.02)\rim = ax.pcolormesh(X, Y, Z, cmap=cmap, norm=norm, shading='nearest')\rcbar = fig.colorbar(im, cax=cax)\rplt.show()\r 更高级的方法是使用 mpl_toolkits.axes_grid1.ImageGrid 类，例如\nfrom mpl_toolkits.axes_grid1 import ImageGrid\rfig = plt.figure(figsize=(8, 8))\rgrid = ImageGrid(\rfig, 111, nrows_ncols=(2, 2), axes_pad=0.5,\rcbar_mode='each', cbar_location='right', cbar_pad=0.1\r)\r# 这里ax是mpl_toolkits.axes_grid1.mpl_axes.Axes\rfor ax in grid:\rax.axis('off')\rim = ax.pcolormesh(X, Y, Z, cmap=cmap, norm=norm, shading='nearest')\r# 官网例子中的cax.colorbar(im)用法自Matplotlib 3.2起废弃.\rcbar = fig.colorbar(im, cax=ax.cax)\rplt.show()\r 结果跟上面一张图差不多。ImageGrid 适合创建子图宽高比固定的组图（例如 imshow 的图像或等经纬度投影的地图），并且对于 colorbar 位置和间距的设置非常便利。此外还有利用 matplotlib.gridspec.GridSpec 和 mpl_toolkits.axes_grid1.axes_divider 的方法，这里就不细讲了。\n4.2 为整个组图添加 其实 colorbar 的 ax 参数还可以接受 Axes 组成的列表（数组），内部会通过 matplotlib.transforms.Bbox.union 方法计算这些 Axes 占据的公共空间，再计算出 colorbar 应占的空间，从而实现为所有 Axes 只添加一个 colorbar。例如\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\rfor ax in axes.flat:\rax.axis('off')\rim = ax.pcolormesh(X, Y, Z, cmap=cmap, norm=norm, shading='nearest')\rcbar = fig.colorbar(im, ax=axes)\rplt.show()\r 再举个 ImageGrid 的例子\nfig = plt.figure(figsize=(8, 8))\rgrid = ImageGrid(\rfig, 111, nrows_ncols=(2, 2), axes_pad=0.5,\rcbar_mode='single', cbar_location='right', cbar_pad=0.2,\r)\rfor ax in grid:\rax.axis('off')\rim = ax.pcolormesh(X, Y, Z, cmap=cmap, norm=norm, shading='nearest')\rcbar = fig.colorbar(im, cax=ax.cax)\rplt.show()\r 结果同上一张图。如果有更复杂的需求，例如在不改变子图形状的前提下，组图中不同区域的子图共用不同的 colorbar，那么建议使用 add_axes 方法，参考 1.2 节的 add_right_cax 函数，将之改写到可接受多个 Axes，指定任意方向；或利用 matplotlib.gridspec.GridSpec 将 cax 穿插在组图间。感兴趣的读者可以读读参考链接中最后那篇。\n5. 参考链接 官方教程\nCustomized Colorbars Tutorial\nOverview of axes_grid1 toolkit\nCartopy 的例子\nUsing Cartopy and AxesGrid toolkit\n可能是全网最详细的 colorbar 调整教程\nmatplotlibのcolorbarを解剖してわかったこと、あるいはもうcolorbar調整に苦労したくない人に捧げる話\n","date":"2021-07-10","permalink":"https://zhajiman.github.io/post/matplotlib_colorbar/","tags":["matplotlib"],"title":"Matplotlib 系列：colorbar 的设置"},{"content":"0. 前言 所谓 colormap（颜色表），就是将一系列颜色按给定的顺序排列在一起。其用处是，我们可以通过某种映射关系，将一系列数值映射到一张 colormap 上去，使不同大小的数值对应不同的颜色。这样一来，在绘制填色图时便能直观地用颜色来反映数值的分布。\n在 Matplotlib 中，数值到颜色的映射关系可以用下面这张流程图来表示\n图中分为前后两部分\n 首先将数组归一化（normalize）到浮点型的 [0, 1] 范围（或整型的 [0, N - 1] 范围）上去。 再把归一化的数组输入给 colormap，查询每个数值对应的颜色。  第二部分的映射关系是固定不变的，但第一部分的映射关系可以通过归一化相关的类加以改变，进而实现对数色标、对称色标、离散色标等一系列填色效果。\n本文将会依次介绍 Colormap 类、Normalize 类，以及实际应用的例子。代码基于 Matplotlib 3.3.4。\n（2022-01-17 更新：增加了一些解释说明，删掉了不实用的介绍，加入了 BoundaryNorm 实现的红蓝色标的例子。）\n1. Colormap 很容易想到，一系列颜色可以用 (N, 3) 或 (N, 4) 形状的 RGB(A) 数组表示。但是 Matplotlib 中的 colormap 并非简单的数组，而是专门用一个 Colormap 类实现的，有着更加方便的重采样（resample）功能。内置的所有 colormap 存放在 matplotlib.cm 模块下，其外观可以在官网的 Choosing Colormaps in Matplotlib 页面看到。\nColormap 分为两个子类：ListedColormap 和 LinearSegmentedColormap，它们被存放在 matplotlib.colors 模块下。在介绍它们之前先做点准备工作\nimport numpy as np import matplotlib.pyplot as plt import matplotlib.cm as cm import matplotlib.colors as mcolors def show_cmap(cmap, norm=None, extend=None): '''展示一个colormap.''' if norm is None: norm = mcolors.Normalize(vmin=0, vmax=cmap.N) im = cm.ScalarMappable(norm=norm, cmap=cmap) fig, ax = plt.subplots(figsize=(6, 1)) fig.subplots_adjust(bottom=0.5) fig.colorbar(im, cax=ax, orientation='horizontal', extend=extend) plt.show()  1.1 ListedColormap 顾名思义，将所需的颜色全部列出来，便能生成这一类的 colormap。初始化参数为\nListedColormap(colors, name='from_list', N=None)  colors 是颜色名组成的列表或 RGB(A) 数组，name 和 N 分别是该 colormap 的名字和所含颜色数，不过自定义对象一般不需要取名，颜色数默认为 len(colors)，所以这两个参数基本用不上。这些参数随后会被赋给对象的同名属性。例如\ncolors = ['darkorange', 'gold', 'lawngreen', 'lightseagreen'] cmap = mcolors.ListedColormap(colors) show_cmap(cmap)  In : cmap.colors Out: ['darkorange', 'gold', 'lawngreen', 'lightseagreen'] In : cmap.N Out: 4  文档中提到的 qualitative colormap 均为 ListedColormap，因为颜色有限且分隔明显，所以能定性反应数值的特征，如下图所示\n以内置的 Set1 为例\nIn : cm.Set1.colors Out: ((0.8941176470588236, 0.10196078431372549, 0.10980392156862745), (0.21568627450980393, 0.49411764705882355, 0.7215686274509804), (0.30196078431372547, 0.6862745098039216, 0.2901960784313726), (0.596078431372549, 0.3058823529411765, 0.6392156862745098), (1.0, 0.4980392156862745, 0.0), (1.0, 1.0, 0.2), (0.6509803921568628, 0.33725490196078434, 0.1568627450980392), (0.9686274509803922, 0.5058823529411764, 0.7490196078431373), (0.6, 0.6, 0.6)) In : cm.Set1.N Out: 9  可以看到 colors 属性以嵌套元组的形式存储。\ncmap 对象可以直接用数值参数调用，索引数值对应的 RGBA 值。根据数值是整型还是浮点型，对应关系也会有所不同，如下图所示\n当参数 x 为整数时，对应第 x - 1 个颜色；当 x 为浮点数时，根据它所在的区间决定颜色。当 x 超出 [0, N - 1] 或 [0, 1] 的范围时，对应于第一个和最后一个颜色。下面的例子里用两种方式获得了 cmap 中所有颜色的 RGBA 值\nIn : cmap(np.arange(cmap.N)) Out: array([[1. , 0.54901961, 0. , 1. ], [1. , 0.84313725, 0. , 1. ], [0.48627451, 0.98823529, 0. , 1. ], [0.1254902 , 0.69803922, 0.66666667, 1. ]]) In : cmap(np.linspace(0, 1, cmap.N)) Out: array([[1. , 0.54901961, 0. , 1. ], [1. , 0.84313725, 0. , 1. ], [0.48627451, 0.98823529, 0. , 1. ], [0.1254902 , 0.69803922, 0.66666667, 1. ]])  显然结果是相同的。再举个利用索引结果创建新 colormap 的例子\ncmap_new = mcolors.ListedColormap( cmap(np.linspace(0, 1, 5)) ) show_cmap(cmap_new)  cmap_new 看起来会是这个样子\n因为给出的参数中，最后两个数落进了同一个区间，所以对应的颜色相同。\n1.2 LinearSegmentedColormap 顾名思义，是通过线性分段构建的 colormap，需要给出红绿蓝三种成分的锚点，然后用线性插值的方式得出锚点间的颜色。直接初始化对象的方法较难理解，说实话我也没太看懂，所以这里介绍其辅助方法\nLinearSegmentedColormap.from_list(name, colors, N=256, gamma=1.0)  name 是对象的名字，这回躲不掉必须填了；colors 是锚点的颜色，锚点对应的数值默认等距分布在 [0, 1] 区间上，不过可以在 colors 的每个颜色前指定数值；N 指定最后插值出几个颜色，默认为 256，所以基本看不出颜色间的间隔；gamma 是伽马校正的参数。例如\ncmap1 = mcolors.LinearSegmentedColormap.from_list('cmap1', colors) show_cmap(cmap1) nodes = [0, 0.8, 0.9, 1] cmap2 = mcolors.LinearSegmentedColormap.from_list( 'cmap2', list(zip(nodes, colors)) ) show_cmap(cmap2)  第二个 colormap 因为黄色系的锚点到了 0.8 的位置，所以视觉上黄色占了很大面积。\n大部分内置 colormap 都属于 LinearSegmentedColormap，例如文档中提到的 sequential colormap，因为颜色连续过渡自然，所以能定量反应数值的大小，如下图所示\n以内置的 jet 为例\nIn : cm.jet.colors ------------------------------------------------------------------------- AttributeError: 'LinearSegmentedColormap' object has no attribute 'colors In : cm.jet.N Out: 256  即 LinearSegmentedColormap 虽然由 N 个颜色组成，但不能像 ListedColormap 那样把它们直接列举出来。cmap 同样可以被调用，当参数 x 为整数时，对应于第 x + 1 个颜色；当 x 为浮点数时，会通过线性插值获取相邻两个颜色中间的颜色。因此，LinearSegmentedColormap 的重采样不仅不会出现重复的颜色，还能得到更为连续渐变的颜色。不过有一说一，当颜色足够多时（即 N 很大时），两种 colormap 的区别就微乎其微了。\n1.3 get_cmap 函数 有时我们希望通过重采样直接得到一个新的 colormap，而不是得到一组 RGBA 值，这个需求可以用 get_cmap 函数实现，例如对 jet 采样 8 个颜色\n# 等价于cm.jet(np.linspace(0, 1, 8)) cmap = cm.get_cmap('jet', 8)  效果如下图，并且采样得到的 colormap 依旧为 LinearSegmentedColormap。\n1.4 set_under、set_over 和 set_bad 1.1 节中提到过，直接调用 cmap 时，若参数 x 超出范围，那么会映射给第一个或最后一个颜色。而 cmap 的 set_under 方法能够改变 x \u0026lt; 0 时对应的颜色，set_over 方法能够改变 x \u0026gt; N - 1 或 x \u0026gt; 1 时对应的颜色。set_bad 则能改变缺测值对应的颜色（见 NumPy 系列：缺测值处理 最后一节）。\n使用 fig.colorbar 方法画 colorbar 时，通过 extend 参数可以指定是否在 colorbar 两端显示出 under 与 over 的颜色。比如\ncmap = cm.get_cmap('jet', 8) cmap.set_under('black') cmap.set_over('white') show_cmap(cmap, extend='both')  1.5 修改内置 colormap 用 get_cmap 函数重采样得到的 colormap 可以直接用 set_xxx 系列方法进行修改，但对内置的 colormap 这样操作则会产生 MatplotlibDeprecationWarning。因为内置 colormap 都是全局对象，原地修改时会影响全局的效果。将来这一行为将会直接报错，官方建议先拷贝再修改。\nimport copy cmap = copy.copy(cm.jet) cmap.set_under('black') cmap.set_over('white')  1.6 拼接内置 colormap 我们可以以内置的 colormap 为素材，自由拼接出新的 colormap。例如\ncolors_cool = cm.cool(np.linspace(0, 1, 128)) colors_spring = cm.spring(np.linspace(0, 1, 128)) colors_all = np.vstack((colors_cool, colors_spring)) cmap_merged = mcolors.ListedColormap(colors_all) show_cmap(cmap_merged)  2. Normalization 上一节的重点是，colormap 能把 [0, 1] 或 [0, N - 1] 范围内的值映射到颜色上，那么这一节就来叙述如何利用归一化的类把原始数据映射到 [0, 1] 或 [0, N - 1] 上。相关的类都存放在 matplotlib.colors 模块中，下面介绍最常用的几种。\n2.1 Normalize 各种二维绘图函数在进行归一化时默认使用 Normalize 类，其它类也都继承自它。其参数为\nNormalize(vmin=None, vmax=None, clip=False)  若给出了 vmin 和 vmax，调用创建的对象时会按线性关系 $$ y = \\frac{x - vmin}{vmax - vmin} $$\n将数据 x 映射为 y。显然只有 [vmin, vmax] 范围内的 x 会刚好映射到 [0, 1] 上，其它范围的 x 会映射出小于 0 或大于 1 的值。若不给定 vmin 和 max，默认用 x 的最小值最大值代替，此时 y 的范围一定是 [0, 1]。例如\nx = np.arange(0, 6) norm = mcolors.Normalize()  In : norm(x) Out: masked_array(data=[0. , 0.2, 0.4, 0.6, 0.8, 1. ], mask=False, fill_value=1e+20)  经 norm 归一化后的值可以传给 colormap，进而按第一节介绍的映射规则得到画图用的颜色。即便 y 超出了 [0, 1] 的范围，也可以映射给第一个或最后一个颜色（或者 set_under 和 set_over 指定的颜色）。换句话说，[vmin, vmax] 范围外的 x 自然对应于 colormap 两端的颜色。\nclip 参数为 True 时，能把 [vmin, vmax] 范围外的 x 映射为 0 或 1，因此使 set_under 与 set_over 的设置失效。一般默认为 False 即可。\n2.2 LogNorm LogNorm 的参数与 Normalize 相同，会先对数据求对数后再进行线性映射 $$ y = \\frac{\\log_{10}(x) - \\log_{10}(vmin)}{\\log_{10}(vmax) - \\log_{10}(vmin)} $$ 其中 vmin 和 vmax 必须为正数，否则会报错；x 可以小于等于 0，不过结果会缺测。例如\nx = np.logspace(0, 3, 6) norm = mcolors.LogNorm(vmin=1E0, vmax=1E3)  In : norm(x) Out: masked_array(data=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], mask=[False, False, False, False, False, False], fill_value=1e+20)  2.3 TwoSlopeNorm 将 [vmin, vmax] 分成两个区间，进行分段线性映射。参数为\nTwoSlopeNorm(vcenter, vmin=None, vmax=None)  其中新增的 vcenter 是分段点，要求 vmin、vcenter 和 vmax 的值依次递增。映射的具体公式为 $$ y = \\begin{cases} -\\infty \u0026amp;\\text{if} \\quad x \u0026lt; vmin \\newline \\frac{(x - vmin)}{2(vcenter - vmin)} \u0026amp;\\text{if} \\quad vmin \\le x \u0026lt; vcenter \\newline \\frac{(x - vcenter)}{2(vmax - vcenter)} + \\frac{1}{2} \u0026amp;\\text{if} \\quad vcenter \\le x \\le vmax \\newline +\\infty \u0026amp;\\text{if} \\quad x \u0026gt; vmax \\end{cases} $$ 其内部是用 np.interp 函数完成计算的，超出 [vmin, vmax] 范围的 x 设置为无穷大（np.inf）。\n2.4 BoundaryNorm 除了线性和对数的映射，有时我们需要的映射关系像是往一组摆在一起的框里投球。例如下图这个例子\n给出一系列边缘靠在一起的 bin（框子），原始数据落入第几个框（左闭右开区间），就对应于第几个颜色。因为这些框边缘的数值可以任意给定，所以很难用简单的函数表示。为了实现这种映射，这里引入 BoundaryNorm 类。其参数为\nBoundaryNorm(boundaries, ncolors, clip=False, extend='neither')  boundaries 为给出的这些 bin 的边缘数值，要求单调递增；ncolors 指定对应的 colormap 含有的颜色数，要求数值大于等于 nbin = len(boundaries) - 1。当 ncolors = nbin 时，映射关系为 $$ y = \\begin{cases} -1 \u0026amp;\\text{if} \\quad x \u0026lt; boundaries[0] \\newline i \u0026amp;\\text{if} \\quad boundaries[i] \\le x \u0026lt; boundaries[i+1] \\newline nbin \u0026amp;\\text{if} \\quad x \\ge boundaries[-1] \\end{cases} $$ 可以看到，落入框中的 x 会被映射到 [0, nbin - 1] 区间，而没有落入框中的 x 会映射为 -1 或 nbin。当 ncolors \u0026gt; nbin 时，程序会通过线性插值（并取整）把上面的结果再映射到 [0, ncolors - 1] 区间。所以可以有两种使用方法：\n ncolors 取 cmap.N，配合完整的 colormap。 ncolors 取 nbin，再把 colormap 采样到只含 nbin 个颜色。  个人一般会选用后者，这样能让每个 bin 与 colormap 中的颜色一一对应。\nextend 参数会增大 nbin 以改变映射结果，直观效果是使第一个和最后一个 bin 对应的颜色区别于 under 和 over 时的颜色。考虑到会使映射关系变复杂，所以我一般不会去设置，但对该效果有需求的读者可以自己试试。下面是一个简单例子\nbins = [0, 0.1, 0.5, 1.0, 5.0, 10.0] nbin = len(bins) - 1 norm = mcolors.BoundaryNorm(bins, nbin) x = [-1, 0.05, 0.2, 0.6, 2, 8, 12]  In : norm(x) Out: masked_array(data=[-1, 0, 1, 2, 3, 4, 5], mask=[False, False, False, False, False, False, False], fill_value=999999, dtype=int64)  2.5 其它归一化 除了上面介绍的四种，还存在关于中心对称线性映射的 CenteredNorm、关于零点对称对数映射的 SymLogNorm、任意幂律关系的 PowerNorm、自定义函数关系的 FuncNorm 等，这些都可以在 官方教程 里找到例子，此处就不详细介绍了。\n3 实际应用 3.1 pcolor 和 contour 的异同 对于画马赛克图的 pcolor、pcolormesh 和 imshow 函数，我们在实际使用中并不需要手动进行数据的归一化和颜色索引，只需在调用函数时通过 cmap 和 norm 参数把 colormap 和归一化的类传入即可，绘图函数会自动计算数据和颜色的对应关系。cmap 默认为 viridis，norm 默认为无参数的 Normalize。下面是例子\n# 生成测试数据. x = np.linspace(0, 10, 100) y = np.linspace(0, 10, 100) X, Y = np.meshgrid(x, y) Z = 1E3 * np.exp(-(np.abs(X - 5)**2 + np.abs(Y - 5)**2)) fig, axes = plt.subplots(1, 2, figsize=(12, 5)) # 直接给出vmin和vmax时会自动用它们创建Normalize. im = axes[0].pcolormesh( X, Y, Z, shading='nearest', cmap=cm.jet, vmin=0, vmax=1000 ) cbar = fig.colorbar(im, ax=axes[0], extend='both') axes[0].set_title('Normalize') # 若在pcolormesh中给定了norm,就不能再指定vmin和vmax了. im = axes[1].pcolormesh( X, Y, Z, shading='nearest', cmap=cm.jet, norm=mcolors.LogNorm(vmin=1E-3, vmax=1E3) ) # 使用LogNorm时,colorbar会自动选用_ColorbarLogLocator来设定刻度. cbar = fig.colorbar(im, ax=axes[1], extend='both') axes[1].set_title('LogNorm') plt.show()  可以看到 LogNorm 能让数据的颜色分布不那么集中。\n而画等高线的 contour 和 contourf 则与 pcolor 有一些细节上的差异。这两个函数多了个 levels 参数，用于指定每条等高线对应的数值。norm 默认为 Normalize(vmin=np.min(levels), np.max(levels))，若给出了 vmin 和 vmax，则优先使用我们给出的值。对于 contour，每条等高线的颜色可以表示为 cmap(norm(levels))；对于 contourf，等高线间填充的颜色可以表示为\n# 在norm不是LogNorm的情况下,layers计算为levels的中点.详请参考matplotlib.contour模块. levels = np.asarray(levels) layers = 0.5 * (levels[1:] + levels[:-1]) colors = cmap(norm(layers))  contourf 默认不会填充 levels 范围以外的颜色，如果有这方面的需求，可以用 extend 参数指定是否让超出范围的数据被填上 colormap 两端的颜色（或 set_under 和 set_over 指定的颜色）。并且当 contourf 指定了 extend 后，就不要在 colorbar 里指定了，否则会产生警告乃至错误。\n举个同时画出等高线和填色图的例子，填色设为半透明\n# 生成测试数据. x = np.linspace(0, 10, 100) y = np.linspace(0, 10, 100) X, Y = np.meshgrid(x, y) Z = (X - 5) ** 2 + (Y - 5) ** 2 # 将Z的值缩放到[0, 100]内. Z = Z / Z.max() * 100 # 设置一个简单的colormap. cmap = mcolors.ListedColormap(['blue', 'orange', 'red', 'purple']) fig, ax = plt.subplots() # contour和contourf默认使用levels的最小最大值作为vmin和vmax. levels = np.linspace(10, 60, 6) im1 = ax.contourf(X, Y, Z, levels=levels, cmap=cmap, alpha=0.5) im2 = ax.contour(X, Y, Z, levels=levels, cmap=cmap, linewidths=2) cbar = fig.colorbar(im1, ax=ax, ticks=levels) # 为等高线添加标签. ax.clabel(im2, colors='k') plt.show()  可以看到，levels 范围以外的部分直接露出了白色背景。等高线的颜色与等高线之间的填色并不完全一致，这是 levels 和 layers 之间的差异导致的。以上提到的这些参数都可以在 contour 和 contourf 函数返回的 QuadContourSet 对象的属性中找到，有兴趣的读者可以自己调试看看。\n3.2 BoundaryNorm 的应用 直接上例子\n# 生成测试数据. x = np.linspace(0, 10, 100) y = np.linspace(0, 10, 100) X, Y = np.meshgrid(x, y) Z = X ** 2 + Y ** 2 # 将Z的值缩放到[0, 100]内. Z = Z / Z.max() * 100 # 设置norm. bins = [1, 5, 10, 20, 40, 80] nbin = len(bins) - 1 norm = mcolors.BoundaryNorm(bins, nbin) # 设置cmap. cmap = cm.get_cmap('jet', nbin) cmap.set_under('white') cmap.set_over('purple') fig, axes = plt.subplots(1, 2, figsize=(12, 5)) # 使用pcolormesh. im = axes[0].pcolormesh(X, Y, Z, cmap=cmap, norm=norm, shading='nearest') cbar = fig.colorbar(im, ax=axes[0], ticks=bins, extend='both') axes[0].set_title('pcolormesh') # 使用contourf. im = axes[1].contourf( X, Y, Z, levels=bins, cmap=cmap, norm=norm, extend='both' ) cbar = fig.colorbar(im, ax=axes[1], ticks=bins) axes[1].set_title('contourf') plt.show()  在对 contourf 应用 BoundaryNorm 时，很容易联想到，等高线就相当于 bins 的边缘，等高线之间的填色正好对应于每个 bin 中的颜色，所以指定 levels=bins 是非常自然的。如果不这样做，contourf 默认会根据数据的范围，利用 MaxNLocator 自动生成 levels，此时由于 levels 与 bins 不匹配，填色就会乱套。\n3.3 红蓝 colormap 当数据表示瞬时值与长时间平均值之间的差值时，我们常用两端分别为蓝色和红色的 colormap，并将数据的负值和正值分别映射到蓝色和红色上，这样画出来的图一眼就能看出哪里偏高哪里偏低。下面分别用 TwoSlopeNorm 和 BoundaryNorm 来实现\n# 生成测试数据. X, Y = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100)) Z1 = np.exp(-X**2 - Y**2) Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2) Z = ((Z1 - Z2) * 2) # 将Z的值缩放到[-5, 10]内. Z = (Z - Z.min()) / (Z.max() - Z.min()) * 15 - 5 # 设定两种colormap和norm. cmap1 = cm.RdBu_r norm1 = mcolors.TwoSlopeNorm(vmin=-5, vcenter=0, vmax=10) bins = np.array([-5, -3, -2, -1, 1, 2, 4, 6, 8, 10]) nbin = len(bins) - 1 n_negative = np.count_nonzero(bins \u0026lt; 0) n_positive = np.count_nonzero(bins \u0026gt; 0) colors = np.vstack(( cmap1(np.linspace(0, 0.5, n_negative))[:-1], cmap1(np.linspace(0.5, 1, n_positive)) )) # 根据bins的区间数新建colormap. cmap2 = mcolors.ListedColormap(colors) norm2 = mcolors.BoundaryNorm(bins, nbin) fig, axes = plt.subplots(1, 2, figsize=(12, 5)) # TwoSlopeNorm的图. levels = np.linspace(bins.min(), bins.max(), 16) im = axes[0].contourf( X, Y, Z, levels=levels, cmap=cmap1, norm=norm1, extend='both' ) cbar = fig.colorbar(im, ax=axes[0], ticks=levels[1::2]) axes[0].set_title('TwoSlopeNorm') # BoundaryNorm的图. im = axes[1].contourf( X, Y, Z, levels=bins, cmap=cmap2, norm=norm2, extend='both' ) cbar = fig.colorbar(im, ax=axes[1], ticks=bins) axes[1].set_title('BoundaryNorm') plt.show()  如果只需要对称的线性红蓝 colormap，用 vmin 和 vmax 成相反数的 Normalize 来实现也是一个选择。\n4. 结语 自 Matplotlib 3.5 起内置的 colormap 将被移入 matplotlib.colormap 模块，从中获取的 colormap 不再是全局对象，而是可以修改的拷贝；并且 get_cmap 函数以后可能被废弃。所以本文的代码不一定长期有效，望读者注意。\n以上便是对 Matplotlib 中 colormap 的简要介绍，有错误的话烦请在评论区指出。而与 colormap 密切相关的 colorbar 的介绍请继续收看 Matplotlib 系列：colorbar 的设置。\n参考链接 参考的全是 Matplotlib 官网的教程\nCustomized Colorbars Tutorial\nCreating Colormaps in Matplotlib\nColormap Normalization\n自定义 colormap 的介绍\nBeautiful custom colormaps with Matplotlib\n","date":"2021-07-05","permalink":"https://zhajiman.github.io/post/matplotlib_colormap/","tags":["matplotlib"],"title":"Matplotlib 系列：colormap 的设置"},{"content":"本博客之前是用软件 Gridea 制作的，这是个静态博客写作客户端，可以作为 Markdown 编辑器，同时简单设置下就能一键生成静态页面并上传到网上，非常适合我这种电脑小白使用。不过前段时间发现怎么都没法上传本地写好的内容，于是决定重新用现在流行的 Hugo 来搭建博客。本文使用的是 0.84.4 版本的 Hugo 和 2.32.0 版本的 Git。\nHugo 的安装 Hugo 是一个由 Go 语言实现的静态网站生成器，因为听说使用起来比较简单，并且主题也多，所以选了它。二进制安装包可以直接在其 Github Releases 页面中下载到，我选择的是 hugo_extended_0.84.4_Windows-64bit.zip。新建一个目录 bin，将安装包里解压出来的东西都丢进去，然后把 bin 目录的路径添加到环境变量中，安装就完事了。以后直接在命令行中调用命令即可。\nHugo 的基本用法 新建网站 在当前目录下新建网站\nhugo new site ./ZhaJiMan.github.io\r 这样当前目录下会生成一个名为 ZhaJiMan.github.io 的网站目录，其结构为\n.\r├── archetypes # 存放文章模板\r├── config.toml # 简单的配置文件\r├── content # 存放文章\r├── data # 存放生成静态页面时的配置文件\r├── layouts # 存放页面布局的模板\r├── static # 存放图片等静态内容\r└── themes # 存放下载的主题\r 之后的所有操作需要 cd 到这个目录下进行。\n添加主题 主题可以在 Hugo Themes 网站上找到，我选择的是自带 TOC 和评论功能的 Fuji，通过 Git 命令安装。\ngit init\rgit submodule add https://github.com/WingLim/hugo-tania themes/hugo-tania\r 然后主题就会下载到 themes 目录中。一般主题的目录里都会含有一个 exampleSite 目录，顾名思义这是作者做好的示例网站，直接把里面的内容复制到网站根目录下，就能完成该主题最基本的配置，并实现示例网站的效果。之后修改根目录下的 config.toml 文件来自定义配置。\n创建文章 Hugo 中的文章都以 Markdown 格式写作。在 content/post 目录下新建一个 Markdown 文件\nhugo new post/rebuild_blog.md\r 默认的文章模板会使 Markdown 文件带有这样的开头\n---\rtitle: \u0026quot;rebuild_blog\u0026quot;\rdate: 2021-07-03T16:47:34+08:00\rdraft: true\r---\r --- 之间的内容服从 YAML 或 TOML 格式。title 即文章标题，默认与文件名相同；date 即日期时间；draft 表示该文章是否为草稿，如果是，那么后面生成静态页面时将不会含有该文章。此外还存在别的参数可供设置。--- 之后的内容自然就是文章正文了。\nFuji 主题还额外强调要在正文中插入简介分割线 \u0026lt;!--more--\u0026gt;，以让文章列表的文章预览部分样式正确。\n预览网站 建立一个本地服务器\nhugo server\r 然后命令行会卡住，在浏览器内输入 http://localhost:1313/ 预览网站，命令行内 Ctrl+C 关闭服务器。Hugo 的一个特色是可以进行动态预览，当你修改本地内容时，变化会马上反映在浏览器中的页面上。\n生成静态页面 直接在生成在默认的 public 目录下\nhugo\r 用 -d 参数可以指定目录，或者在配置文件里用 publishDir 参数指定默认的目录。\n发布到 Github 上 这里用 Github Pages 来部署博客。首先在 config.yaml 里指定\npublishDir: docs\r 然后再一个 hugo 命令，这样就把静态页面输出到 docs 目录下了。\n接着在 Github 上以 ZhaJiMan.github.io 的名字（根据自己的用户名而定）新建一个空仓库，进行下面的 Git 命令\ngit add .\rgit commit -m \u0026quot;first commit\u0026quot;\rgit branch -M main\rgit remote add origin https://github.com/ZhaJiMan/ZhaJiMan.github.io.git\rgit push -u origin main\r 这段改编自空仓库页面出现的提示，大意是\n 将网站目录下的所有内容暂存。 把暂存的内容提交给版本库。 把主分支的名字从 master 改为 main。 添加远程仓库。 把本地内容推送到远程仓库里。  推送成功后，进入仓库的设置页面，点击侧栏的 Pages，再把 Source 选项改为 main 分支下的 docs 目录，这样 Github Pages 就会根据我们推送上去的 docs 目录里的静态页面来显示网站。这里指定 docs 的好处是还可以把网站的所有文件都备份到仓库里（不包含以 submodule 形式添加主题，详见参考链接）。最后在与仓库同名的网站 https://zhajiman.github.io/ 上看看自己的博客吧！\n工作流 总结一下上面的流程\n 用 Markdown 写作。 用 hugo server 本地预览。 用 hugo 生成静态页面。 用 Git 的 add、commit 和 push 命令推送到网上。  其它功能 插入图片 以名为 capslock.jpg 的图片为例，将该图片放入 static 目录下，再在 Markdown 文件中以 /capslock.jpg 的路径引用即可。路径之所以写成这个形式，是因为 Hugo 会自动在图片路径前追加 static 的路径。为了区分开不同文章的用图，还可以在 static 下新建子目录，例如下面的写法\n![capslock](/rebuild_blog/capslock.jpg)\r 其实这种隐式的路径在上一节中也频繁出现过。虽然 Hugo 可以解析这种路径，但 Markdown 编辑器不能，所以在编辑器的预览中会看不到图片。\n渲染公式 Fuji 主题支持用 KaTex 渲染公式，使用方法为在文章开头或配置文件中添加 math: true 或 katex: true。使用过程中发现，KaTex 不能正常渲染行内公式，参考 KaTex 官网 Auto-render Extension 的例子，将 themes/fuji/layouts/partials/math.html 中的 KaTex 调用换成\n\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css\u0026quot; integrity=\u0026quot;sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc\u0026quot; crossorigin=\u0026quot;anonymous\u0026quot;\u0026gt;\r\u0026lt;script defer src=\u0026quot;https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js\u0026quot; integrity=\u0026quot;sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp\u0026quot; crossorigin=\u0026quot;anonymous\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;script defer src=\u0026quot;https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js\u0026quot; integrity=\u0026quot;sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl\u0026quot; crossorigin=\u0026quot;anonymous\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;script\u0026gt;\rdocument.addEventListener(\u0026quot;DOMContentLoaded\u0026quot;, function() {\rrenderMathInElement(document.body, {\rdelimiters: [\r{left: '$$', right: '$$', display: true},\r{left: '$', right: '$', display: false},\r{left: '\\\\(', right: '\\\\)', display: false},\r{left: '\\\\[', right: '\\\\]', display: true}\r],\rthrowOnError : false\r});\r});\r\u0026lt;/script\u0026gt;\r 这样行间公式与行内公式就都可以正常渲染。原理似乎是在函数 renderMathInElement 中指定识别公式的分隔符，不过具体细节我也不懂。本文便采用 KaTex 进行渲染，例如行内公式为 $e^{ix} = \\cos{x} + i\\sin{x}$，行间公式为 $$ P_e(\\omega) = \\frac{\\hbar \\omega^3}{4\\pi^2 c^2} \\frac{1}{\\exp{(\\hbar \\omega / k_B T)} - 1} $$\n评论系统 Fuji 主题支持 Disqus、utterances 和 DisqusJS 三种评论系统，并且设置起来非常简单。这里采用依托于 Github issues 的 utterances。进入 https://utteranc.es/，按指示把 utterances app 安装到存储博客的仓库，然后在 config.toml 中设置\nutterancesRepo = \u0026quot;ZhaJiMan/ZhaJiMan.github.io\u0026quot; # 格式为username/username/github.io\rutterancesIssueTerm = \u0026quot;pathname\u0026quot; # 以页面的pathname来生成issues\r 文章最下面就会出现评论区了，用 Github 账号登录即可发送评论。\n设置网站图标 依据 Fuji 主页的说明，把自己喜欢的图片上传到 https://realfavicongenerator.net/ 上，再把打包好的图标压缩包下载下来，解压到 static 目录中，接着把该网站提供的 HTML 代码粘贴到 layouts/partials/favicon.html 文件中，并修改一下 href 属性指向的路径即可。\n显示点击量 这里使用 不蒜子 实现统计，需要在主题的 HTML 文件中添加脚本，再用标签显示计数，用法请见其官网。具体到 Fuji 主题，首先在 themes/fuji/layouts/partials/head.html 文件中添加不蒜子的脚本\n\u0026lt;!-- 不蒜子脚本 --\u0026gt;\r\u0026lt;script async src=\u0026quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r 接着在 themes/fuji/layouts/_default/single.html 文件的第八行、post-meta（文章元数据）的块中加入标签\n\u0026lt;!-- 显示文章点击量 --\u0026gt;\r\u0026lt;span\u0026gt;\u0026lt;i class=\u0026quot;iconfont icon-time-sharp\u0026quot;\u0026gt;\u0026lt;/i\u0026gt;\u0026amp;nbsp;\u0026lt;span id=\u0026quot;busuanzi_value_page_pv\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;\u0026amp;nbsp;views\u0026lt;/span\u0026gt;\r single.html 控制文章页面的显示，把标签加到 post-meta 块中能让计数显示在文章标题下面一行处。其中 \u0026lt;i class=\u0026quot;iconfont icon-time-sharp\u0026quot;\u0026gt; 是我在主题的 post_meta.html 中抄来的，能指定元数据的图标和字体。不蒜子的标签采用了官网提到的极简模式。\n最后考虑在博客的页脚加上总访问量的计数。在 themes/fuji/layouts/partials/footer.html 文件的 footer 块中加入\n\u0026lt;!-- 显示网站访问量 --\u0026gt;\rVisits: \u0026lt;span id=\u0026quot;busuanzi_value_site_pv\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;\r 摆放标签的位置都是我胡乱试出来的，在不同主题下的做法各不相同。\n修改样式 依据 Fuji 主页的说明，利用 assets/scss/_custom_var.scss 文件修改 SCSS 变量（例如换颜色、换字体），利用 assets/scss/_custom_rule.scss 文件改写 SCSS 规则。\n别人的博客 最后放两个别人用 Hugo + Fuji 搭的博客\nhttps://marcoscheel.de/post/2020/10/20201011-my-blog-has-moved/\nhttps://masatakashiwagi.github.io/portfolio/post/hugo-portfolio/\n参考链接 如何使用Hugo在GitHub Pages上搭建免费个人网站\n生物信息基础：实用Git命令，掌握这些就够了\nhugo 导入图片，两种方式\nsingle or double dollar sign as formula delimiter\nGit中submodule的使用\nhugo建站 | 我的第一个博客网站\n","date":"2021-07-03","permalink":"https://zhajiman.github.io/post/rebuild_blog/","tags":["hugo","github"],"title":"用 Hugo 重新搭建博客"},{"content":"最近又碰到了给出地球上两点的经纬度，然后计算它们之间距离的问题。之前曾经通过查维基写过简单的博文，不过现在实践时发现博文里问题较多，故重撰如下。\n地球的形状 为了计算地球上两点之间的距离，首先需要对地球的形状有个概念，以定义距离的几何表示。我们的一般常识是：地球是一个赤道方向略长、两极方向略短的椭球，且表面有着不规则起伏的地形。这种形状肯定无法直接计算，所以希望能简化为一个能用简单数学式子描述的形状。下面是一个简单且夸张的图示\n黑线表示地球的固体表面，因为地形起伏而显得不规则。不过这里只是夸张的画法，地形落差相对于地球半径而言其实微乎其微。黑线以上的蓝线是海平面，假设作为重力位能等势面的海平面能延伸到大陆内部，那么真实的海平面可以和假想的海平面共同构成一个封闭曲面，称为大地水准面（geoid）。由于地球内部质量分布不均，不同方向上重力有差异，所以大地水准面也会有些不规则。为了进一步简化，考虑用一个旋转椭球体去拟合大地水准面，拟合结果即为地球椭球体（earth ellipsoid）。因为地球椭球体可以用简单的数学式子描述，所以非常便于计算经纬度和海拔。对拟合效果的不同定义能导致不同的地球椭球体，例如考虑全球的拟合效果，常用 WGS84 坐标系（图中红线）；而考虑区域的拟合效果时，会设计局地的坐标系（图中绿线）。\n如果还想偷懒，可以进一步把旋转椭球体简化为球体。例如，在 WGS84 坐标系中，赤道方向半径 $a = 6378.1370\\ \\rm{km}$，两极方向半径 $b = 6356.7523\\ \\rm{km}$，椭球扁率 $f$ 为 $$ f = \\frac{a-b}{a} \\approx 0.003 $$ 因为扁率足够低，所以可以进一步近似为球体。图示如下\nWGS 标准定义地球的平均半径 $R$ 为 $$ R = \\frac{2a+b}{3} \\approx 6371\\ \\rm{km} $$ Wiki 上说当球体取这一半径时能减小球体与椭球体在估计两点间距离时的误差，具体来源有待查证。总之，利用上述简化的形状模型，便能着手计算地球上两点间的距离。下面先从最简单的球体开始介绍。\n球体上两点间的距离 假设地球是一个 $R = 6371\\ \\rm{km}$ 的球体，如下图所示\n球坐标系中以北极方向为 $z$ 轴，赤道平面为 $xy$ 平面，球面上一点 $P$ 的经度和纬度分别为 $\\lambda$ 和 $\\phi$，取值范围为 $$ \\lambda \\in [-180^\\circ, 180^\\circ] \\quad \\text{or} \\quad [0^\\circ, 360^\\circ] $$\n$$ \\phi \\in [-90^\\circ,90^\\circ] $$\n球面上两点间的距离指的是两点间长度最短的弧线，由于这一弧线肯定位于两点所在的大圆上，所以又称作大圆距离（great-circle distance）。首先来讨论一下经线和纬线上两点间的距离。\n同一经线上的两点经度相同，纬度相差 $\\Delta \\phi$。由于经线都是半个大圆弧，所以两点间的距离直接由弧长公式得到 $$ \\Delta d = R \\Delta \\phi $$ 容易看出，经线上纬度每相差 1°，距离相差约 111 km。\n同一纬线上的两点纬度相同，经度相差 $\\Delta \\lambda$。与经线不同，纬线（或者说纬圈）是球面上的小圆，计算距离差时，首先把球的半径 $R$ 乘上 $\\cos \\phi$ 转化为小圆半径，再套用弧长公式 $$ \\Delta d = R \\cos \\phi \\Delta \\lambda $$ 容易看出，赤道纬线上经度相差 1° 时，距离差依旧是 111 km 但纬度越高，这一距离越小。例如对于纬度 40°N 的北京，纬线上 1° 仅相当于 85 km。越靠近两个极点，距离就越接近 0。经线和纬线上的距离公式可以用下面这张图来总结\n需要注意，公式中的 $\\Delta \\phi$ 和 $\\Delta \\lambda$ 在计算时需要转换为弧度单位。通常而言，在中低纬度地区为了方便，可以说 $1^\\circ \\approx 111\\ \\rm{km}$。\n说完经线和纬线上的距离，接着拓展为球体上任意两点间的距离。设球面上有两个点 $P(\\lambda_1, \\phi_1)$ 和 $Q(\\lambda_2, \\phi_2)$，这两点间的距离即过 $P$ 点和 $Q$ 点的大圆上的弧 $PQ$ 的长度 $\\Delta d$，如下图所示\n显然弧长 $\\Delta d$ 与弧 $PQ$ 的圆心角 $\\Delta \\sigma$ 满足 $$ \\Delta d = R \\Delta \\sigma $$ 所以计算距离的问题转化为如何计算圆心角 $\\Delta \\sigma$。由球面三角学中的球面余弦定理 $$ \\Delta \\sigma = \\arccos (\\sin \\phi_1 \\sin \\phi_2 + \\cos \\phi_1 \\cos \\phi_2 \\cos (\\Delta \\lambda)) $$ 其中 $\\Delta \\lambda$ 的正负不影响结果（后面将会出现的 $\\Delta \\phi$ 也是），最后得到的 $\\Delta \\sigma$ 的范围是 $[0,\\pi]$。然而当两个点特别靠近时，$\\arccos$ 括号内的值接近于 1，而 $\\arccos$ 函数在这一点附近的变化率较大，计算时的舍入误差会因此变大。所以另外会使用数值上更加稳定的 haversine 公式。首先定义半正矢函数 $$ \\rm{hav}\\ \\theta = \\sin^2 \\frac{\\theta}{2} $$ 带入到球面余弦定理的公式中，易得 $$ \\begin{align} \\Delta \\sigma \u0026amp;= \\rm{archav}(\\rm{hav}(\\Delta \\phi) + \\cos \\phi_1 \\cos \\phi_2 \\rm{hav}(\\Delta \\lambda)) \\newline \u0026amp;= 2 \\arcsin \\sqrt{\\sin^2(\\frac{\\Delta \\phi}{2}) + \\cos \\phi_1 \\cos \\phi_2 \\sin^2(\\frac{\\Delta \\lambda}{2})} \\end{align} $$ 当两个点特别靠近时，$\\arcsin$ 后面的值接近于 0，而 $\\arcsin$ 函数在这一点附近的变化率较小，所以会比直接用球面余弦公式来得更精确。不过也可以反过来推测，haversine 公式在两点相对的情况下（例如南北两极）误差会变大。下面就此进行测试，给出赤道上两点的经纬度，用这两个公式分别计算圆心角，比较它们与理论值之间的差异。\nimport numpy as np import matplotlib.pyplot as plt import matplotlib.ticker as mticker from scipy.ndimage import gaussian_filter1d def cosine(lon1, lat1, lon2, lat2): '''利用球面余弦公式计算两点间的圆心角.''' lon1, lat1, lon2, lat2 = map(np.deg2rad, [lon1, lat1, lon2, lat2]) dlon = lon2 - lon1 a = np.sin(lat1) * np.sin(lat2) b = np.cos(lat1) * np.cos(lat2) * np.cos(dlon) dtheta = np.arccos(a + b) return np.rad2deg(dtheta) def hav(x): '''计算半正矢函数.''' return np.sin(x / 2)**2 def haversine(lon1, lat1, lon2, lat2): '''利用haversine公式计算两点间的圆心角.''' lon1, lat1, lon2, lat2 = map(np.deg2rad, [lon1, lat1, lon2, lat2]) dlon = lon2 - lon1 dlat = lat2 - lat1 a = hav(dlat) b = np.cos(lat1) * np.cos(lat2) * hav(dlon) dtheta = 2 * np.arcsin(np.sqrt(a + b)) return np.rad2deg(dtheta) if __name__ == '__main__': npt = 10000 # 点1的经度为-180°,点2的经度范围为[-180°, 180°]. lon1 = np.full(npt, -180) lat1 = np.zeros(npt) lon2 = np.linspace(-180, 180, npt + 2)[1:-1] # 避开0°夹角. lat2 = np.zeros(npt) # 计算理论的圆心角,和两种公式导出的圆心角. dlon = np.abs(lon2 - lon1) deg_tru = np.where(dlon \u0026gt; 180, 360 - dlon, dlon) deg_cos = cosine(lon1, lat1, lon2, lat2) deg_hav = haversine(lon1, lat1, lon2, lat2) # 计算与理论值之间的误差. err_cos = np.abs(deg_cos - deg_tru) / deg_tru * 100 err_hav = np.abs(deg_hav - deg_tru) / deg_tru * 100 # 对结果进行平滑. err_cos = gaussian_filter1d(err_cos, sigma=3) err_hav = gaussian_filter1d(err_hav, sigma=3) # 画图. fig, ax = plt.subplots() ax.plot(dlon, err_cos, lw=1, label='cosine') ax.plot(dlon, err_hav, lw=1, label='haversine') leg = ax.legend(frameon=False) for line in leg.get_lines(): line.set_linewidth(2) # 设置x轴. ax.set_xlabel('Longitude Difference (°)', fontsize='large') ax.set_xlim(-10, 370) ax.xaxis.set_major_locator(mticker.MultipleLocator(60)) ax.xaxis.set_minor_locator(mticker.AutoMinorLocator(2)) # y轴采用对数坐标. ax.set_ylabel('Deviation (%)', fontsize='large') ax.set_yscale('log') ax.set_ylim(1E-13, None) ax.grid(ls='--') plt.show()  当两点间夹角接近于 0° 时，如图中最左边和最右边所示，球面余弦公式的误差大于 haversine 公式；当两点近乎相对时，如图正中间所示，haversine 公式的误差大于球面余弦公式。一个奇怪的地方是，haversine 公式在两点间夹角趋于 0° 时的误差还要略大于两点相对的情况，我也想不出原因，也许跟计算 $\\Delta \\phi$ 时产生的舍入误差有关？不知道有没有读者能予以解答。但总地看来，在 64 位浮点精度下这两个公式的误差完全可以忽略，实际使用时任选其一即可。如果想要现成的 haversine 公式实现，可以调用 scikit-learn 包里的 sklearn.metrics.pairwise.haversine_distances 函数。\n此外维基上还提到了一个所谓球体情况下的 Vincenty 公式，声称这个公式对于任意位置的两点都精确。但我测试后发现结果比较离谱，计算出了负的圆心角，并且也可以通过数学证明这个公式是错的，所以请读者小心引用。\n椭球上两点间的距离 比球体近似更精确的是椭球近似，椭球上两点距离的计算一般采用 Vincenty 公式，这是一种精度很高的迭代法，具体分为两种\n direct method：已知一点的坐标，给出朝向另一点的距离和方位角，用公式计算出另一点的坐标。 inverse method：已知两点的坐标，用公式计算出两点间的距离和方位角。  显然我们这里需要的是 inverse method，即根据两点坐标逆向求解它们之间的距离。维基上的相关公式还有点复杂，我也不懂具体原理，所以这里就直接调包了。Python 中的 pyproj 包提供对地理坐标的变换操作，其中 Geod 类可以生成一个代表地球椭球体的对象，利用其 inv 方法即可实现 inverse method\nfrom pyproj import Geod # 海口的经纬度. lon1, lat1 = 110.33, 20.07 # 北京的经纬度. lon2, lat2 = 116.40, 39.91 # 生成一个球体,默认半径R=6370997.0m g1 = Geod(ellps='sphere') # 生成一个WGS84坐标系下的椭球. g2 = Geod(ellps='WGS84') # 计算WGS84椭球上两点之间的方位角和距离,默认经纬度单位为degree. az12, az21, dist = g2.inv(lon1, lat1, lon2, lat2)  例如上面计算出海口到北京的距离为 2274.54 km，而球面余弦公式和 haversine 公式对这个结果的误差为 0.27 %。光看数字可能不太形象，那假设高铁时速为 250 km/h，再考虑途中有弯弯绕绕，这个距离需要坐上十几个小时的高铁。不过如第一节所述，计算出的距离会根据我们选取的地球椭球体的变化而发生变化，并且很难说哪个椭球的结果就更精确——它们都是对大地水准面的有效近似，只不过在不同区域的表现不同罢了。\nPython 中的 GeoPy 包也提供类似的距离计算功能，有兴趣的读者可以试试看。\n参考资料 Wikipedia: Great-circle distance\n知乎：如何区分测量学中的大地水准面、大地基准、似大地水准面、地球椭球等概念？\npyproj.Geod\n","date":"2021-06-05","permalink":"https://zhajiman.github.io/post/distance_on_earth/","tags":["测地学"],"title":"地球上两点之间的距离（改）"},{"content":"前言 昨天一同学问我怎么把已经画好的图片的 DPI 改到 300，以满足期刊对图片清晰度的要求。上网搜索一番后才发现，虽然我经常在 Matplotlib 中用 dpi 参数来调节图片清晰度，但实际上我对这个概念半懂不懂。这次借这个契机调研和总结一下相关的知识。本文将会依次介绍\n 分辨率和 DPI 是什么。 DPI 和清晰度的关系。 如何导出期刊要求的高 DPI 图片。  分辨率 这里的图片指的是位图（bitmap），一张图片由无数个彩色的小像素点组成，Matplotlib 支持的位图格式有 png、jpg、jpeg、png、tiff 等。我们常用分辨率（resolution）来描述图片的大小，例如说一张图片的分辨率是 800 x 400，即指这张图片宽 800 个像素，高 400 个像素。Windows 对一张 jpg 图片打开右键菜单，在“属性”里的“详细信息”里就能看到图片的分辨率，如下图所示\n在其它领域里分辨率一词通常描述仪器分辨细节的精细程度，而图片的分辨率仅仅是指图片大小，所以对于图片大小来说，一个更准确的术语是 pixel dimensions。不过既然 Windows 的菜单里都这么显示了，那后文将继续沿用分辨率的说法。\n尺寸 除了用像素数，图片的尺寸还可以用物理单位来描述，用来指定打印时图片在纸上的大小。例如对于一张分辨率为 800 x 400 的图片，我们希望维持原宽高比打印出来，那么可以设定其尺寸为宽 8 英寸，高 4 英寸（1 英寸约为 2.54 厘米）。这个尺寸可以任意设定，毕竟想打印多大完全由你决定。\nDPI 和 PPI 如果说分辨率和尺寸是长度量的话，那么 DPI 和 PPI 就是密度量。它们的定义如下\n  DPI（dots per inch）：每英寸长度里含有的打印机墨点数。\n  PPI（pixels per inch）：每英寸长度里含有的像素数。\n  DPI 表现的是打印机的精细程度。对于同样大小的纸张，打印机的 DPI 更高，打印时就会用上更多墨点，那么打印效果自然也更好。\n电子设备借鉴了打印设备里 DPI 的概念，用 PPI 来衡量像素点的物理尺寸。PPI 对于显示器和图片的意义稍有不同，下面来分别介绍。首先，显示器的 PPI 计算公式为\nPPI = 对角线像素数 / 对角线物理长度  给定屏幕大小，PPI 更高则屏幕含有的像素数更多，那么显示效果会更好。例如苹果的 iPhone 就强调其 Retina 屏幕的像素密度高达 326 PPI，有着超出人眼识别能力的细腻效果（广告语看看就得了）。\n对图片来说，PPI 和 DPI 这两个术语经常混淆使用，例如 Windows 菜单就称呼图片单位英寸的像素数为 DPI，那么后文也会沿用这一说法。图片 DPI 的计算方法是\n水平DPI = 宽度像素数 / 物理宽度 垂直DPI = 高度像素数 / 物理高度  可以看出，DPI 就是将图片从像素尺寸缩放到物理尺寸的比值。另外，DPI 的倒数即每个像素的单位物理长度，因为我们总是希望像素的物理形状是正方形，所以大多数情况下水平 DPI 就等于垂直 DPI，这样打印出来的图片也能维持原有的宽高比。\n一些图片格式会记录图片的 DPI 值，Windows 下图片的右键菜单属性栏里便能看到。我们在对图片进行排版或打印时，软件会根据图片的分辨率和 DPI 自动设定图片的纸上尺寸。不过如果你想把图片打印大点，那么根据定义计算，图片 DPI 会变小；想打印小点，图片 DPI 就会变大——没错，DPI 并不是图片的固有属性，真正决定 DPI 的是图片分辨率和你想要的纸上尺寸，右键菜单属性栏里的数值只是个参考。这一点还可以从两个例子说明，一是 png 格式压根不含 DPI 值，你得根据打印需求自己去算；二是可以用 Pillow 库直接修改图片的 DPI 值\nfrom PIL import Image # test1.tif的原始DPI为50 img = Image.open('test1.tif') img.save('test2.tif', dpi=(300, 300), compression=None)  用上面的代码可以把一张特别糊的图片改成 300 DPI 的“出版级”图片，然而图片清晰度和体积一点没变，依旧说明图片元信息（metadata）里的 DPI 值只是个摆设。\nDPI 与清晰度 我们可能听过 DPI 越高越清晰的说法，这里需要明确，DPI 是打印机、显示器，还是图片的 DPI？清晰是指什么东西清晰？\n原则上打印机的 DPI 越高，打印出的纸质图片越清晰；显示器的 PPI 越高，显示效果越好。对图片则要分情况讨论。如果给定图片分辨率，DPI 越高，打印出来的纸质图片越小，虽然越小越不容易看出瑕疵，但那也不能说成是打印效果更好。如果给定纸上尺寸，DPI 越高，图片的像素数越多，于是问题转化成了：图片像素越多，就会越清晰吗？\n答案是不一定，示意图如下（转自知乎专栏 影响图像画质的因素：图片的分辨率和像素浅谈）\n每一排从右往左，采样分辨率从 50 x 50 降至 1 x 1，清晰度显著下降，说明像素越多越清晰；但第一排到第二排将分辨率用 PS 放大到 10 倍，清晰度并没有显著提高，只是像加了柔和滤镜一样。就我个人的理解，只有在从源头生成图片的过程中才有像素越多越清晰的规律，例如拍照时采样了更多像素点、画画时用更多像素描绘细节等；如果只是对图片进行后处理来增多像素的话就不一定能更清晰，例如各种插值方法。\n回到前面的问题，给定纸上尺寸时，DPI 越高图片像素数越多，说明图片本身很可能会更清晰，那么在不超出打印机 DPI 水平的前提下，打印出来的纸质图片也很可能更清晰。\n期刊的 300 DPI 要求 由上一节的讨论，我们便能理解期刊为什么对配图的 DPI 有要求了，因为高 DPI 预示着配图在杂志上的显示效果应该会很好（无论是纸质版还是电子版）。下面以 AGU（美国地球物理学会）对位图的要求为例，用 Matplotlib 演示导出高 DPI 图片的方法。\n要求 tif 和 jpg 格式的图片在期刊的纸面尺寸上有 300 - 600 的 DPI，tif 图采用 LZW 压缩，jpg 图选择最高的 quality。1/4 版面大小的图片尺寸是 95 x 115 mm。程序如下\nimport matplotlib.pyplot as plt w = 95 / 10 / 2.54 h = 115 / 10 / 2.54 fig = plt.figure(figsize=(w, h)) fig.savefig('output.tif', dpi=600, pil_kwargs={'compression': 'tiff_lzw'}) fig.savefig('output.jpg', dpi=600, pil_kwargs={'quality': 95})  plt.figure 函数的 figsize 参数要求单位为英寸，所以要先把版面尺寸的单位从毫米换算到英寸。fig.savefig 方法里可以直接指定 DPI，压缩方法这种与图片格式相关的参数需要传给 PIL 来实现。最后能得到两张分辨率为 2244 x 2716，600 DPI 的图片。需要注意如果 dpi 参数的值太高，生成的图片的分辨率和体积太大。\n在 Matplotlib 中，给定 figsize，dpi 越大，绘制同一个元素时会用到更多像素，所以最后导出的图片会更清晰。此即前面提过的从源头上生成清晰的图片。而后处理增加 DPI 的方法也有：导入 PS 中插值放大；粘贴到 PPT 修改 slide 的分辨率和 DPI，再导出整张 slide；用 AI 把位图转换成矢量图等。后处理方法的问题在于，如果处理前图片就很糊，那么处理后只能得到高 DPI 的假高清图。\n当然，最最简单的方式是，从一开始就不要画位图，全部以矢量图的格式导出（eps、pdf 等），这样就完全没有本文中的问题了，所以本文白写了（悲）。\n额外说明 额外说明一点搜到的实用小知识。\nMatplotlib 中的线宽和字体字号是以磅（point）为单位的，有\n1 pt = 1/72 inch  例如，linewidth=72 时，线宽恰好为 1 英寸。注意这是个物理单位，对应于纸上长度。所以增大 figsize 时图中元素会显得更小更细，而增大 dpi 时图中元素大小不变，但图片像素更多、显示效果更清晰。\n参考资料 Dots per inch - Wikipedia\nRelationship between dpi and figure size\nHow to ensure your images meet the minimum requirement for printing - DPI explained\nGRAPHIC REQUIREMENTS - AGU\nDPI 和 PPI 的区别是什么？ 照片的分辨率300dpi那么它的水平分辨率和垂直分辨率分别是多少？\n","date":"2021-04-08","permalink":"https://zhajiman.github.io/post/matplotlib_dpi/","tags":["matplotlib"],"title":"Matplotlib 系列：导出高 DPI 的图片"},{"content":"前言 之前在 Linux 上用 Python 处理系统的文件和目录时，我都是简单粗暴地用 os.system 函数直接执行 shell 命令来实现的。例如新建一个目录并把文件移动进去，我会这么写\ndirpath = './result'\rfilepath = './data.txt'\ros.system(f'mkdir {dirpath}')\ros.system(f'mv {filepath} {dirpath}')\r 即把 shell 命令硬编码到程序中。但最近在 Windows 上运行老程序时，因为 os.system 默认调用 CMD，所以这种写法的老代码全部木大。\n其实借助 Python 标准库中用于系统交互和路径处理的模块，就能尽可能降低代码对平台的依赖，并且模块中也提供有许多方便的函数。本文会记录那些最常用的功能。\n基础知识 首先明确一些基础知识，以免后面发生混淆。目录（directory）即我们常说的文件夹，能够存放文件和其它目录。而路径（path）是用于标识文件和目录在文件系统中具体位置的字符串，路径的末尾是文件或者目录的名字，而前面则是一级一级的父目录，每一项通过路径分隔符隔开。\nLinux 和 Mac 的路径分隔符是正斜杠 /，而 Windows 用的是反斜杠 \\。在 Python 的字符串中，因为反斜杠还有转义的作用，所以要么用 \\\\ 表示一个反斜杠 ，要么使用 raw 字符串（不过以反斜杠结尾时会引起语法解析的错误）。例如\n# Linux下的路径\rdirpath = './a/b/c'\r# Windows下的路径\rdirpath1 = './/a//b//c'\rdirpath2 = r'./a/b/c'\r 注意虽然程序中字面值是 \\\\，但打印或输出时是正常的 \\。其实现在的 Windows 内核兼容正斜杠的写法，在 Python 程序中我们完全可以只使用正斜杠（甚至混用都没问题）。\n下面再来谈一谈目录的路径结尾是否该加上斜杠的问题。有些人习惯在目录的路径结尾再添上一个斜杠，以显示这个路径表示的是一个目录而不是文件，并且之后在进行字符串连接时也不必手动插入斜杠。在绝大多数情况下，加或不加并不会影响到命令行的行为。\n考虑到 Python 中许多函数在处理路径时会自动去掉结尾的斜杠，以免影响路径的分割（os.path.basename、os.path.dirname 等函数），本文中不会在结尾加上斜杠。\nos 这个模块提供一些与操作系统进行交互的函数，例如创建和删除目录等。\nos.sep：属性，值是系统所用的路径分隔符的字符串。\nos.getcwd：获取工作目录的路径。\nos.chdir：切换工作目录，功能同 shell 中的 cd 命令。\nos.listdir：返回指定的目录（默认是工作目录）下所有文件和目录的名字组成的列表。注意列表元素的顺序是任意的（尽管我们的运行结果可能是有序的）。\nos.walk：自上而下遍历一棵目录树，每到一个目录时 yield 一个 (dirpath, dirnames, filenames) 的三元组。其中 dirpath 是该目录的路径，dirnames 是该目录下子目录名字组成的列表，filenames 是该目录下文件名组成的列表。下面举个找出目录下所有文件的例子\ndef get_all_filepath(dirpath):\rfor dirpath, dirnames, filenames in os.walk(dirpath):\rfor filename in filenames:\ryield os.path.join(dirpath, filename)\r os.mkdir：创建一个目录。\nos.makedirs：递归地创建一个目录，即就算我们给出的路径中含有尚不存在的目录，系统也能顺便给创建了。\nos.rmdir：删除一个空目录，如果目录非空则会报错。\nos.removedirs：递归地删除空目录。即根据路径从右往左逐个删，碰到非空的目录时就会停下（不然那不得把你根目录给端了）。\nos.remove：删除一个文件。如果路径指向目录的话会报错。\nos.rename：给文件或目录重命名。如果重命名到另一个目录下面，就相当于剪切。当目标路径已经存在时，会有比较复杂的行为，建议不要这么做。\nos.replace：相当于 os.rename，但当目标路径指向已经存在的目录时会报错，指向文件时则会直接替换。\nos 模块中关于文件和目录的常用函数差不多就这些。你可能会问，怎么删除目录的函数都只能作用于空目录，那非空的目录怎么办？这就需要用到更高级的文件操作库——shutil。\nshutil 这个模块提供正经的文件/目录的复制、剪切、删除操作。\nshutil.copyfile：复制文件，要求两个参数都为文件路径。\nshutil.copy：同样是复制文件，但目标路径可以为目录，这样相当于保持文件名不变复制过去。\nshutil.copytree：顾名思义，直接复制一整棵目录树，即复制非空的目录。\nshutil.rmtree：删除一整棵目录树。\nshutil.move：将文件或非空目录移动到目标目录下面。\nglob 这个模块的功能非常单纯：提供 Unix shell 风格的路径搜索。即可以用通配符实现灵活的匹配，又能直接拿到文件和目录的路径，方便操作。\nglob.glob：给出含通配符的路径，将与之匹配的路径汇集成列表返回。因为这个函数内部是由 os.listdir 实现的，所以也不能保证结果的顺序。Python 3.5 以后提供 recursive 选项，指定是否进行递归搜索，用 ** 匹配目录下的所有内容。\n一些例子如下\n# 得到路径dirpath下的文件和目录的路径\rglob.glob(os.path.join(dirpath, '*'))\r# 得到路径dirpath下所有py文件的路径\rglob.glob(os.path.join(dirpath, '**', '*.py'), recursive=True)\r 如果给出的路径是相对路径，那么结果也会是相对路径，绝对路径同理。\n如果希望搜索的结果有序排列，可以用列表的 sort 方法或 sorted 函数进行排序。下面举个搜索路径下所有图片，并按文件名排序的例子\ndirpath = './pics'\rfilepaths = glob.glob(os.path.join(dirpath, '*.png'))\rfilepaths.sort(key=lambda x: os.path.basename(x))\r 如果需要节省内存，glob 模块还提供返回生成器的 glob.iglob 函数。\nos.path 这个模块提供许多处理路径的函数，其实在前面的例子中已经出现过好几次了。\nos.path.normpath：将路径规范化。能将多余的分隔符去掉，例如 A//B 、A/B/ 和 A/./B 都会变成 A/B。可以看出，结尾有斜杠对于 Python 来说是不“规范”的。Windows 系统下还会将路径中的正斜杠都替换成反斜杠。\nos.path.abspath：将路径转换为规范的绝对路径。\nos.path.relpath：将路径转换为规范的相对路径。\nos.path.basename：返回路径的基名（即文件或目录的名字）。需要注意，如果路径结尾有斜杠，那么会返回空字符串。\nos.path.dirname：返回路径的父目录。需要注意，如果路径结尾有斜杠，那么返回的就只是去掉末尾斜杠的路径。\nos.path.splitext：输入一个文件路径，返回一个二元组，第二个元素是这个文件的扩展名（含 .），第一个元素就是扩展名前面的路径。如果路径不指向文件，那么第二个元素会是空字符串。\nos.path.exists：判断路径是否存在。\nos.path.isfile：判断路径是否指向文件。\nos.path.isdir：判断路径是否指向目录。路径结尾的斜杠不会影响结果。\nos.path.join：最常用的函数之一，能将多个路径连接在一起，自动在每个路径之间依据 os.sep 的值添加分隔符。\n# Linux下\rIn : os.path.join('a', 'b', 'c')\rOut: 'a/b/c'\r# Windows下\rIn : os.path.join('a', 'b', 'c')\rOut: 'a\\\\b\\\\c'\r 这个函数的行为有点复杂，下面再举几个例子\n# Windows下\r# 路径中的正斜杠替换掉了os.sep\rIn : os.path.join('a', 'b/', 'c')\rOut: 'a\\\\b/c'\r# 结尾的斜杠会被保留\rIn : os.path.join('a', 'b', 'c/')\rOut: 'a\\\\b\\\\c/'\r# 最后一个路径为空字符串时,相当于在结尾添加斜杠\rIn : os.path.join('a', 'b', '')\rOut: 'a\\\\b\\\\'\r Linux 下的行为是一样的。另外还有什么路径如果在根目录或盘符下，那么连接时前面的路径会被忽略之类的行为，这里就不细说了。\nos.expanduser：将一个路径中的 ~ 符号替换成 user 目录的路径。\nos.path 模块是处理路径的经典模块，但我在使用中遇到的问题是，在 Windows 下如果想使用正斜杠，因为这个模块默认用反斜杠来进行连接和替换操作，会导致产生的字符串中两种斜杠相混杂。虽然这种路径完全合法，但作为结果输出时就很难看。可以考虑使用 os.path.normpath 函数来规范化，或者试试下一节将会介绍的模块。\npathlib 于 Python 3.4 引入的新模块，提供了面向对象风格的路径操作，能够完全替代 os.path 和 glob 模块，并涵盖一部分 os 模块的功能。这里简单介绍一下其用法。\npathlib 中的类由上面的图片表示。最顶层的是 PurePath，提供不涉及 I/O 的路径计算；Path 类又称 concrete path，继承 PurePath 的同时提供 I/O 的功能；剩下的几个类从名字可以看出是与平台相关的，我们一般不需要关心，让程序自动决定即可。\n前面提到的路径都是字符串，但 pathlib 会把路径作为一个对象\nfrom pathlib import Path\rp = Path('a/b/c')\r# Linux下\rIn : p\rOut: PosixPath('a/b/c')\r# 获取字符串\rIn : str(p)\rOut: 'a/b/c'\r# Windows下\rIn : p\rOut: WindowsPath('a/b/c')\r# 获取字符串\rIn : str(p)\rOut: 'a\\\\b\\\\c'\r Path 对象内部以正斜杠的形式表示路径，在转换成字符串时会自动根据系统选取分隔符，另外还会自动去掉路径结尾的斜杠。这下我们就不用操心斜杠混用的问题。下面便来介绍 Path 对象的方法和属性。需要注意的是，很多方法返回的依然是 Path 对象。\nPath.exists：判断路径是否存在。\nPath.is_file：判断路径是否指向文件。\nPath.is_dir：判断路径是否指向目录。\nPath.cwd：同 os.getcwd。\nPath.iterdir：同 os.listdir，不过返回的是生成器。\nPath.mkdir：创建该路径表示的目录。parent 参数指定是否顺带着将不存在的父目录也也一并创建了，等同于 os.makedirs 的功能。\nPath.rmdir：删除该路径表示的空目录。\nPath.touch：创建该路径表示的文件。\nPath.open：相当于对路径指向的文件调用 open 函数。\nPath.unlink：删除一个文件或者符号链接。\nPath.rename：同 os.rename。\nPath.replace：同 os.replace。\nPath.resolve：得到绝对路径，或解析符号链接。\nPurePath.name：属性，同 os.path.basename。\nPurePath.parent：属性，同 os.path.dirname。可以写出 p.parent.parent 这样的表达。\nPurePath.parents：属性，由不同层级的父目录的路径组成的序列。例如 p.parents[0] 等于 p.parent，p.parents[1] 等于 p.parent.parent。\nPurePath.suffix：属性，返回文件的扩展名（含 .），如果是目录则返回空字符串。\nPurePath.stem：属性，返回文件名不含扩展名的那一部分，如果是目录就直接返回目录名。\nPurePath.joinpath：同 os.path.join。不过现在通过重载运算符 /，有了更方便的表达\nIn : Path('a') / 'b' / 'c'\rOut: WindowsPath('a/b/c')\r Path.expanduser：同 os.path.expanduser。\nPath.glob：同 glob.iglob，即返回的是生成器。不过现在不需要指定 recursive 参数，当模式中含有 ** 时就会进行递归搜索。\nPath.rglob：相当于在 Path.glob 的模式里提前加上了 **/。即 Path.glob('**/*') 等同于 Path.rglob('*')。\n可以看到 pathlib 提供了丰富的路径操作，再结合 shutil 就足以应对日常使用。另外在 Python 3.6 之后，os 与 os.path 中许多函数能够直接接受 Path 对象作为参数，于是这些模块完全可以互通。pathlib 的缺点也不是没有\n  Python 3.6 以后才算得上完善，并且 API 以后可能会发生变化。\n  读取文件时多一道将 Path 对象转换成字符串的步骤。\n  小结 以上记录了最常用的功能。回到本文开头的问题，我觉得 Windows 平台下可以选择下面的方案\n  os + os.path，路径分隔符全部采用反斜杠。\n  pathlib，路径分隔符全部采用正斜杠。\n  到底选哪种，以后慢慢实践就知道了。\n参考资料 What is the difference between path and directory?\nWindows 的路径中表示文件层级为什么会用反斜杠，而 UNIX 系统都用斜杠？\nShould a directory path variable end with a trailing slash?\nPython os 模块详解\nHow is Pythons glob.glob ordered?\n你应该使用pathlib替代os.path\n","date":"2021-03-26","permalink":"https://zhajiman.github.io/post/python_path/","tags":["python"],"title":"Python 系列：操作文件和目录的路径"},{"content":"前言 Cartopy 中的 Plate Carrée 投影使用方便，但在展示中国地图时会使中国的形状显得很瘪，与之相比，Lambert 投影的效果会更加美观，下图显示了两种投影的差异\n所以本文将会介绍如何在 Cartopy 中实现 Lambert 投影，并为地图添上合适的刻度。文中 Cartopy 的版本是 0.18.0。\nLambert 投影的简单介绍 这里的 Lambert 投影指的是 Lambert conformal conic 投影（兰勃特等角圆锥投影），是通过让圆锥面与地球相切（割），然后将地球表面投影到圆锥面上来实现的。作为一种等角地图投影，Lambert 投影能够较好地保留区域的角度和形状，适合用于对中纬度东西方向分布的大陆板块进行制图。详细的描述请见维基和 ArcMap 上的介绍。\n在 Cartopy 中，这一投影通过 LambertConformal 类来实现\nimport cartopy.crs as ccrs\rmap_proj = ccrs.LambertConformal(\rcentral_longitude=105, standard_parallels=(25, 47)\r)\r 这个类的参数有很多，这里为了画出中国地图，只需要指定中央经线 central_longitude=105，两条标准纬线 standard_parallels=(25, 47)，参数来源是 中国区域Lambert\u0026amp;Albers投影参数 这篇博文。其实笔者对这些参数也没什么概念，如果有错误还请读者指出。\n按照这个设置便可以画出全球的地图了，并且中国位于地图中心\n用 set_extent 方法截取区域 我们一般需要通过 GeoAxes 的 set_extent 方法截取我们关心的区域，下面截取 80°E-130°E，15°N-55°N 的范围\nextent = [80, 130, 15, 55]\rax.set_extent(extent, crs=ccrs.PlateCarree())\r 结果如下图，原本扇形的全球地图会被截取成矩形\n道理上来说给出经纬度的边界，截取出来的应该是一个更小的扇形，但按 issue #697 的说法，set_extent 会选出一个刚好包住这个小扇形的矩形作为边界。这里来验证一下这个说法\nimport matplotlib.path as mpath\rrect = mpath.Path([\r[extent[0], extent[2]],\r[extent[0], extent[3]],\r[extent[1], extent[3]],\r[extent[1], extent[2]],\r[extent[0], extent[2]]\r]).interpolated(20)\rline = rect.vertices\rax.plot(line[:, 0], line[:, 1], lw=1, c='r', transform=ccrs.Geodetic())\r 这段代码是将 extent 所描述的小扇形画在地图上，结果在上一张图里有。可以看到，这个小扇形确实刚好被矩形边界给包住。\n如果确实需要截取出扇形的区域，可以用 set_boundary 方法，效果如下图\nax.set_boundary(rect, transform=ccrs.Geodetic())\r 截取后反而中国显示不全了，需要重新调整 extent 的值。\n为地图添加刻度——默认方法 Cartopy 的版本在 0.17 及以下时，只支持给 Plate Carrée 和 Mercator 投影的地图添加刻度。一个变通的方法是用 ax.text 方法手动添加刻度标签，例子见 Python气象绘图教程 的第 18 期。\n等到了最新的 0.18 版本，gridlines 方法有了给所有投影添加刻度标签的能力。下面来测试一下\nax.gridlines(\rxlocs=np.arange(-180, 180 + 1, 10), ylocs=np.arange(-90, 90 + 1, 10),\rdraw_labels=True, x_inline=False, y_inline=False,\rlinewidth=0.5, linestyle='--', color='gray'\r)\r xlocs 与 ylocs 指定网格线的经纬度位置，实际上超出地图边界的网格并不会被画出，所以这里给出的范围比较宽。draw_labels 指示是否画出刻度标签，而 x_inline 与 y_inline 指示这些标签是否画在地图里面。inline 的选项开启后效果比较乱，所以这里都关闭。结果如下图\n默认的效果十分拉胯，四个方向上都有标签，并且有着多余的旋转效果。那么再修改 gl的属性 看看\n# 关闭顶部和右边的标签,同时禁用旋转.\rgl.top_labels = False\rgl.right_labels = False\rgl.rotate_labels = False\r 结果改善了很多，但仍然有很奇怪的地方：虽然关闭了右边的纬度标签，但经度的标签出现在了两边的 y 轴上。根据 issue #1530，一个很不优雅的解决方法是将网格线分两次来画\n  第一次画出纬线和 90°E-120°E 的经线，并且 draw_label=True。\n  第二次单独画出 70°E、80°E、130°E、140°E 的经线，并且 draw_label=False。\n  结果这里就不展示了，肯定能去掉 y 轴上的经度标签，但显然这个方法有点“事后擦屁股”的意思。\n为地图添加刻度——自制方法 这里尝试自己写一个添加刻度的函数。思路来自 Cartopy 的 Gridliner 类的源码和 Labelling grid lines on a Lambert Conformal projection 这篇 note。\n原理是想办法在 Lambert 投影坐标系（这里亦即 Matplotlib 的 data 坐标系）下表示出 xy 轴和网格线的空间位置，若一条网格线与一个轴线相交，那么这个交点的位置即刻度的位置。最后直接将这些位置用于 set_xticks 和 set_yticks 方法。判断两线相交用到了 Shapley 库。代码和效果如下\nimport numpy as np\rimport shapely.geometry as sgeom\rimport matplotlib.pyplot as plt\rimport cartopy.crs as ccrs\rfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\rdef find_x_intersections(ax, xticks):\r'''找出xticks对应的经线与下x轴的交点在data坐标下的位置和对应的ticklabel.'''\r# 获取地图的矩形边界和最大的经纬度范围.\rx0, x1, y0, y1 = ax.get_extent()\rlon0, lon1, lat0, lat1 = ax.get_extent(ccrs.PlateCarree())\rxaxis = sgeom.LineString([(x0, y0), (x1, y0)])\r# 仅选取能落入地图范围内的ticks.\rlon_ticks = [tick for tick in xticks if tick \u0026gt;= lon0 and tick \u0026lt;= lon1]\r# 每条经线有nstep个点.\rnstep = 50\rxlocs = []\rxticklabels = []\rfor tick in lon_ticks:\rlon_line = sgeom.LineString(\rax.projection.transform_points(\rccrs.Geodetic(),\rnp.full(nstep, tick),\rnp.linspace(lat0, lat1, nstep)\r)[:, :2]\r)\r# 如果经线与x轴有交点,获取其位置.\rif xaxis.intersects(lon_line):\rpoint = xaxis.intersection(lon_line)\rxlocs.append(point.x)\rxticklabels.append(tick)\relse:\rcontinue\r# 用formatter添上度数和东西标识.\rformatter = LongitudeFormatter()\rxticklabels = [formatter(label) for label in xticklabels]\rreturn xlocs, xticklabels\rdef find_y_intersections(ax, yticks):\r'''找出yticks对应的纬线与左y轴的交点在data坐标下的位置和对应的ticklabel.'''\rx0, x1, y0, y1 = ax.get_extent()\rlon0, lon1, lat0, lat1 = ax.get_extent(ccrs.PlateCarree())\ryaxis = sgeom.LineString([(x0, y0), (x0, y1)])\rlat_ticks = [tick for tick in yticks if tick \u0026gt;= lat0 and tick \u0026lt;= lat1]\rnstep = 50\rylocs = []\ryticklabels = []\rfor tick in lat_ticks:\r# 注意这里与find_x_intersections的不同.\rlat_line = sgeom.LineString(\rax.projection.transform_points(\rccrs.Geodetic(),\rnp.linspace(lon0, lon1, nstep),\rnp.full(nstep, tick)\r)[:, :2]\r)\rif yaxis.intersects(lat_line):\rpoint = yaxis.intersection(lat_line)\rylocs.append(point.y)\ryticklabels.append(tick)\relse:\rcontinue\rformatter = LatitudeFormatter()\ryticklabels = [formatter(label) for label in yticklabels]\rreturn ylocs, yticklabels\rdef set_lambert_ticks(ax, xticks, yticks):\r'''\r给一个LambertConformal投影的GeoAxes在下x轴与左y轴上添加ticks.\r要求地图边界是矩形的,即ax需要提前被set_extent方法截取成矩形.\r否则可能会出现错误.\rParameters\r----------\rax : GeoAxes\r投影为LambertConformal的Axes.\rxticks : list of floats\rx轴上tick的位置.\ryticks : list of floats\ry轴上tick的位置.\rReturns\r-------\rNone\r'''\r# 设置x轴.\rxlocs, xticklabels = find_x_intersections(ax, xticks)\rax.set_xticks(xlocs)\rax.set_xticklabels(xticklabels)\r# 设置y轴.\rylocs, yticklabels = find_y_intersections(ax, yticks)\rax.set_yticks(ylocs)\rax.set_yticklabels(yticklabels)\r 这次的效果就好很多了，并且相比于默认方法，坐标轴上也有了刻度的凸起。需要注意的是，这个方法要求在设置刻度之前就通过 set_extent 方法截取出矩形的边界，否则可能有奇怪的结果。另外经测试对 Albers 投影也适用。\n也许下次更新后 Cartopy 的刻度标注功能能得到改善，就算没有，我们也可以根据上面描述的思路来自制刻度。\n","date":"2021-03-24","permalink":"https://zhajiman.github.io/post/cartopy_lambert/","tags":["cartopy","matplotlib"],"title":"Cartopy 系列：为 Lambert 投影地图添加刻度"},{"content":"简介 常用的地图可视化的编程工具有 MATLAB、IDL、GrADS、GMT、NCL 等。我之前一直使用的是脚本语言 NCL，易用性不错，画地图的效果也很好。然而 2019 年初，NCAR 宣布 NCL 将停止更新，并会在日后转为 Python 的绘图包。于是我开始考虑转投 Python，同时觉得在 Python 环境下如果还是用 PyNGL 那一套语法的话，未免有些换汤不换药。因此我选择用 Python 环境下专有的 Cartopy 包来画地图。\n此前 Python 最常用的地图包是 Basemap，然而它将于 2020 年被弃用，官方推荐使用 Cartopy 包作为替代。Cartopy 是英国气象局开发的地图绘图包，实现了 Basemap 的大部分功能，还可以通过 Matplotlib 的 API 实现丰富的自定义效果。\n本文将会从一个 NCL 转 Python 的入门者的角度，介绍如何安装 Cartopy，如何绘制地图，并实现一些常用的效果。代码基于 0.18.0 版本的 Cartopy。\n提示 本文其实更新过数次，每次都修正了一些表述或 bug，如果还存在问题的话请读者在评论区指出。另外强烈建议读完本文后继续阅读 Cartopy 系列：对入门教程的补充，解答了更多常见的问题。\n安装 Cartopy 和相关的库 通过 Conda 来安装 Cartopy 是最为简单方便的。首先我们需要下载最新的 Python 3 的 Conda 环境（Anaconda 或 Miniconda 皆可），设置国内镜像源，建立好虚拟环境，然后参照 Cartopy 官网的 installation guide，执行操作：\nconda install -c conda-forge cartopy\r 接着便会开始安装 Cartopy，以及 Numpy、Matplotlib 等一系列相关包。Cartopy 的安装就是这么简单。之后还可以考虑去安装 netCDF4、h5py、pyhdf 等支持特定数据格式读写的包。\n画地图的基本流程 以一个简单的例子来说明：\n# 导入所需的库\rimport matplotlib as mpl\rimport matplotlib.pyplot as plt\rimport cartopy.crs as ccrs\r# 创建画布以及ax\rfig = plt.figure()\rax = fig.add_subplot(111, projection=ccrs.PlateCarree())\r# 调用ax的方法画海岸线\rax.coastlines()\rplt.show()\r Cartopy 是利用 Matplotlib 来画图的，因此首先要导入 pyplot 模块。在 Cartopy 中，每种投影都是一个类，被存放在 cartopy.crs 模块中，crs 即坐标参考系统（Coordinate Reference Systems）之意。所以接着要导入这个模块。这里选取最常用的等距圆柱投影 ccrs.PlateCarree 作为地图投影。\n我们知道，Matplotlib 画图是通过调用 Axes 类的方法来完成的。Cartopy 创造了一个 Axes 的子类，GeoAxes，它继承了前者的基本功能，还添加了一系列绘制地图元素的方法。创建一个 GeoAxes 对象的办法是，在创建 axes（或 subplot）时，通过参数 projection 指定一个 ccrs 中的投影。这里便利用这一方法生成了一个等距圆柱投影下的 ax。\n最后调用 ax 的方法 coastlines 画出海岸线，默认以本初子午线为中心，比例尺为 1:110m（m 表示 million）。\n因此用 Cartopy 画地图的基本流程并不复杂：\n 创建画布。 通过指定 projection 参数，创建 GeoAxes 对象。 调用 GeoAxes 的方法画图。  GeoAxes 的一些有用的方法 GeoAxes 有不少有用的方法，这里列举如下：\n set_global：让地图的显示范围扩展至投影的最大范围。例如，对 PlateCarree 投影的 ax 使用后，地图会变成全球的。 set_extent：给出元组 (x0, x1, y0, y1) 以限制地图的显示范围。 set_xticks：设置 x 轴的刻度。 set_yticks：设置 y 轴的刻度。 gridlines：给地图添加网格线。 coastlines：在地图上绘制海岸线。 stock_img：给地图添加低分辨率的地形图背景。 add_feature：给地图添加特征（例如陆地或海洋的填充、河流等）。  后文中具体的例子中将会经常用到这些方法。\n使用不同的投影 # 选取多种投影\rprojections = [\rccrs.PlateCarree(),\rccrs.Robinson(),\rccrs.Mercator(),\rccrs.Orthographic()\r]\r# 画出多子图\rfig = plt.figure()\rfor i, proj in enumerate(projections, 1):\rax = fig.add_subplot(2, 2, i, projection=proj)\rax.stock_img() # 添加低分辨率的地形图\rax.coastlines()\rax.set_title(f'{type(proj)}', fontsize='small')\rplt.show()\r 这个例子展示了如何使用其它投影和画出多子图。其中 stock_img 方法可以给地图添加低分辨率的地形背景图，让地图显得不那么寒碜。\n在初始化投影时可以指定一些参数，例如 ccrs.PlateCarree(central_longitude=180) 可以让等距圆柱投影的全球图像的中央位于太平洋的 180 度经线处。\n画多子图还可以用 plt.subplots 函数，但是投影就只能通过 subplot_kw 参数给出，并且每张子图的投影要求一致。\n在地图上添加特征（Features） 除了画出海岸线外，我们常常需要在地图上画出更多特征，例如陆地海洋、河流湖泊等。cartopy.feature 中便准备了许多常用的特征对象。需要注意的是，这些对象的默认比例是 1:110m。\nimport cartopy.feature as cfeature\rfig = plt.figure()\rproj = ccrs.PlateCarree()\rax = fig.add_subplot(111, projection=proj)\r# 设置经纬度范围,限定为中国\r# 注意指定crs关键字,否则范围不一定完全准确\rextents = [75, 150, 15, 60]\rax.set_extent(extents, crs=proj)\r# 添加各种特征\rax.add_feature(cfeature.OCEAN)\rax.add_feature(cfeature.LAND, edgecolor='black')\rax.add_feature(cfeature.LAKES, edgecolor='black')\rax.add_feature(cfeature.RIVERS)\rax.add_feature(cfeature.BORDERS)\r# 添加网格线\rax.gridlines(linestyle='--')\rplt.show()\r add_feature 方法能够把 cfeature 里的特征对象添加到地图上。上面的例子中就依次添加了海洋、陆地、湖泊、河流，还有国界线的特征。由于渲染实际上采用的是 Matplotlib 里 annotations 的方法，所以添加的特征本质上就是一些线或者多边形，edgecolor、facecolor 等常用关键字都可以用来指定这些特征的效果。\nCartopy 本身自带一些常用的地图数据，不过有些特征并没有内置，而是会在脚本运行时自动从 Natural Earth 网站上下载下来，此时命令行可能会提示一些警告信息。下载完成后，以后使用这个特征都不会再出现警告。\n另外存在一个非常重要的问题，Cartopy自带的中国地图数据不符合我国的地图标准，例如上图中缺少台湾地区，藏南区域边界有误。后面的小节还会再提到如何画出正确的中国地图。\n设置地图分辨率 Cartopy 自带的 Natural Earth 的地图有三档分辨率：1:10m、1:50m、1:110m。默认分辨率为 1:110m，这在很多场合下显得很粗糙。设置分辨率的方法如下：\n# coastlines方法使用resolution关键字\rax.coastlines(resolution='50m')\r# add_feature方法中,则要调用cfeature对象的with_scale方法\rax.add_feature(cfeature.OCEAN.with_scale('50m'))\r 接着是一个例子：\nfig = plt.figure()\rres = ['110m', '50m', '10m']\rextents = [75, 150, 15, 60]\rproj = ccrs.PlateCarree()\rfor i, res in enumerate(['110m', '50m', '10m']):\rax = fig.add_subplot(1, 3, i+1, projection=proj)\rax.set_extent(extents, crs=proj)\rax.add_feature(cfeature.OCEAN.with_scale(res))\rax.add_feature(cfeature.LAND.with_scale(res), edgecolor='black')\rax.add_feature(cfeature.LAKES.with_scale(res), edgecolor='black')\rax.add_feature(cfeature.RIVERS.with_scale(res))\rax.add_feature(cfeature.BORDERS.with_scale(res))\rax.gridlines(linestyle='--')\rax.set_title('resolution=' + res)\rplt.show()\r 可以看到绘制效果有很大区别，不过相应地，分辨率越高画图速度越慢。\n下载地图 Cartopy 自带的地图数据保存在下面这个命令显示的目录中\nimport cartopy\rprint(cartopy.config['data_dir'])\r 一般来说自带的地图足以满足日常需求，如果想手动下载地图，可以到 Natural Earth 网站上下载所需的地图数据。该网页提供三类地图数据：\n Cultural：国界线、道路、铁路等文化信息。 Physical：陆地、海洋、海岸线、湖泊、冰川等地质信息。 Raster：各种分辨率的地形起伏栅格文件。  其中 Cultural 和 Physical 数据可以作为常用的特征来进行添加，而 Raster 数据则需要用 imshow 方法来作为图片显示。把下载好的文件解压到 data_dir 下对应的子目录中即可。\n在地图上添加数据 在直接调用 ax.plot、ax.contourf 等方法在地图上添加数据之前，需要了解 Cartopy 的一个核心概念：在创建一个 GeoAxes 对象时，通过 projection 关键字指定了这个地图所处的投影坐标系，这个坐标系的投影方式和原点位置都可以被指定。但是我们手上的数据很可能并不是定义在这个坐标系下的（例如那些规整的经纬度网格数据），因此在调用画图方法往地图上添加数据时，需要通过 transform 关键字指定我们的数据所处的坐标系。画图过程中，Cartopy 会自动进行这两个坐标系之间的换算，把我们的数据正确投影到地图的坐标系上。下面给出一个例子：\n# 定义一个在PlateCarree投影中的方框\rx = [-100.0, -100.0, 100.0, 100.0, -100.0]\ry = [-60.0, 60.0, 60.0, -60.0, -60.0]\r# 选取两种地图投影\rmap_proj = [ccrs.PlateCarree(), ccrs.Mollweide()]\rdata_proj = ccrs.PlateCarree()\rfig = plt.figure()\rax1 = fig.add_subplot(211, projection=map_proj[0])\rax1.stock_img()\rax1.plot(x, y, marker='o', transform=data_proj)\rax1.fill(x, y, color='coral', transform=data_proj, alpha=0.4)\rax1.set_title('PlateCarree')\rax2 = fig.add_subplot(212, projection=map_proj[1])\rax2.stock_img()\rax2.plot(x, y, marker='o', transform=data_proj)\rax2.fill(x, y, color='coral', transform=data_proj, alpha=0.4)\rax2.set_title('Mollweide')\rplt.show()\r 可以看到，等距圆柱投影地图上的一个方框，在摩尔威投影的地图上会向两边“长胖”——尽管这两个形状代表同一个几何体。如果不给出 transform 关键字，那么 Cartopy 会默认数据所在的坐标系是 PlateCarree()。为了严谨起见，建议在使用任何画图方法（plot、contourf、pcolormesh 等）时都给出 transform 关键字。\n为地图添加经纬度刻度 在 0.17 及以前的版本中，Cartopy 仅支持为直角坐标系统（等距圆柱投影和麦卡托投影）添加刻度，而对兰勃特投影这样的则无能为力。0.18 版本开始，虽然官网说已经实现了对所有投影添加刻度的功能（PR #1117），但实际效果还是挺奇怪。因此这里就只以等距圆柱投影为例\n# 导入Cartopy专门提供的经纬度的Formatter\rfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\rmap_proj = ccrs.PlateCarree()\rfig = plt.figure()\rax = fig.add_subplot(111, projection=map_proj)\rax.set_global()\rax.stock_img()\r# 设置大刻度和小刻度\rtick_proj = ccrs.PlateCarree()\rax.set_xticks(np.arange(-180, 180 + 60, 60), crs=tick_proj)\rax.set_xticks(np.arange(-180, 180 + 30, 30), minor=True, crs=tick_proj)\rax.set_yticks(np.arange(-90, 90 + 30, 30), crs=tick_proj)\rax.set_yticks(np.arange(-90, 90 + 15, 15), minor=True, crs=tick_proj)\r# 利用Formatter格式化刻度标签\rax.xaxis.set_major_formatter(LongitudeFormatter())\rax.yaxis.set_major_formatter(LatitudeFormatter())\rplt.show()\r Cartopy 中需要用 GeoAxes 类的 set_xticks 和 set_yticks 方法来分别设置经度和纬度刻度。这两个方法还可以通过 minor 参数，指定是否添上小刻度。其中 crs 关键字指的是我们给出的刻度是在什么坐标系统下定义的，这样好换算至 ax 所在的坐标系统，原理同上一节所述。如果不指定，就很容易出现把刻度画到地图外的情况。除了 set_xticks，set_extent 方法同样有 crs 关键字，我们需要多加注意。\n接着利用 Cartopy 专门提供的 Formatter：LongitudeFormatter 和 LatitudeFormatter 来格式化刻度标签，使之能有东经西经、南纬北纬的字母标识。值得一提的是，这两个类还能用在普通的 Axes 上（例如拿来画纬高图）。\n在标识刻度的过程中，有时可能会出现下图这样的问题\n即全球地图的最右端缺失了 0° 的标识，这是 Cartopy 内部在换算刻度时用到了 mod 计算而导致的，解决方法见 stack overflow 上的 这个讨论，这里就不赘述了。额外提一句，NCL 对于这种情况就能正确处理。\nCartopy 还有一个很坑的地方在于，set_extent 与指定刻度的效果会互相覆盖：如果先用前者设置好了地图的显示范围，接下来的 set_xticks 超出了 extents 的范围的话，最后的出图范围就会以刻度的范围为准。因此使用时要注意刻度的范围，或把 set_extent 操作放在最后实施。\n除了利用 set_xticks 和 set_yticks 方法，还可以在画网格线的同时画出刻度。例子如下：\nax = plt.axes(projection=ccrs.Mercator())\rax.coastlines()\r# 开启网格线\rgl = ax.gridlines(\rcrs=ccrs.PlateCarree(), draw_labels=True,\rlinewidth=1, color='gray', linestyle='--',\rxlocs=[-180, -45, 0, 45, 180],\rylocs=np.arange(-80, 81, 20)\r)\r# 隐藏上边和左边的刻度标签\rgl.top_labels = False\rgl.left_labels = False\r# 设置刻度标签的风格\rgl.xlabel_style = {'color': 'red', 'weight': 'bold'}\rgl.ylabel_style = {'size': 10, 'color': 'gray'}\rplt.show()\r gridlines 方法可以为地图添加网格线，其中 xlocs 和 ylocs 关键字指定经纬度刻度（还可以接受 Locator），crs 参数指定刻度所属的坐标系统，xformatter 和 yformatter 关键字指定刻度的 Formatter——不过默认即为 LongitudeFormatter 和 LatitudeFormatter，所以这里可以省略。这种方法的优点是网格线 gl 所属的 Gridliner 类有丰富的可调选项，缺点是这些刻度并非真正意义上的刻度，而只是网格线的标签，所以坐标轴上会缺少凸出的线条。\n绘制正确的中国地图 我在网上找到了两个绘制中国地图的教程：\n 捍卫祖国领土从每一张地图开始 Cartopy 绘图示例库  第一个链接提供了正确的中国省界的 shapefile，用 Cartopy 的 shapereader 读取后即可绘制。第二个链接则利用的是 GMT 中文社区上提供的省界的经纬度数据。两个链接都给出了完整的代码，经测试都可以正常作图。第一个链接的效果图如下：\n问题在于两种方法的画图速度都非常慢，可能是因为给出的 shapefile 分辨率太高？我自己用的是 Meteoinfo 里自带的 bou2_4p.shp 文件，这个文件分辨率适中，画图速度比较理想。使用方法同第一个链接。\n从入门到放弃 最后来个 NCL 与 Cartopy 在画图方面的简单对比吧。\nNCL：\n 画地图参数多，效果好，官方文档详尽。 画图速度较快。 绘图语法虽然麻烦，但能写出很规整的代码。 默认的画图模板不好看，改善效果很麻烦。  Cartopy：\n 画地图的可调参数比 NCL 少，需要通过 Matplotlib 魔改上去。 官方文档信息不全，缺乏例子，有问题只能靠 Stack Overflow。 画图速度偏慢。 画等经纬度投影的效果还行，但是对于其它投影经常会有 bug。 pcolormesh 等方法绘制的图像在跨越 0° 经度时常常会出问题。 与 Matplotlib 配合较好。  总之，我现在觉得，除非是对 Python 丰富的扩展库有需求的话，单就画点科研用的地图，从 NCL 转 Python 并没有太大的优势，还会让你陷入同 bug 作战的漩涡中。NCL 语言虽然冷门，但它从上世纪90年代发展至今，版本号已经达到 6.6.2，多年下来已经累计了足够多的实用功能。虽然这一优秀的工具停止了开发，但它依旧适用于一般的数据处理和可视化工作。\n不过技多不压身，学点 Cartopy，就当是熟悉一下 Python 的功能吧。\n画图的例子 下面举一个读取 NETCDF 格式的 ERA5 文件并画图的例子。首先在 map_funcs.py 文件里定义一些常用的自定义函数\n#----------------------------------------------------------------------------\r# 2019-09-10\r# 绘制地图用的函数.\r#----------------------------------------------------------------------------\rimport matplotlib.ticker as mticker\rimport matplotlib.patches as mpatches\rimport cartopy.crs as ccrs\rimport cartopy.io.shapereader as shpreader\rfrom cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\rdef add_Chinese_provinces(ax, **kwargs):\r'''\r在地图上画出中国省界的shapefile.\rParameters\r----------\rax : GeoAxes\r目标地图.\r**kwargs\r绘制shape时用到的参数.例如linewidth,edgecolor和facecolor等.\r'''\rproj = ccrs.PlateCarree()\rreader = shpreader.Reader('D:/maps/shps/bou2_4p.shp')\rprovinces = reader.geometries()\rax.add_geometries(provinces, proj, **kwargs)\rreader.close()\rdef set_map_extent_and_ticks(\rax, extents, xticks, yticks, nx=0, ny=0,\rxformatter=None, yformatter=None\r):\r'''\r设置矩形投影的地图的经纬度范围和刻度.\rParameters\r----------\rax : GeoAxes\r目标地图.支持_RectangularProjection和Mercator投影.\rextents : 4-tuple of float or None\r经纬度范围[lonmin, lonmax, latmin, latmax].值为None表示全球.\rxticks : array_like\r经度主刻度的坐标.\ryticks : array_like\r纬度主刻度的坐标.\rnx : int, optional\r经度主刻度之间次刻度的个数.默认没有次刻度.\r当经度不是等距分布时,请不要进行设置.\rny : int, optional\r纬度主刻度之间次刻度的个数.默认没有次刻度.\r当纬度不是等距分布时,请不要进行设置.\rxformatter : Formatter, optional\r经度主刻度的Formatter.默认使用无参数的LongitudeFormatter.\ryformatter : Formatter, optional\r纬度主刻度的Formatter.默认使用无参数的LatitudeFormatter.\r'''\r# 设置主刻度.\rproj = ccrs.PlateCarree()\rax.set_xticks(xticks, crs=proj)\rax.set_yticks(yticks, crs=proj)\r# 设置次刻度.\rxlocator = mticker.AutoMinorLocator(nx + 1)\rylocator = mticker.AutoMinorLocator(ny + 1)\rax.xaxis.set_minor_locator(xlocator)\rax.yaxis.set_minor_locator(ylocator)\r# 设置Formatter.\rif xformatter is None:\rxformatter = LongitudeFormatter()\rif yformatter is None:\ryformatter = LatitudeFormatter()\rax.xaxis.set_major_formatter(xformatter)\rax.yaxis.set_major_formatter(yformatter)\r# 在最后调用set_extent,防止刻度拓宽显示范围.\rif extents is None:\rax.set_global()\relse:\rax.set_extent(extents, crs=proj)\rdef add_box_on_map(ax, extents, **rect_kw):\r'''\r在地图上画出一个方框.\rParameters\r----------\rax : GeoAxes\r目标地图.最好为矩形投影,否则效果可能很糟.\rextents : 4-tuple of float\r方框的经纬度范围[lonmin, lonmax, latmin, latmax].\r**rect_kw\r创建Rectangle时的关键字参数.\r例如linewidth,edgecolor和facecolor等.\r'''\rlonmin, lonmax, latmin, latmax = extents\rrect = mpatches.Rectangle(\r(lonmin, latmin), lonmax - lonmin, latmax - latmin,\rtransform=ccrs.PlateCarree(), **rect_kw\r)\rax.add_patch(rect)\r 其中 add_Chinese_provinces 函数用于在地图上画出中国省界的 shapefile；set_map_extent_and_ticks 用于设置矩形投影（例如 PlateCarree）地图的显示范围和刻度，代码参考了 Cartopy 和 GeoCAT-viz 的源码。接着是主程序\n#-------------------------------------------------------------------------\r# 2019-09-10\r# 画出ERA5数据在500hPa高度的相对湿度和水平风场.\r#-------------------------------------------------------------------------\rimport numpy as np\rimport xarray as xr\rimport matplotlib.pyplot as plt\rimport matplotlib.ticker as mticker\rimport matplotlib.patches as mpatches\rimport cartopy.crs as ccrs\rfrom map_funcs import add_Chinese_provinces, set_map_extent_and_ticks\rif __name__ == '__main__':\r# 设置绘图区域.\rlonmin, lonmax = 75, 150\rlatmin, latmax = 15, 60\rextents = [lonmin, lonmax, latmin, latmax]\r# 读取extents区域内的数据.\rfilename = 't_uv_rh_gp_ERA5.nc'\rwith xr.open_dataset(filename) as ds:\r# ERA5文件的纬度单调递减,所以先反转过来.\rds = ds.sortby(ds.latitude)\rds = ds.isel(time=0).sel(\rlongitude=slice(lonmin, lonmax),\rlatitude=slice(latmin, latmax),\rlevel=500\r)\rproj = ccrs.PlateCarree()\rfig = plt.figure()\rax = fig.add_subplot(111, projection=proj)\r# 添加海岸线和中国省界.\rax.coastlines(resolution='10m', lw=0.3)\radd_Chinese_provinces(ax, lw=0.3, ec='k', fc='none')\r# 设置经纬度刻度.\rset_map_extent_and_ticks(\rax, extents,\rxticks=np.arange(-180, 190, 15),\ryticks=np.arange(-90, 100, 15),\rnx=1, ny=1\r)\rax.tick_params(labelsize='small')\r# 画出相对湿度的填色图.\rim = ax.contourf(\rds.longitude, ds.latitude, ds.r,\rlevels=np.linspace(0, 100, 11), cmap='RdYlBu_r',\rextend='both', alpha=0.8\r)\rcbar = fig.colorbar(\rim, ax=ax, shrink=0.9, pad=0.1, orientation='horizontal',\rformat=mticker.PercentFormatter()\r)\rcbar.ax.tick_params(labelsize='small')\r# 画出风箭头.直接使用DataArray会报错,所以转换成ndarray.\rQ = ax.quiver(\rds.longitude.values, ds.latitude.values,\rds.u.values, ds.v.values,\rscale_units='inches', scale=180, angles='uv',\runits='inches', width=0.008, headwidth=4,\rregrid_shape=20, transform=proj\r)\r# 在ax右下角腾出放图例的空间.\r# zorder需大于1,以避免被之前画过的内容遮挡.\rw, h = 0.12, 0.12\rrect = mpatches.Rectangle(\r(1 - w, 0), w, h, transform=ax.transAxes,\rfc='white', ec='k', lw=0.5, zorder=1.1\r)\rax.add_patch(rect)\r# 添加风箭头的图例.\rqk = ax.quiverkey(\rQ, X=1-w/2, Y=0.7*h, U=40,\rlabel=f'{40} m/s', labelpos='S', labelsep=0.05,\rfontproperties={'size': 'x-small'}\r)\rtitle = 'Relative Humidity and Wind at 500 hPa'\rax.set_title(title, fontsize='medium')\rfig.savefig('rh_wnd.png', dpi=200, bbox_inches='tight')\rplt.close(fig)\r 其中绘制风箭头的部分可以参考 Matplotlib 系列：图解 quiver。\n补充链接 本文介绍的只是 Cartopy 的最简单的功能，还有诸如读取 shapefile、地图 mask、使用网络地图等功能都没有介绍（因为我也没用到过……）。下面补充一些可能有帮助的链接\n  一个地球与环境数据科学的教程：Making Maps with Cartopy\n  云台书使的绘图教程，内容非常全面，含有地图裁剪等高级内容：Python气象绘图教程\n  Unidata 给出的例子：Unidata Example Gallery\n  GeoCAT 给出的仿 NCL 的例子：GeoCAT-examples\n  Cartopy 开发成员对于数据跨越边界时的解说：preventing spurious horizontal lines for ungridded pcolor(mesh) data\n  谈到了如何重复利用底图的方法：(筆記) python的cartopy使用、清除已畫的資料方法\n ","date":"2021-03-23","permalink":"https://zhajiman.github.io/post/cartopy_introduction/","tags":["cartopy","matplotlib"],"title":"Cartopy 系列：从入门到放弃"},{"content":"现实中观测的数据或多或少会有缺失的部分，通常称为缺测值（missing value）。NumPy 因为设计上的问题，不能像 R 和 NCL 那样原生支持缺测类型，而是有两种处理缺测的实现：NaN 和 masked array。下面便来依次介绍它们。代码基于 NumPy 1.20.1。\nNaN NaN（not a number）由 IEEE 754 浮点数标准首次引入，是一种特殊的浮点数，用于表示未定义或不可表示的值（即缺测）。NaN 的位模式（bitpattern）是符号位任意，阶码全为 1，尾数最高位表示 NaN 类型，尾数剩余的位不全为 0。作为对比，无穷大的位模式是，符号位决定无穷的正负，阶码全为 1，尾数全为 0。\nNumPy 中用 np.nan 表示一个 NaN，我们可以把数组中的元素赋值为 np.nan，以表示该元素缺测。NaN 的特性如下\n NaN 是一种特殊的浮点数，它可以是 float32 或 float64，但是通常没有其它类型的 NaN。所以不要尝试给整数类型的数组元素赋值为 NaN，不然会发生类型错误。 当 NaN 进行加减乘除时，结果也会变为 NaN。当 NaN 参与比较大小时，结果总是 False。 由于 NaN 的位模式的任意性，一般来说 np.nan == np.nan 的结果为 False。要判断数组中是否含有 NaN 的话，有专门的函数 np.isnan 来进行判断。 当把数组中的元素赋值为 NaN 时，会直接覆盖掉该元素原有的值。  一般我们得到的原始数据中的缺测值不会直接用 NaN 表示，而是会用人为给定的填充值（fill value）表示，例如用 -9999 指示某个数据缺测。在读取为 ndarray 后，为了避免这些 -9999 参与计算，需要把它们赋值为 NaN，此时可以用 np.isclose 函数来筛选出填充值\nfill_value = -9999.0\rmask = np.isclose(data, fill_value)\rdata[mask] = np.nan\r 有时我们需要利用数据中剩下的有效数据进行计算，那么便需要忽略（ignore）这些缺测值。实现方法有两种，一是利用np.isnan函数筛选出有效值再进行计算\ndata_valid = data[~np.isnan(data)]\rmean_value = np.mean(data_valid)\r 二是使用一些会自动跳过 NaN 的特殊函数\nmean_value = np.nanmean(data)\rstd_value = np.nanstd(data)\r 如上图所示，这样的函数以 \u0026ldquo;nan\u0026rdquo; 作为前缀，可惜这种函数不过十来个。并且当数组元素（沿某一维度）全为 NaN 时，这些函数的行为还会有所不同\n nanargmin 和 nanargmax 会直接报错。 nansum、nancumsum、nanprod 和 nancumprod 会将 NaN 替换为 0 或 1，再计算出有意义的结果。 其它函数会报警（空切片、全 NaN、自由度小于 0），并返回 NaN。  所以需要小心全为 NaN 的情况。\nNumPy 的普通函数接受含 NaN 的数组时，效果五花八门：有的会报错，有的会返回 NaN，有的会返回正确的结果，有的会返回错误的结果。此外，有些 SciPy 的函数能够通过 nan_policy 参数指定如何对待 NaN。总之，使用 NaN 时要多加小心。\nMasked Array NumPy 中对缺测值还有另一种实现——masked array。思路是创建一个和 data 数组同样形状的布尔类型 mask 数组，两个数组的元素一一对应。若 mask 数组中某个位置的元素值为 True，那么 data 数组中对应的元素则被判定为 masked（即缺测）；若值为 False，则 data 数组对应的元素判定为有效。\nData 数组和 mask 数组打包而成的对象就称作 masked array，属于 ndarray 的子类，继承了许多 ndarray 的方法。NumPy 中的 ma 模块提供了创建和操作 masked array 的功能。\nmasked array 的特性如下\n 对整型、浮点型、布尔型数组都适用，因为 mask 数组并不依赖于 NaN 的位模式。 缺测的元素进行加减乘除和比较大小时，结果也会变成缺测。 不保证缺测元素的原始值在经过复杂计算后依然保留。 能够记录给定的填充值。 ma 模块提供大量能够忽略缺测值的计算函数，masked array 对象也带有许多方法。  下面介绍使用 masked array 的基本方法\nimport numpy.ma as ma\r# 直接给出原始数组和mask来创建masked array\rx = ma.array([1, 2, 3], mask=[True, False, False])\r# 返回原始数组\rx.data\r# 返回mask数组\rx.mask\r# 指定填充值\rx.fill_value = -9999\r# 把data中数值等于fill_value的元素设为masked状态,并指定填充值为fill_value\rx = ma.masked_equal(data, fill_value)\r# 同上,但是内部使用了np.isclose方法,更适用于浮点数\rx = ma.masked_values(data, fill_value)\r# 把data中数值大于/小于(等于)fill_value的元素设为masked状态\r# 填充值会被设定为默认值\rx = ma.masked_greater(data, value)\rx = ma.masked_greater_equal(data, value)\rx = ma.masked_less(data, value)\rx = ma.masked_less_equal(data, value)\r# 用条件式决定是否masked\r# 填充值会被设定为默认值\rx = ma.masked_where(data \u0026gt; 0, data)\r# 把NaN和inf的元素mask掉\rx = ma.masked_invalid(data)\r# 统计有效值的个数\rn = x.count()\r# 使用忽略缺测值的函数和方法\rmean_value = ma.mean(x)\rmean_value = x.mean()\rcos_value = ma.cos(x)\r# 从masked array中提取出有效值,返回一维的ndarray\rx_valid = x[~x.mask]\rx_valid = x.compressed()\r# 设定fill_value\rx.fill_value = 0\r# 填充缺测值,返回ndarray,默认使用fill_value属性填充\ry = x.filled()\r Masked array 比较大小后得到的布尔数组依旧是 masked array，并且可能含有缺测部分，如果再用这个布尔数组去索引 masked array，那么结果里也会含有缺测部分，此时只要再使用 compressed 方法，就能得到真正不含缺测的有效值。例如\n# x: [1, 2, -]\rx = ma.array([1, 2, 3], mask=[False, False, True])\r# cond: [False, True, -]\rcond = x \u0026gt;= 2\r# x_valid: [2, -]\rx_valid = x[cond]\r# x_valid_true: [2]\rx_valid_true = x.compressed()\r 下面再来讲讲如何修改 mask。首先可以直接修改 mask 数组的数值。又或者，可以用模块中的 ma.masked 来进行修改，这是一个可以设置元素缺测状态的常量\n# 把第一个元素设为 masked\rx[0] = ma.masked\r# 全部设为缺测\rx[:] = ma.masked\r 需要注意，模块中还存在一个 ma.nomask 量，但它本质上是布尔类型的 False，所以不要用它来做上面的操作，否则会导致元素的数值直接变为 0。\n除此之外，还有一种方法是直接给处于 masked 状态的元素赋值，这样会让元素不再缺测，但如果 masked array 的 hard_mask 参数为 True 的话（默认为 False），会拒绝这样的直接改写。个人觉得最好不要这样直接改写，所以对此有需求的读者可以参考 NumPy 文档的说明。\n类似于 NaN 一节的讨论，若 masked array（沿某一维度）全部缺测时，用于 masked array 的函数和方法均能直接返回缺测，而不会弹出乱七八糟的报错和警告，这一点比较符合直觉。可以看出 masked array 对于全部缺测的情况更为宽容一些。\n如果使用 NumPy 的普通函数来操作 masked array 的话，经常无法正确处理缺测值，并且会返回 ndarray 对象。所以 ma 模块提供了很多同名的但适用于 masked array 的函数，例如 concatenate、hstack、vstack、where 等。此外 SciPy 中还存在一个 stats.mstats 模块，专门提供处理 masked array 的统计函数。\n两种方法的对比 首先指出 masked array 相比 NaN 方法的优势\n 把数据、缺测值位置，和填充值打包到了一起，当数组特别多时，更加易于管理。 对于整型数组和布尔数组也能使用。 用于处理 masked array 的函数远多于处理 NaN 的函数。 对于全部缺测的情况更为宽容。  但是 masked array 的缺点也是显而易见的\n 多附带了一个布尔数组，增加了内存的消耗。 计算上可能会更慢。  下面就举一个测试计算速度的例子\nimport numpy as np\rimport numpy.ma as ma\rx = np.random.rand(1000, 1000)\rflag = np.random.randint(0, 2, (1000, 1000))\r# 设置NaN数组\rx_nan = x.copy()\rx_nan[flag] = np.nan\r# 设置masked array\rx_mask = ma.array(x, mask=flag)\r 接着用 IPython 的命令进行测试\n可以看到计算速度慢上 6 倍之多。不过有一说一，我在使用过程也碰到过 masked array 反而更快的情况。所以到底选择哪一种实现，还得由具体场景、测试结果，以及自己的使用习惯来决定。\n还有别的处理方式吗？ Pandas 和 xarray 都采用了 NaN 的实现方式，其对象的许多方法都默认开启跳过 NaN 的 skipna 的选项。其中 pandas 从 1.0 版本开始，实验性地增加了类似于 masked array 的实现——pd.NA，使浮点型以外的数据类型也能用上缺测值，有兴趣的读者可以去试试。\nMatplotlib 中的缺测值 如果是使用简单的 plt.plot 函数，NaN 或者 masked value 的点会被认为数据在那里断开了，效果如下图\n不过 plt.bar 会产生警告，并把 masked value 转换为 nan。\n对于 plt.imshow、plt.pcolor，和 plt.pcolormesh，它们绘制的是色块，NaN 或者 masked value 所在的色块默认为透明的。如果要用颜色指示出缺测值，需要调整 colormap 的设置\nimport copy\r# 使用copy以免影响全局的colormap\rcmap = copy.copy(plt.cm.viridis)\r# 设置缺测值的颜色和透明度\rcmap.set_bad('gray', 1.0)\r 下面的例子中，缺测值的颜色被设定成蓝色\n以上两个例子都来自 Matplotlib 官网，代码见文末的参考链接。\n而对于填色图 plt.contourf，缺测值区域不会被画出，会直接露出 axes 的背景色，所以可以通过修改背景色来表示缺测的颜色。聊胜于无的是，还可以通过 corner_mask 参数指定缺测区域的边角画法。不过一般还是建议经过插值等填补处理后再来画填色图吧。\n参考链接 NumPy 的文档和一些文章\nThe numpy.ma module\nNEP 12 — Missing Data Functionality in NumPy\nNEP 26 — Summary of Missing Data NEPs and discussion\npandas 的文档\nWorking with missing data\nWiki 和 stack overflow 上的讨论\nNaN Wikipedia\nWhy are Numpy masked arrays useful?\nMatplotlib 的缺测\nPlotting masked and NaN values\nImage Masked\nContour Demo\n","date":"2020-07-13","permalink":"https://zhajiman.github.io/post/numpy_missing_value/","tags":["numpy"],"title":"NumPy 系列：缺测值处理"},{"content":"这里简单介绍一下立体角的概念。\n定义 在考虑辐射传输问题时，为了度量源点对某一范围的视场角大小，我们引入立体角的概念。通常教材上的定义如下图所示，一个半径为 $r$ 的球体，用顶点与球心重合的圆锥去截球面，截取的球面积 $A$ 的大小除以半径的平方，即是立体角。\n公式为 $$ \\Omega = \\frac{A}{r^2} $$ 立体角的单位是无量纲的球面度（steradian），简写为 sr。实际上，除了用圆锥，你用任何几何体去截都行，只要能在球面上划分一块连续的区域，其面积为 $A$，那么便可以通过上面的定义式计算出其立体角。\n关键在于，立体角的本质是一段封闭曲线对于观察点所张开的角度，只有这个角度是重要的，毕竟我们引入立体角就是为了获得这个视场角。而封闭曲线围成的曲面具体是什么形状，其实并不重要。就如同下图所示。\n于是，为了从球面立体角的定义式出发计算任意曲面的立体角，把曲面的面微元都投影到以矢径为半径的球面上，投影面积除以矢径长度的平方后，再做面积分，式子为 $$ \\Omega = \\iint_S \\frac{\\vec{e_r} \\cdot d\\vec{S}}{r^2} $$ 其中 $r$ 为观察点到曲面上一点的距离，$\\vec{e_r}$ 为矢径 $\\vec{r}$ 的单位矢量，$d\\vec{S}$ 为曲面 $S$ 上法向的微元面积，$\\vec{e_r} \\cdot d\\vec{S}$ 即意味着把面积微元投影到球面上，于是根据球面的面积微元表达式，得到立体角的微元表达式 $$ d\\Omega = \\frac{\\vec{e_r} \\cdot d\\vec{S}}{r^2} = \\frac{dS_0}{r^2} = \\frac{r^2 sin\\theta d\\theta d\\varphi}{r^2} = sin\\theta d\\theta d\\varphi $$ 其中 $\\theta$ 为天顶角，$\\varphi$ 为方位角。从这个表达式可以看出，立体角的大小与 $r$ 无关，而只与曲面张成的角度（即 $\\theta$ 和 $\\varphi$ 的范围）有关，也就是说，给定一个角度张成的锥体，其中截取的任意形状、任意距离的曲面的空间角都相等。若观察点被封闭曲面包围，对全空间积分，很容易得到 $$ \\Omega = \\iint d\\Omega = \\int_{0}^{2\\pi} \\int_{0}^{\\pi} sin\\theta d\\theta d\\varphi = 4\\pi $$ 即封闭曲面内任一点所张成的立体角的大小为 $4\\pi$。这一结果还可以从球面的例子来验证，球面面积为 $4\\pi r^2$，除以 $r^2$ 后得球心处的立体角为 $4\\pi$。\n有这样的可能，曲面对于 $\\vec{r}$ 来说不是单值的，即曲面在空间中绕来绕去发生了重叠。此时立体角的公式依然成立，因为一旦曲面发生重叠，立体角锥一定会穿过曲面三次，其中两次计算的立体角由于投影面积的方向性会抵消，只剩下穿过一次的结果。这种情况的证明可见于电磁学教材上（虽然这种情况我们也完全不用管就是了）。\n一个例子：两个相隔较远物体互相张成的立体角 两个任意形状的几何体 $A$ 和 $B$，相距为 $R$。图示如下\n设物体 $B$ 对 物体 $A$ 中心张成的立体角为 $\\Omega_B$，物体 $A$ 对 物体 $B$ 中心张成的立体角为 $\\Omega_A$。这个张角的范围是从一个物体中心向另一个物体表面做切线得到的。根据定义式，有 $$ \\Omega = \\iint_S \\frac{\\vec{e_r} \\cdot d\\vec{S}}{r^2} $$ $\\vec{r}$ 为物体中心到另一个物体表面的矢径。当两个物体相隔很远，$R$ 远大于它们自身的长度尺度时，$\\vec{r}$ 的长度变动很小，长度近似等于 $R$ ，其方向变动也很小，方向近似不变，与两物体中心连线平行。这一近似可以用照射到地球的太阳光近乎平行的事实来说明。如下图所示\n太阳光从太阳出发时是从中心往外辐射的，但由于日地距离远大于太阳和地球的尺度，到达地球的太阳光近乎是平行光。我们把这里的太阳光换成矢径 $\\vec{r}$，便能理解这一近似。于是有 $$ \\vec{r} \\approx \\vec{R} $$ $$ \\Omega \\approx \\iint_S \\frac{\\vec{e_R} \\cdot d\\vec{S}}{R^2} = \\frac{1}{R^2} \\iint_S \\vec{e_R} \\cdot d\\vec{S} = \\frac{S_0}{R^2} $$\n其中 $S_0$ 为物体表面在以 $\\vec{\\rm{e}_R}$ 为法向的平面上的投影面积。设物体 $A$ 和 物体 $B$ 的投影面积分别为 $S_A$ 和 $S_B$，最后可以得出它们互相张成的立体角 $$ \\Omega_A = \\frac{S_A}{R^2} $$ 同时易得等式 $$ S_A\\Omega_B = S_B\\Omega_A $$ $$ \\Omega_B = \\frac{S_B}{R^2} $$\n这个等式可以应用于辐射测量或雷达探测中，这里就不再赘述了。\n参考资料 Solid Angle Wikipedia\nSolid angle and projections\n","date":"2019-10-27","permalink":"https://zhajiman.github.io/post/solid_angle/","tags":["辐射"],"title":"立体角简介"},{"content":"那是谁 是谁 是谁\n那是炸鸡 炸鸡人 炸鸡人\n背负着快餐的名义\n舍弃了一切去战斗的男人\n炸鸡之腿是手枪腿\n炸鸡之块是原味鸡\n炸鸡之翼是麦辣翅\n炸鸡的特点是外脆里嫩\n将炸鸡之力 集于一身\n美味的英雄\n炸鸡人 炸鸡人\n","date":"2019-08-22","permalink":"https://zhajiman.github.io/post/fried_chicken_man/","tags":["nonsense"],"title":"炸 鸡 人"}]