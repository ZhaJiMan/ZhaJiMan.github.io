<!DOCTYPE html>
<html lang="en">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.84.4" />



<link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
<link rel="manifest" href="/icons/site.webmanifest">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">


<title>PyTorch 时间序列预测入门 - 炸鸡人博客</title>


<meta name="author" content="炸鸡人" />


<meta name="description" content="A minimal Hugo theme with nice theme color." />


<meta name="keywords" content="pytorch, 时间序列, 翻译" />


<meta property="og:title" content="PyTorch 时间序列预测入门" />
<meta name="twitter:title" content="PyTorch 时间序列预测入门" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://zhajiman.github.io/post/pytorch_time_series_tutorial/" /><meta property="og:description" content="


最近学习用 PyTorch 做时间序列预测，发现只有 TensorFlow 官网的教程 把时间窗口的选取和模型的设置讲得直观易懂，故改编如下。本人也只是入门水平，翻译错误之处还请指正。

本文是利用深度学习做时间序列预测的入门教程，用到的模型包括卷积神经网络（CNN）和循环神经网络（RNN）。全文分为两大部分，又可以细分为：

预测单个时间步：

预测一个特征。
预测所有特征。


预测多个时间步：

单发预测：模型跑一次输出所有时间步的结果。
自回归：每次输出一个时间步的预测，再把结果喂给模型得到下一步的预测。



本文用到的数据和 notebook 可以在 GitHub 仓库 找到。" />
<meta name="twitter:description" content="


最近学习用 PyTorch 做时间序列预测，发现只有 TensorFlow 官网的教程 把时间窗口的选取和模型的设置讲得直观易懂，故改编如下。本人也只是入门水平，翻译错误之处还请指正。

本文是利用深度学习做时间序列预测的入门教程，用到的模型包括卷积神经网络（CNN）和循环神经网络（RNN）。全文分为两大部分，又可以细分为：

预测单个时间步：

预测一个特征。
预测所有特征。


预测多个时间步：

单发预测：模型跑一次输出所有时间步的结果。
自回归：每次输出一个时间步的预测，再把结果喂给模型得到下一步的预测。



本文用到的数据和 notebook 可以在 GitHub 仓库 找到。" /><meta property="og:image" content="https://zhajiman.github.io/img/og.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://zhajiman.github.io/img/og.png" /><meta property="article:published_time" content="2022-10-15T00:00:00+00:00" /><meta property="article:modified_time" content="2022-10-15T00:00:00+00:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>




<link rel="stylesheet" href="https://zhajiman.github.io/assets/css/fuji.min.css" />









<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</head>

<body
  data-theme="light"
  data-theme-auto='false'
  >
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>

    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://zhajiman.github.io/">炸鸡人博客</a>
            
            <span class="title-sub">基本上无害</span>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://zhajiman.github.io/post/pytorch_time_series_tutorial/">PyTorch 时间序列预测入门</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2022-10-15</span>

<span><i class="iconfont icon-file-tray-sharp"></i>&nbsp;12481 words</span>

<span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/pytorch">pytorch</a>&nbsp;<a href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97">时间序列</a>&nbsp;<a href="/tags/%E7%BF%BB%E8%AF%91">翻译</a>&nbsp;</span>

        
        <span><i class="iconfont icon-time-sharp"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>&nbsp;views</span>
    </div>
    
    <div class="post-content markdown-body">
        <p><img class="img-zoomable" src="/pytorch_time_series_tutorial/xkcd.png" alt="xkcd" />
</p>
<blockquote>
<p>最近学习用 PyTorch 做时间序列预测，发现只有 <a href="https://www.tensorflow.org/tutorials/structured_data/time_series" target="_blank">TensorFlow 官网的教程</a> 把时间窗口的选取和模型的设置讲得直观易懂，故改编如下。本人也只是入门水平，翻译错误之处还请指正。</p>
</blockquote>
<p>本文是利用深度学习做时间序列预测的入门教程，用到的模型包括卷积神经网络（CNN）和循环神经网络（RNN）。全文分为两大部分，又可以细分为：</p>
<ul>
<li>预测单个时间步：
<ul>
<li>预测一个特征。</li>
<li>预测所有特征。</li>
</ul>
</li>
<li>预测多个时间步：
<ul>
<li>单发预测：模型跑一次输出所有时间步的结果。</li>
<li>自回归：每次输出一个时间步的预测，再把结果喂给模型得到下一步的预测。</li>
</ul>
</li>
</ul>
<p>本文用到的数据和 notebook 可以在 <a href="https://github.com/ZhaJiMan/pytorch_time_series_tutorial" target="_blank">GitHub 仓库</a> 找到。</p>
<h2 id="基本设置">基本设置</h2>
<pre><code class="language-Python">import numpy as np
import pandas as pd
from scipy.fft import rfft
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import seaborn as sns
%config InlineBackend.figure_format = 'retina'

plt.rcParams['figure.figsize'] = (8, 6)
plt.rcParams['axes.grid'] = False
</code></pre>
<p>之后的代码均在 Jupyter Notebook 中运行。</p>
<h2 id="天气数据集">天气数据集</h2>
<p>示例数据集采用马克斯普朗克生物地球化学研究所的 <a href="https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip" target="_blank">天气时间序列数据集</a>，点击链接就能下（不过需要翻墙），在本地解压后得到 CSV 表格。该数据集包含 14 个特征，例如气温、气压和湿度等。时间范围从 2009 年到 2016 年，采样分辨率为 10 分钟。</p>
<p>本教程只考虑逐小时的预测，因此这里通过跳步索引降采样得到逐小时的数据：</p>
<pre><code class="language-Python"># 从第5行开始, 每6条记录选中一条.
df = pd.read_csv('jena_climate_2009_2016.csv')[5::6]
df.index = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')
df.head()
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/table_1.png" alt="table_1" />
</p>
<p>画出其中一些特征的时间序列：</p>
<pre><code class="language-Python">plot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)']
plot_features = df[plot_cols]
plot_features.plot(subplots=True)

plot_features = df[plot_cols][:480]
plot_features.plot(subplots=True)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/features_timeseries_1.png" alt="features_timeseries_1" />
</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/features_timeseries_2.png" alt="features_timeseries_2" />
</p>
<h3 id="查看并清理数据">查看并清理数据</h3>
<p>接下来看看数据集的基本统计量：</p>
<pre><code class="language-Python">df.describe().transpose()
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/table_2.png" alt="table_2" />
</p>
<h4 id="风速">风速</h4>
<p>上表中最明显的就是风速 <code>wv (m/s)</code> 和最大风速 <code>max. wv (m/s)</code> 两个特征的最小值跑到了 <code>-9999</code>，意味着很可能有错。因为风速肯定是非负的，所以这里用零替换掉这些异常值：</p>
<pre><code class="language-Python">df['wv (m/s)'] = df['wv (m/s)'].clip(lower=0)
df['max. wv (m/s)'] = df['max. wv (m/s)'].clip(lower=0)
</code></pre>
<h3 id="特征工程">特征工程</h3>
<p>深入学习如何建模之前，需要理解你的数据，并保证传入模型的数据有合理的格式。</p>
<h4 id="风">风</h4>
<p>数据表格的最后一列是以角度为单位的风向。角度对模型来说并不是一个好的输入：0° 和 360° 在数值上差很多，但在几何上应该非常接近且可以平滑过渡。另外无风时（风速为零）风向不应该起作用。</p>
<p>目前风向和风速的联合分布如下图所示：</p>
<pre><code class="language-Python">plt.hist2d(df['wd (deg)'], df['wv (m/s)'], bins=(50, 50), vmin=0, vmax=400)
plt.colorbar()
plt.xlabel('Wind Direction [deg]')
plt.ylabel('Wind Velocity [m/s]')
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/wind_distribution_1.png" alt="wind_distribution_1" />
</p>
<p>如果将风向和风速转为风矢量的话，模型将更容易解读风数据：</p>
<pre><code class="language-Python">wv = df.pop('wv (m/s)')
max_wv = df.pop('max. wv (m/s)')

# 风向转为极坐标的风向, 单位转为弧度.
wd_rad = np.deg2rad(270 - df.pop('wd (deg)'))

# 计算风速的xy分量.
df['Wx'] = wv * np.cos(wd_rad)
df['Wy'] = wv * np.sin(wd_rad)

# 计算最大风速的xy分量.
df['max Wx'] = max_wv * np.cos(wd_rad)
df['max Wy'] = max_wv * np.sin(wd_rad)
</code></pre>
<p>转换后风速分量的联合分布更易于被模型解读：</p>
<pre><code class="language-Python">plt.hist2d(df['Wx'], df['Wy'], bins=(50, 50), vmin=0, vmax=400)
plt.colorbar()
plt.xlabel('Wind X [m/s]')
plt.ylabel('Wind Y [m/s]')
plt.axis('tight')
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/wind_distribution_2.png" alt="wind_distribution_2" />
</p>
<h4 id="时间">时间</h4>
<p>时间索引也非常有用，不过当然不是指字符串形式的。首先转换成秒：</p>
<pre><code class="language-Python">timestamp_s = df.index.map(pd.Timestamp.timestamp)
</code></pre>
<p>跟风向的情况类似，用秒表示的时间对模型来说用处也不大。我们注意到天气数据明显存在以日和年为周期的周期性，你可以用余弦和正弦函数来表示这些周期信号：</p>
<pre><code class="language-Python">day = 24 * 60 * 60
year = 365.2425 * day

df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))
df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))
df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))
df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))
</code></pre>
<pre><code class="language-Python">plt.plot(np.array(df['Day sin'])[:25])
plt.plot(np.array(df['Day cos'])[:25])
plt.xlabel('Time [h]')
plt.title('Time of day signal')
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/time_signal.png" alt="time_signal.png" />
</p>
<p>这些值给模型提供了最重要的频率特征，不过这需要你提前确定哪些频率是重要的。如果你缺少这一信息，可以考虑使用快速傅里叶变换（FFT）来找出这些频率。为了验证前面提出的日和年周期的假设，下面对气温序列应用 <code>scipy.fft.rfft</code>。图中 <code>1/year</code> 和 <code>1/day</code> 频率处有显著的峰值：</p>
<pre><code class="language-Python">fft = rfft(df['T (degC)'].to_numpy())
f_per_dataset = np.arange(0, len(fft))

n_samples_h = len(df['T (degC)'])
hours_per_year = 24 * 365.2524
years_per_dataset = n_samples_h / hours_per_year

f_per_year = f_per_dataset / years_per_dataset
plt.step(f_per_year, np.abs(fft))
plt.xscale('log')
plt.ylim(0, 400000)
plt.xlim([0.1, max(plt.xlim())])
plt.xticks([1, 365.2524], labels=['1/Year', '1/day'])
_ = plt.xlabel('Frequency (log scale)')
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/rfft.png" alt="rfft" />
</p>
<h3 id="数据划分">数据划分</h3>
<p>下面用 <code>(70%, 20%, 10%)</code> 的比例划分训练集、验证集和测试集。特别注意不要在划分前随机打乱数据，原因有两个：</p>
<ol>
<li>保证后续可以将数据切成许多由连续样本构成的窗口。</li>
<li>保证用于评估的验证集和测试集是在模型训练完后收集的，使评估结果更符合实际情况。</li>
</ol>
<pre><code class="language-Python">n = len(df)
i1 = int(n * 0.7)
i2 = int(n * 0.9)
train_df = df.iloc[:i1]
val_df = df.iloc[i1:i2]
test_df = df.iloc[i2:]

num_features = df.shape[1]
</code></pre>
<h3 id="标准化数据">标准化数据</h3>
<p>在训练神经网络前最好对特征进行放缩，而标准化就是放缩的常用手法：为每个特征减去平均值再除以标准差。平均值和标准差只能在训练集上计算，以防模型接触到验证集和测试集。</p>
<p>一个有争议的观点是：模型在训练阶段不应该接触训练集中的未来值，并且应该使用滑动平均来做标准化。本教程的重点并不在此，而且验证集和测试集的划分已经够你得出较为可信的预报评分了。所以方便起见，本教程只是简单做个平均。</p>
<pre><code class="language-Python">train_mean = train_df.mean()
train_std = train_df.std()

train_df = (train_df - train_mean) / train_std
val_df = (val_df - train_mean) / train_std
test_df = (test_df - train_mean) / train_std
</code></pre>
<p>现在我们来看一眼所有特征的分布。有些特征确实拖着长尾，但至少没有 <code>-9999</code> 的风速那种明显的错误。</p>
<pre><code class="language-Python">df_std = (df - train_mean) / train_std
df_std = df_std.melt(var_name='Column', value_name='Normalized')
plt.figure(figsize=(12, 6))
ax = sns.violinplot(x='Column', y='Normalized', data=df_std)
_ = ax.set_xticklabels(df.keys(), rotation=90)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/violinplot.png" alt="violinplot" />
</p>
<h2 id="数据分窗">数据分窗</h2>
<p>本教程中的模型基于连续样本构成的窗口来做预测。这种输入窗口的主要特征是：</p>
<ul>
<li>输入和标签窗口的宽度（即时间步数）。</li>
<li>输入和标签窗口间的时间偏移量。</li>
<li>哪些特征充当输入，哪些充当标签，哪些二者皆是。</li>
</ul>
<p>本教程会构造一系列模型（包括线性回归、DNN、CNN 和 RNN 模型），用它们做两类预测：</p>
<ul>
<li>单变量和多变量输出的预测。</li>
<li>单时间步和多时间步的预测。</li>
</ul>
<p>本节重点介绍如何实现数据分窗，以便在后续的所有模型中复用。</p>
<p>根据具体任务和模型类型的不同，你可能需要生成不同结构的数据窗口，下面列举几例：</p>
<ol>
<li>例如，已知 24 小时的历史数据，预测 24 小时后那个时刻的天气，你可能会定义这样的窗口：</li>
</ol>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/raw_window_24h.png" alt="raw_window_24h" />
</p>
<ol start="2">
<li>已知 6 小时的历史数据向后预测 1 小时，需要这样的窗口：</li>
</ol>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/raw_window_1h.png" alt="raw_window_1h" />
</p>
<p>本节剩下的部分会定义一个 <code>WindowGenerator</code> 类，它可以：</p>
<ol>
<li>处理上面那些图示中的下标索引和偏移。</li>
<li>将窗口中的特征划分为 <code>(features, labels)</code> 对。</li>
<li>画出窗口中的时间序列。</li>
<li>利用 PyTorch 的 <code>Dataset</code> 和 <code>DataLoader</code>，从训练集、验证集和测试集中生成批数据。</li>
</ol>
<h3 id="1-下标索引和偏移">1. 下标索引和偏移</h3>
<p>先从创建 <code>WindowGenerator</code> 类开始吧。<code>__init__</code> 方法包含了处理输入和标签下标索引相关的所有逻辑。这里还需要训练集、验证集和测试集的 <code>DataFrame</code>，后续用来生成 <code>torch.utils.data.DataLoader</code>。</p>
<pre><code class="language-Python">class WindowGenerator:
    def __init__(
        self, input_width, label_width, shift,
        train_df=train_df, val_df=val_df, test_df=test_df,
        label_columns=None
    ):
        # 存储原始数据.
        self.train_df = train_df
        self.val_df = val_df
        self.test_df = test_df
        
        # 找出标签列的下标索引.
        self.columns = train_df.columns
        if label_columns is None:
            self.label_columns = self.columns
        else:
            self.label_columns = pd.Index(label_columns)
        self.label_column_indices = [
            self.columns.get_loc(name) for name in self.label_columns
        ]
        
        # 计算窗口的参数.
        self.input_width = input_width
        self.label_width = label_width
        self.shift = shift
        self.total_window_size = input_width + shift
        
        self.input_slice = slice(input_width)
        self.input_indices = np.arange(input_width)
        
        self.label_start = self.total_window_size - label_width
        self.label_slice = slice(self.label_start, None)
        self.label_indices = np.arange(self.label_start, self.total_window_size)

    def __repr__(self):
        return '\n'.join([
            f'Total window size: {self.total_window_size}',
            f'Input indices: {self.input_indices}',
            f'Label indices: {self.label_indices}',
            f'Label column names(s): {self.label_columns.to_list()}'
        ])
</code></pre>
<p>下面的代码构造了本节开头图示中的两种窗口：</p>
<pre><code class="language-Python">w1 = WindowGenerator(
    input_width=24,
    label_width=1,
    shift=24,
    label_columns=['T (degC)']
)
w1
</code></pre>
<pre><code>Total window size: 48
Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
Label indices: [47]
Label column names(s): ['T (degC)']
</code></pre>
<pre><code class="language-Python">w2 = WindowGenerator(
    input_width=6,
    label_width=1,
    shift=1,
    label_columns=['T (degC)']
)
w2
</code></pre>
<pre><code>Total window size: 7
Input indices: [0 1 2 3 4 5]
Label indices: [6]
Label column names(s): ['T (degC)']
</code></pre>
<h3 id="2-划分">2. 划分</h3>
<p><code>split_window</code> 方法可以将一串连续的输入值转为由输入值组成的一个窗口，和由标签值组成的一个窗口。前面定义的 <code>w2</code> 会被分割成这个样子：</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/split_window.png" alt="split_window" />
</p>
<p>虽然上图并没有展示数据中 <code>features</code> 所在的维度，但 <code>split_window</code> 方法是可以正确处理 <code>label_columns</code> 的，所以既能用在单变量输出，也能用在多变量输出的例子里。</p>
<pre><code class="language-Python">def split_window(self, features):
    inputs = features[self.input_slice, :]
    labels = features[self.label_slice, self.label_column_indices]
    
    return inputs, labels

WindowGenerator.split_window = split_window
</code></pre>
<p>下面来试试：</p>
<pre><code class="language-Python">example_window = train_df.iloc[:w2.total_window_size].to_numpy()
example_inputs, example_labels = w2.split_window(example_window)

print('All shapes are: (time, features)')
print(f'Window shape: {example_window.shape}')
print(f'Inputs shape: {example_inputs.shape}')
print(f'Labels shape: {example_labels.shape}')
</code></pre>
<pre><code>All shapes are: (time, features)
Window shape: (7, 19)
Inputs shape: (6, 19)
Labels shape: (1, 1)
</code></pre>
<p>Pytorch 中数组的形状通常表现为：最外层的下标索引对应批大小的维度，中间的下标索引对应时间或空间（宽高）维，而最内层的下标索引对应每种特征。</p>
<p>上面代码的功能是，将宽 7 个时间步，每步含 19 个特征的窗口划分为宽 6 个时间步，含 19 个特征的输入窗口，和 1 个时间步宽，只含 1 个特征的标签窗口。因为 <code>w2</code> 在初始化时指定了 <code>label_columns=['T (degC)']</code>，所以标签窗口只含一个特征。本教程在起步阶段还是先搭一些预测单变量的模型。</p>
<h3 id="3-生成-dataloader">3. 生成 DataLoader</h3>
<p>先自定义一个 <code>TimeseriesDataset</code>，接收数组 <code>data</code> 并将其转为张量，以 <code>window</code> 作为窗口宽度。假设 <code>data</code> 形如 <code>(time, features)</code>，那么 <code>dataset[0]</code> 对应于 <code>data[0:window]</code>，<code>dataset[1]</code> 对应 <code>data[1:window+1]</code>，以此类推直至 <code>data[time-window:time]</code>，即 <code>dataset</code> 中共计 <code>time - window + 1</code> 个窗口。然后将 <code>split_window</code> 作为 <code>transform</code> 参数传入，将每个窗口切成 <code>(input_window, label_window)</code> 对，最后将 <code>dataset</code> 传给 <code>DataLoader</code>，在窗口的第一维堆叠出批维度。</p>
<pre><code class="language-Python">class TimeseriesDataset(Dataset):
    def __init__(self, data, window, transform=None):
        self.data = torch.tensor(data, dtype=torch.float)
        self.window = window
        self.transform = transform
    
    def __len__(self):
        return len(self.data) - self.window + 1
    
    def __getitem__(self, index):
        if index &lt; 0:
            index += len(self)
        features = self.data[index:index+self.window]
        if self.transform is not None:
            features = self.transform(features)
            
        return features

def make_dataloader(self, df):
    data = df.to_numpy()
    dataset = TimeseriesDataset(
        data=data,
        window=self.total_window_size,
        transform=self.split_window
    )
    dataloader = DataLoader(
        dataset=dataset,
        batch_size=32,
        shuffle=True
    )
    
    return dataloader

WindowGenerator.make_dataloader = make_dataloader
</code></pre>
<p>再定义一些类属性，能直接以 <code>DataLoader</code> 的形式获取 <code>WindowGenerator</code> 对象里存着的训练集、验证集和测试集的数据。为了方便测试和画图还加了个 <code>example</code> 属性，返回切好的一批窗口：</p>
<pre><code class="language-Python">@property
def train(self):
    return self.make_dataloader(self.train_df)

@property
def val(self):
    return self.make_dataloader(self.val_df)

@property
def test(self):
    return self.make_dataloader(self.test_df)
    
@property
def example(self):
    '''获取并缓存一个批次的(inputs, labels)窗口.'''
    result = getattr(self, '_example', None)
    if result is None:
        result = next(iter(self.train))
        self._example = result
    
    return result

WindowGenerator.train = train
WindowGenerator.val = val
WindowGenerator.test = test
WindowGenerator.example = example
</code></pre>
<p>现在你能用 <code>WindowGenerator</code> 对象获取 <code>DataLoader</code> 并轻松迭代整个数据集了。让我们看看迭代 <code>DataLoader</code> 时元素的形状：</p>
<pre><code class="language-Python">example_inputs, example_labels = w2.example
print(f'Inputs shape (batch, time, features): {example_inputs.shape}')
print(f'Labels shape (batch, time, features): {example_labels.shape}')
</code></pre>
<pre><code>Inputs shape (batch, time, features): torch.Size([32, 6, 19])
Labels shape (batch, time, features): torch.Size([32, 1, 1])
</code></pre>
<h3 id="4-画图">4. 画图</h3>
<p>为了简单展示一下分出来的窗口，这里定义画图方法：</p>
<pre><code class="language-Python">def tensor_to_numpy(tensor):
    '''张量转为NumPy数组.'''
    if tensor.requires_grad:
        tensor = tensor.detach()
    if tensor.device.type == 'cuda':
        tensor = tensor.cpu()

    return tensor.numpy()

def plot(self, model=None, plot_col='T (degC)', max_subplots=3):
    # 从缓存的一批窗口中获取输入和标签.
    inputs, labels = self.example
    if model is not None:
        model.eval()
        with torch.no_grad():
            predictions = tensor_to_numpy(model(inputs))
    inputs = tensor_to_numpy(inputs)
    labels = tensor_to_numpy(labels)
    
    plt.figure(figsize=(12, 8))
    plot_col_index = self.columns.get_loc(plot_col)
    max_n = min(max_subplots, len(inputs))
    
    # 子图数量不超过max_subplots和批大小.
    for n in range(max_n):
        plt.subplot(max_n, 1, n + 1)
        plt.ylabel(f'{plot_col} [normed]')
        plt.plot(
            self.input_indices, inputs[n, :, plot_col_index],
            label='Inputs', marker='.', zorder=1
        )
        
        # 标签窗口里没有plot_col时则跳过.
        try:
            label_col_index = self.label_columns.get_loc(plot_col)
        except KeyError:
            continue
    
        plt.scatter(
            self.label_indices, labels[n, :, label_col_index],
            c='#2ca02c', edgecolors='k', label='Labels'
        )
        
        # 画出预测值.
        if model is not None:
            plt.scatter(
                self.label_indices, predictions[n, :, label_col_index],
                c='#ff7f0e', s=64, marker='X', edgecolors='k',
                label='Predictions'
            )
        
        if n == 0:
            plt.legend()
    
    plt.xlabel('Time [h]')

WindowGenerator.plot = plot
</code></pre>
<p>该方法会按时间对齐输入序列、标签序列和（之后产生的）预测序列：</p>
<pre><code class="language-Python">w2.plot()
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/plot_method_1.png" alt="plot_method_1" />
</p>
<p>默认画气温，也可以画其它特征，但作为例子的 <code>w2</code> 窗口中只有气温这一个标签特征。</p>
<pre><code class="language-Python">w2.plot(plot_col='p (mbar)')
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/plot_method_2.png" alt="plot_method_2" />
</p>
<h3 id="总结">总结</h3>
<p>前面 <code>WindowGenerator</code> 类的定义分布得比较零散，为了方便使用，这里把定义总结在一起：</p>
<pre><code class="language-Python">class WindowGenerator:
    def __init__(
        self, input_width, label_width, shift,
        train_df=train_df, val_df=val_df, test_df=test_df,
        label_columns=None
    ):
        # 存储原始数据.
        self.train_df = train_df
        self.val_df = val_df
        self.test_df = test_df
        
        # 找出标签列的下标索引.
        self.columns = train_df.columns
        if label_columns is None:
            self.label_columns = self.columns
        else:
            self.label_columns = pd.Index(label_columns)
        self.label_column_indices = [
            self.columns.get_loc(name) for name in self.label_columns
        ]
        
        # 计算窗口的参数.
        self.input_width = input_width
        self.label_width = label_width
        self.shift = shift
        self.total_window_size = input_width + shift
        
        self.input_slice = slice(input_width)
        self.input_indices = np.arange(input_width)
        
        self.label_start = self.total_window_size - label_width
        self.label_slice = slice(self.label_start, None)
        self.label_indices = np.arange(self.label_start, self.total_window_size)

    def __repr__(self):
        return '\n'.join([
            f'Total window size: {self.total_window_size}',
            f'Input indices: {self.input_indices}',
            f'Label indices: {self.label_indices}',
            f'Label column names(s): {self.label_columns.to_list()}'
        ])

    def split_window(self, features):
        inputs = features[self.input_slice, :]
        labels = features[self.label_slice, self.label_column_indices]
        
        return inputs, labels

    def make_dataloader(self, df):
        data = df.to_numpy()
        dataset = TimeseriesDataset(
            data=data,
            window=self.total_window_size,
            transform=self.split_window
        )
        dataloader = DataLoader(
            dataset=dataset,
            batch_size=32,
            shuffle=True
        )
        
        return dataloader

    @property
    def train(self):
        return self.make_dataloader(self.train_df)

    @property
    def val(self):
        return self.make_dataloader(self.val_df)

    @property
    def test(self):
        return self.make_dataloader(self.test_df)
        
    @property
    def example(self):
        '''获取并缓存一个批次的(inputs, labels)窗口.'''
        result = getattr(self, '_example', None)
        if result is None:
            result = next(iter(self.train))
            self._example = result
        
        return result

    def plot(self, model=None, plot_col='T (degC)', max_subplots=3):
        # 从缓存的一批窗口中获取输入和标签.
        inputs, labels = self.example
        if model is not None:
            model.eval()
            with torch.no_grad():
                predictions = tensor_to_numpy(model(inputs))
        inputs = tensor_to_numpy(inputs)
        labels = tensor_to_numpy(labels)
        
        plt.figure(figsize=(12, 8))
        plot_col_index = self.columns.get_loc(plot_col)
        max_n = min(max_subplots, len(inputs))
        
        # 子图数量不超过max_subplots和批大小.
        for n in range(max_n):
            plt.subplot(max_n, 1, n + 1)
            plt.ylabel(f'{plot_col} [normed]')
            plt.plot(
                self.input_indices, inputs[n, :, plot_col_index],
                label='Inputs', marker='.', zorder=1
            )
            
            # 标签窗口里没有plot_col时则跳过.
            try:
                label_col_index = self.label_columns.get_loc(plot_col)
            except KeyError:
                continue
        
            plt.scatter(
                self.label_indices, labels[n, :, label_col_index],
                c='#2ca02c', edgecolors='k', label='Labels'
            )
            
            # 画出预测值.
            if model is not None:
                plt.scatter(
                    self.label_indices, predictions[n, :, label_col_index],
                    c='#ff7f0e', s=64, marker='X', edgecolors='k',
                    label='Predictions'
                )
            
            if n == 0:
                plt.legend()
        
        plt.xlabel('Time [h]')
</code></pre>
<h2 id="单步模型">单步模型</h2>
<p>基于这种数据，最简单的模型就是利用当前时间步的信息预测下个时间步（一小时后）的一个特征值。所以我们先来搭个预测下小时 <code>T (degC)</code> 的模型。</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/narrow_window.png" alt="narrow_window" />
</p>
<p>设置一个 <code>WindowGenerator</code> 对象，构造如图所示的单步的 <code>(input, label)</code> 对：</p>
<pre><code class="language-Python">single_step_window = WindowGenerator(
    input_width=1,
    label_width=1,
    shift=1,
    label_columns=['T (degC)']
)
single_step_window
</code></pre>
<pre><code>Total window size: 2
Input indices: [0]
Label indices: [1]
Label column names(s): ['T (degC)']
</code></pre>
<p>这个 <code>window</code> 对象能用训练集、验证集和测试集的数据创建 <code>DataLoader</code> 对象，方便你在不同批次的数据上进行迭代：</p>
<pre><code class="language-Python">example_inputs, example_labels = single_step_window.example
print(f'Inputs shape (batch, time, features): {example_inputs.shape}')
print(f'Labels shape (batch, time, features): {example_labels.shape}')
</code></pre>
<pre><code>Inputs shape (batch, time, features): torch.Size([32, 1, 19])
Labels shape (batch, time, features): torch.Size([32, 1, 1])
</code></pre>
<h3 id="模型类">模型类</h3>
<p>TensorFlow 的 <code>tf.keras.Model</code> 类可以通过 <code>fit</code> 方法一键训练，通过 <code>evaluate</code> 方法在测试集上评估模型的表现。而 PyTorch 的 <code>torch.nn.Module</code> 类只提供计算前向传播的功能，在数据集上进行训练和评估的功能需要手工实现。这里仿照 Keras 定义一个 <code>Model</code> 类，实现训练和评估相关的方法：</p>
<pre><code class="language-Python">class Model(nn.Module):
    def compile(self, loss_fn, metric_fn, optimizer=None):
        self.loss_fn = loss_fn
        self.metric_fn = metric_fn
        self.optimizer = optimizer
    
    def train_epoch(self, dataloader):
        self.train()
        avg_loss = 0
        avg_metric = 0
        
        for x, y in dataloader:
            yp = self(x)
            loss = self.loss_fn(y, yp)
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            avg_loss += loss.item()
            avg_metric += self.metric_fn(y, yp).item()
        
        num_batches = len(dataloader)
        avg_loss /= num_batches
        avg_metric /= num_batches
        
        return avg_loss, avg_metric
    
    @torch.no_grad()
    def evaluate(self, dataloader):
        self.eval()
        avg_loss = 0
        avg_metric = 0
        
        for x, y in dataloader:
            yp = self(x)
            avg_loss += self.loss_fn(y, yp).item()
            avg_metric += self.metric_fn(y, yp).item()
        
        num_batchs = len(dataloader)
        avg_loss /= num_batchs
        avg_metric /= num_batchs
        
        return avg_loss, avg_metric
</code></pre>
<p>本教程后续还会用早停法提前终止训练，这里也实现一个。原理是 <code>EarlyStopping</code> 类会记录训练过程中出现过的最小损失 <code>min_loss</code>，当传入的 <code>loss</code> 连续 <code>patience</code> 次超过 <code>min_loss + min_delta</code> 时，认为后续 <code>loss</code> 只会不断增长，该停止训练了。</p>
<pre><code class="language-Python">class EarlyStopping:
    def __init__(self, min_delta=0, patience=1):
        self.min_delta = min_delta
        self.patience = patience
        self.min_loss = np.inf
        self.counter = 0

    def __call__(self, loss):
        if loss &lt; self.min_loss:
            self.min_loss = loss
            self.counter = 0
        if loss &gt; (self.min_loss + self.min_delta):
            self.counter += 1
        
        return self.counter &gt;= self.patience
</code></pre>
<h3 id="基准模型">基准模型</h3>
<p>在构造一个可训练的模型之前，最好先整一个基准模型作为对照组，稍后还可以跟更复杂的模型比较预测表现。</p>
<p>我们的第一个任务是根据当前所有特征的数值预测一小时后的气温。注意当前所有特征包含当前气温。因此，可以让模型直接返回当前气温作为预测值，即预言气温不会变。考虑到现实中逐小时的气温变化并不算大，这一基准模型还是比较合理的。当然，如果你要预测更远的未来的话，这个基准模型就很不靠谱了。</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/baseline.png" alt="baseline" />
</p>
<pre><code class="language-Python">class Baseline(Model):
    def __init__(self, label_index=None):
        super(Baseline, self).__init__()
        self.label_index = label_index
    
    def forward(self, inputs):
        if self.label_index is None:
            return inputs
        else:
            return inputs[:, :, [self.label_index]]
</code></pre>
<p>实例化对象并直接在验证集和测试集上评估预测表现：</p>
<pre><code class="language-Python">baseline = Baseline(label_index=df.columns.get_loc('T (degC)'))
baseline.compile(loss_fn=nn.MSELoss(), metric_fn=nn.L1Loss())

val_performance = {}
test_performance = {}
loss, metric = baseline.evaluate(single_step_window.val)
print(f'loss: {loss:.4f} - metric: {metric:.4f}')

val_performance['Baseline'] = baseline.evaluate(single_step_window.val)
test_performance['Baseline'] = baseline.evaluate(single_step_window.test)
</code></pre>
<pre><code>loss: 0.0128 - metric: 0.0784
</code></pre>
<p>其中评分 <code>metric</code> 使用的是 <code>nn.L1Loss</code>，即平均绝对误差（MAE）。虽然这几行代码把一些评分打印在了屏幕上，但很难让我们对模型的好坏有直观的认识。<code>WindowGenerator</code> 有画出输入、标签和预测结果的方法，但其输入和输出的时间步都只有一步，恐怕画不出什么有意思的图像。</p>
<p>为了方便演示，这里创建一个更宽的 <code>WindowGenerator</code> 对象，每次能切出连续 24 小时的输入和标签序列。虽然结构与 <code>single_step_window</code> 不同，但 <code>wide_window</code> 并没有改变模型的预测方式，模型依旧用一个小时的输入预测下一小时的气温。这种情况下 <code>time</code> 维就好比 <code>batch</code> 维：不同时间步的预测都是独立产生的，互不干涉。</p>
<pre><code class="language-Python">wide_window = WindowGenerator(
    input_width=24,
    label_width=24,
    shift=1,
    label_columns=['T (degC)']
)
wide_window
</code></pre>
<pre><code>Total window size: 25
Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
Label indices: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
Label column names(s): ['T (degC)']
</code></pre>
<p>扩展后的窗口能直接传给 <code>baseline</code> 模型做预测，不需要修改任何代码。这是因为初始化窗口对象时指定了输入和标签序列的时间步数相同，并且基准模型只是从输入中抽取特定几列特征作为输出，没有什么复杂的前向过程：</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/last_window.png" alt="last_window" />
</p>
<pre><code class="language-Python">print('Inputs shape:', wide_window.example[0].shape)
print('Output shape:', baseline(wide_window.example[0]).shape)
</code></pre>
<pre><code>Inputs shape: torch.Size([32, 24, 19])
Output shape: torch.Size([32, 24, 1])
</code></pre>
<p>然后来画基准模型的预测结果，发现预测结果就是输入序列右移一个小时而已：</p>
<pre><code class="language-Python">wide_window.plot(baseline)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/baseline_plot.png" alt="baseline_plot" />
</p>
<p>关于上图的一些解释：</p>
<ul>
<li>蓝色的 <code>Inputs</code> 折线表示每个时间步上输入的气温。注意模型是输入了所有特征的，但这里只画出了气温而已。</li>
<li>绿色的 <code>Labels</code> 散点表示目标时间步上的真实气温。这些点显示在预测时刻而非输入时刻，这也是 <code>Labels</code> 的时间范围要比 <code>Inputs</code> 的范围向右偏移一步的原因。</li>
<li>橙色的 <code>Predictions</code> 散点是模型在输出时间步上的预测值。如果 <code>Predictions</code> 的叉叉和 <code>Labels</code> 的圆点重合，说明模型完美预测了气温。</li>
</ul>
<h3 id="线性模型">线性模型</h3>
<p>对于单步预测的任务，最简单的可训练模型就是在输入和输出之间插入线性变换层。此时输出完全由当前时间步的输入决定：</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/narrow_window.png" alt="narrow_window" />
</p>
<p>一层 <code>torch.nn.Linear</code>就是一个线性模型，只会对输入的最后一维进行变换，将形如 <code>(batch, time, in_features)</code> 的数据变成 <code>(batch, time, out_features)</code> 的形状。且不同的 <code>batch</code> 和 <code>time</code> 下标都对应一个线性变换，这些变换之间互相独立：</p>
<pre><code class="language-Python">class Linear(Model):
    def __init__(self, in_features, out_features):
        super(Linear, self).__init__()
        self.layer = nn.Linear(in_features, out_features)
    
    def forward(self, inputs):
        return self.layer(inputs)
</code></pre>
<pre><code class="language-Python">linear = nn.Linear(num_features, 1)
print('Input shape:', single_step_window.example[0].shape)
print('Output shape:', linear(single_step_window.example[0]).shape)
</code></pre>
<pre><code>Input shape: torch.Size([32, 1, 19])
Output shape: torch.Size([32, 1, 1])
</code></pre>
<p>本教程将会训练很多模型，因此把训练流程打包成一个函数：</p>
<pre><code class="language-Python">def compile_and_fit(model, window, max_epochs=20, patience=2):
    model.compile(
        loss_fn=nn.MSELoss(),
        metric_fn=nn.L1Loss(),
        optimizer=optim.Adam(model.parameters())
    )
    early_stopping = EarlyStopping(patience=patience)

    for t in range(max_epochs):
        loss, metric = model.train_epoch(window.train)
        val_loss, val_metric = model.evaluate(window.val)
        info = ' - '.join([
            f'[Epoch {t + 1}/{max_epochs}]',
            f'loss: {loss:.4f}',
            f'metric: {metric:.4f}',
            f'val_loss: {val_loss:.4f}',
            f'val_metric: {val_metric:.4f}'
        ])
        print(info)
        if early_stopping(val_loss):
            break
</code></pre>
<p>训练线性模型并评估其表现：</p>
<pre><code class="language-Python">compile_and_fit(linear, single_step_window)

val_performance['Linear'] = linear.evaluate(single_step_window.val)
test_performance['Linear'] = linear.evaluate(single_step_window.test)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.1688 - metric: 0.2229 - val_loss: 0.0176 - val_metric: 0.1021
[Epoch 2/20] - loss: 0.0151 - metric: 0.0901 - val_loss: 0.0098 - val_metric: 0.0734
[Epoch 3/20] - loss: 0.0096 - metric: 0.0718 - val_loss: 0.0089 - val_metric: 0.0701
[Epoch 4/20] - loss: 0.0092 - metric: 0.0704 - val_loss: 0.0089 - val_metric: 0.0703
[Epoch 5/20] - loss: 0.0092 - metric: 0.0704 - val_loss: 0.0088 - val_metric: 0.0690
[Epoch 6/20] - loss: 0.0092 - metric: 0.0702 - val_loss: 0.0090 - val_metric: 0.0711
[Epoch 7/20] - loss: 0.0091 - metric: 0.0700 - val_loss: 0.0088 - val_metric: 0.0696
</code></pre>
<p>跟 <code>baseline</code> 模型类似，<code>linear</code> 模型也可以直接用在宽窗口产生的批量数据上，这种用法能让模型在一串连续的时间步上给出一组互相独立的预测。此时 <code>time</code> 维的功能跟 <code>batch</code> 维类似。每个时间步上的预测互不影响。</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/wide_window.png" alt="wide_window" />
</p>
<pre><code class="language-Python">print('Input shape:', wide_window.example[0].shape)
print('Output shape:', linear(wide_window.example[0]).shape)
</code></pre>
<pre><code>Input shape: torch.Size([32, 24, 19])
Output shape: torch.Size([32, 24, 1])
</code></pre>
<p>注意 <code>wide_window</code> 只是方便一次性预测多个时间步和画图，训练和评估还得用 <code>single_step_window</code>，不然相当于批大小从 <code>batch_size</code> 增大为 <code>batch_size * input_width</code>。</p>
<p>下面画出几例 <code>linear</code> 在 <code>wide_window</code> 上的预测结果，可见大部分时刻线性模型的效果比直接用输入时刻的气温当预测更好，但在有些时刻要更差些：</p>
<pre><code class="language-Python">wide_window.plot(linear)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/linear_plot.png" alt="linear_plot" />
</p>
<p>线性模型的一大好处就是易于解读，因为线性层的权重就是多变量线性回归里的系数。你可以对所有输入特征对应的权重做可视化：</p>
<pre><code class="language-Python">x = np.arange(num_features)
_, ax = plt.subplots()
ax.bar(x, tensor_to_numpy(linear.layer.weight[0, :]))
ax.set_xticks(x)
ax.set_xticklabels(train_df.columns, rotation=90)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/linear_weight.png" alt="linear_weight" />
</p>
<p>有时训练出来的模型里 <code>T (degC)</code> 的权重都不是最高的（例如本图就是……），这算是随机初始化权重所带来的一个毛病。</p>
<h3 id="密集层模型">密集层模型</h3>
<p>在我们尝试多时间步输入的模型之前，有必要先来测试一下更深更强力的单时间步输入模型。</p>
<p>下面这个模型跟 <code>linear</code> 很像，不过在输入和输出之间多加了两层线性层和激活函数：</p>
<pre><code class="language-Python">class Dense(Model):
    def __init__(self, in_features, out_features):
        super(Dense, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(in_features, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, out_features)
        )
    
    def forward(self, x):
        return self.layers(x)

dense = Dense(num_features, 1)
compile_and_fit(dense, single_step_window)

val_performance['Dense'] = dense.evaluate(single_step_window.val)
test_performance['Dense'] = dense.evaluate(single_step_window.test)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.0189 - metric: 0.0783 - val_loss: 0.0074 - val_metric: 0.0624
[Epoch 2/20] - loss: 0.0076 - metric: 0.0625 - val_loss: 0.0074 - val_metric: 0.0636
[Epoch 3/20] - loss: 0.0072 - metric: 0.0606 - val_loss: 0.0075 - val_metric: 0.0615
</code></pre>
<h3 id="多步密集层模型">多步密集层模型</h3>
<p>单时间步的模型无法获知输入在时间维度上的“上下文信息”，即模型看不到输入特征随时间的变化情况。为此模型在做预测时应该获取多个时间步的输入：</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/conv_window.png" alt="conv_window" />
</p>
<p><code>baseline</code>、<code>linear</code> 和 <code>dense</code> 模型都是单独处理每个时间步的输入，而这里要介绍的模型将会一次性接收多个时间步的输入并输出单个时间步的标签。创建窗口时注意 <code>shift</code> 参数指的是输入输出窗口末尾间的偏移量：</p>
<pre><code class="language-Python">CONV_WIDTH = 3
conv_window = WindowGenerator(
    input_width=CONV_WIDTH,
    label_width=1,
    shift=1,
    label_columns=['T (degC)']
)
conv_window
</code></pre>
<pre><code>Total window size: 4
Input indices: [0 1 2]
Label indices: [3]
Label column names(s): ['T (degC)']
</code></pre>
<pre><code class="language-Python">conv_window.plot()
ax = plt.gcf().axes[0]
ax.set_title('Given 3 hours of inputs, predict 1 hour into the future.')
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/conv_window_plot.png" alt="conv_window_plot" />
</p>
<p>要实现这种模型，可以在 <code>dense</code> 模型前加一层 <code>nn.Flatten</code>，将形如 <code>(batch, time, features)</code> 的输入摊平成 <code>(batch, time * features)</code>，此处 <code>time=3</code>，这样一来前三个时间步的特征都会输进模型。网络最后一层再通过 <code>torch.Tensor.reshape</code> 将 <code>(batch, 1 * 1)</code> 的输出转为 <code>(batch, 1, 1)</code> 。</p>
<pre><code class="language-Python">class nnReshape(nn.Module):
    def __init__(self, *shape):
        super(nnReshape, self).__init__()
        self.shape = shape

    def forward(self, x):
        return x.reshape(self.shape)

class MultiStepDense(Model):
    def __init__(self, in_features, out_features):
        super(MultiStepDense, self).__init__()
        self.layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(CONV_WIDTH * in_features, 32),
            nn.ReLU(),
            nn.Linear(32, 32),
            nn.ReLU(),
            nn.Linear(32, out_features),
            nnReshape(-1, 1, out_features)
        )
    
    def forward(self, x):
        return self.layers(x)
</code></pre>
<pre><code class="language-Python">multi_step_dense = MultiStepDense(num_features, 1)
print('Input shape:', conv_window.example[0].shape)
print('Output shape:', multi_step_dense(conv_window.example[0]).shape)
</code></pre>
<pre><code>Input shape: torch.Size([32, 3, 19])
Output shape: torch.Size([32, 1, 1])
</code></pre>
<pre><code class="language-Python">compile_and_fit(multi_step_dense, conv_window)
val_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.val)
test_performance['Multi step dense'] = multi_step_dense.evaluate(conv_window.test)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.0270 - metric: 0.0899 - val_loss: 0.0070 - val_metric: 0.0598
[Epoch 2/20] - loss: 0.0073 - metric: 0.0609 - val_loss: 0.0066 - val_metric: 0.0583
[Epoch 3/20] - loss: 0.0070 - metric: 0.0596 - val_loss: 0.0062 - val_metric: 0.0552
[Epoch 4/20] - loss: 0.0068 - metric: 0.0585 - val_loss: 0.0076 - val_metric: 0.0619
[Epoch 5/20] - loss: 0.0066 - metric: 0.0577 - val_loss: 0.0067 - val_metric: 0.0580
</code></pre>
<pre><code class="language-Python">conv_window.plot(multi_step_dense)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multistepdense_plot.png" alt="multistepdense_plot" />
</p>
<p>该模型的主要缺点是，要求输入和标签数据的窗口宽度必须为 <code>CONV_WIDTH</code> 和 <code>1</code>，而 <code>wide_window</code> 对象划分的数据就不能传入。</p>
<pre><code class="language-Python">print('Input shape:', wide_window.example[0].shape)
try:
    print('Output shape:', multi_step_dense(wide_window.example[0]).shape)
except Exception as e:
    print(f'\n{type(e).__name__}:{e}')
</code></pre>
<pre><code>Input shape: torch.Size([32, 24, 19])

RuntimeError:mat1 and mat2 shapes cannot be multiplied (32x456 and 57x32)
</code></pre>
<p>下一节的卷积模型将会解决这个问题。</p>
<h3 id="卷积神经网络">卷积神经网络</h3>
<p>卷积层（<code>torch.nn.Conv1d</code>）同样可以用多个时间步的输入做预测。下面是跟 <code>multi_step_dense</code> <strong>相同</strong>的模型，不过用卷积改写了一下。改动在于：</p>
<ul>
<li>用 <code>nn.Conv1d</code> 替换掉了 <code>nn.Flatten</code> 和 <code>nn.Linear</code>。不过因为 PyTorch 的一维卷积是对输入的最后一维做的，而这里希望对时间维，也就是第二维做，因此要用 <code>torch.Tensor.transpose</code> 转置一下。</li>
<li>最后不需要 <code>nnReshape</code> 层了，因为前面都保留了时间维。</li>
</ul>
<pre><code class="language-Python">class nnTranspose(nn.Module):
    def __init__(self, dim0, dim1):
        super(nnTranspose, self).__init__()
        self.dim0 = dim0
        self.dim1 = dim1

    def forward(self, x):
        return x.transpose(self.dim0, self.dim1)

class ConvModel(Model):
    def __init__(self, in_features, out_features):
        super(ConvModel, self).__init__()
        self.layers = nn.Sequential(
            nnTranspose(1, 2),
            nn.Conv1d(in_features, 32, CONV_WIDTH),
            nnTranspose(1, 2),
            nn.Linear(32, 32),
            nn.ReLU(),
            nn.Linear(32, out_features)
        )
    
    def forward(self, x):
        return self.layers(x)
</code></pre>
<p>在 <code>example</code> 批上检查一下模型输出的形状：</p>
<pre><code class="language-Python">conv_model = ConvModel(num_features, 1)
print('Input shape:', conv_window.example[0].shape)
print('Output shape:', conv_model(conv_window.example[0]).shape)
</code></pre>
<pre><code>Input shape: torch.Size([32, 3, 19])
Output shape: torch.Size([32, 1, 1])
</code></pre>
<p>在 <code>conv_window</code> 上训练并评估，其表现应该和 <code>multi_step_dense</code> 非常接近。</p>
<pre><code class="language-Python">compile_and_fit(conv_model, conv_window)
val_performance['Conv'] = conv_model.evaluate(conv_window.val)
test_performance['Conv'] = conv_model.evaluate(conv_window.test)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.0158 - metric: 0.0789 - val_loss: 0.0081 - val_metric: 0.0667
[Epoch 2/20] - loss: 0.0075 - metric: 0.0618 - val_loss: 0.0081 - val_metric: 0.0676
[Epoch 3/20] - loss: 0.0073 - metric: 0.0608 - val_loss: 0.0068 - val_metric: 0.0596
[Epoch 4/20] - loss: 0.0070 - metric: 0.0594 - val_loss: 0.0068 - val_metric: 0.0589
[Epoch 5/20] - loss: 0.0070 - metric: 0.0593 - val_loss: 0.0061 - val_metric: 0.0547
[Epoch 6/20] - loss: 0.0068 - metric: 0.0583 - val_loss: 0.0062 - val_metric: 0.0551
</code></pre>
<p><code>conv_model</code> 和 <code>multi_step_dense</code> 的差异在于，<code>conv_model</code> 能在任意长度的输入上进行预测。一维卷积相当于滑动窗口版的线性层：</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/wide_conv_window.png" alt="wide_conv_window" />
</p>
<p>如果你在更宽的输入上跑一下模型，会自动产生更宽的输出：</p>
<pre><code class="language-Python">print('Wide window')
print('Input shape:', wide_window.example[0].shape)
print('Labels shape:', wide_window.example[1].shape)
print('Output shape:', conv_model(wide_window.example[0]).shape)
</code></pre>
<pre><code>Wide window
Input shape: torch.Size([32, 24, 19])
Labels shape: torch.Size([32, 24, 1])
Output shape: torch.Size([32, 22, 1])
</code></pre>
<p>可以看到输出的长度要比输入短两格，这是因为无论输入有多宽，开头 <code>CONV_WIDTH</code> 个输入总得用来“启动”第一个预测，导致预测的长度总是比输入短 <code>CONV_WIDTH - 1</code> 格。以前面的图示为例，<code>t=0,1,2</code> 时刻的输入产生 <code>t=3</code> 的预测，<code>t=1,2,3</code> 时刻的输入产生 <code>t=4</code> 的预测，后面以此类推。但 <code>t=1</code> 的预测需要 <code>t=-2,-1,0</code> 的输入，<code>t=2</code> 的预测需要 <code>t=-1,0,1</code> 的输入，而这里并没有 <code>t=-2,-1</code> 的数据，因此预测序列要比输入序列短两格。</p>
<p>显然 <code>wide_window</code> 产生的标签窗口并不满足这一点，所以如果想在更宽的输入上训练或画图，就需要 <code>WindowGenerator</code> 对象切出的标签窗口比输入窗口右移一步，同时开头要短两格：</p>
<pre><code class="language-Python">LABEL_WIDTH = 24
INPUT_WIDTH = LABEL_WIDTH + (CONV_WIDTH - 1)
wide_conv_window = WindowGenerator(
    input_width=INPUT_WIDTH,
    label_width=LABEL_WIDTH,
    shift=1,
    label_columns=['T (degC)']
)
wide_conv_window
</code></pre>
<pre><code>Total window size: 27
Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25]
Label indices: [ 3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26]
Label column names(s): ['T (degC)']
</code></pre>
<pre><code class="language-Python">print('Wide conv window')
print('Input shape:', wide_conv_window.example[0].shape)
print('Labels shape:', wide_conv_window.example[1].shape)
print('Output shape:', conv_model(wide_conv_window.example[0]).shape)
</code></pre>
<pre><code>Wide conv window
Input shape: torch.Size([32, 26, 19])
Labels shape: torch.Size([32, 24, 1])
Output shape: torch.Size([32, 24, 1])
</code></pre>
<p>现在终于能在更宽的窗口上画出模型的预测结果了。注意前三个输入值后才出现第一个预测值，因为每步预测都源于前三步的输入：</p>
<pre><code class="language-Python">wide_conv_window.plot(conv_model)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/convmodel_plot.png" alt="convmodel_plot" />
</p>
<h3 id="循环神经网络">循环神经网络</h3>
<p>循环神经网络（RNN）是一种特别适合处理时间序列的神经网络。RNN 会一步接一步地处理时间序列，同时用一个内部状态记录每步输入的信息。你可以在 <a href="https://www.tensorflow.org/text/tutorials/text_generation" target="_blank">用 RNN 生成文本</a> 的教程和 <a href="https://www.tensorflow.org/guide/keras/rnn" target="_blank">用 Keras 学循环神经网络（RNN）</a> 的指南里学到更多。</p>
<p>本教程用到的 RNN 层是长短期记忆（LSTM，<code>torch.nn.LSTM</code>）。将一个序列输入 LSTM 后能得到等长的序列和最后一个 cell 的状态（hidden state 和 cell state），用来做单步预测的话大致有两种思路：</p>
<ol>
<li>输出序列的最后一步已经包含了输入序列所有时间步的信息，输出序列前面每步的运算相当于是在预热（warmup）模型。这最后一步可以用作下一时刻的预测。</li>
</ol>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/lstm_1_window.png" alt="lstm_1_window" />
</p>
<ol start="2">
<li>直接以输出序列作为下一时刻的预测值，相当于一次性给出了与输出序列等长的预测序列。</li>
</ol>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/lstm_many_window.png" alt="lstm_many_window" />
</p>
<p>下面用思路 2 进行演示：</p>
<pre><code class="language-Python">class LstmModel(Model):
    def __init__(self, in_features, out_features):
        super(LstmModel, self).__init__()
        self.lstm = nn.LSTM(in_features, 32, batch_first=True)
        self.linear = nn.Linear(32, out_features)
    
    def forward(self, x):
        output, _ = self.lstm(x)
        return self.linear(output)
</code></pre>
<blockquote>
<p>提示：这种用法可能使模型的表现变差，因为输出的第一步并只用到了输入第一步的信息，后面的输出里才会逐渐积累历史输入的信息。因此输出序列前几步的表现可能不比简单的 <code>linear</code> 和 <code>dense</code> 之类的模型强。</p>
</blockquote>
<pre><code class="language-Python">lstm_model = LstmModel(num_features, 1)
print('Input shape:', wide_window.example[0].shape)
print('Output shape:', lstm_model(wide_window.example[0]).shape)
</code></pre>
<pre><code>Input shape: torch.Size([32, 24, 19])
Output shape: torch.Size([32, 24, 1])
</code></pre>
<pre><code class="language-Python">compile_and_fit(lstm_model, wide_window)
val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)
test_performance['LSTM'] = lstm_model.evaluate(wide_window.test)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.0311 - metric: 0.0900 - val_loss: 0.0063 - val_metric: 0.0552
[Epoch 2/20] - loss: 0.0063 - metric: 0.0548 - val_loss: 0.0058 - val_metric: 0.0526
[Epoch 3/20] - loss: 0.0059 - metric: 0.0529 - val_loss: 0.0056 - val_metric: 0.0517
[Epoch 4/20] - loss: 0.0057 - metric: 0.0521 - val_loss: 0.0056 - val_metric: 0.0513
[Epoch 5/20] - loss: 0.0056 - metric: 0.0514 - val_loss: 0.0055 - val_metric: 0.0508
[Epoch 6/20] - loss: 0.0055 - metric: 0.0510 - val_loss: 0.0055 - val_metric: 0.0512
[Epoch 7/20] - loss: 0.0054 - metric: 0.0505 - val_loss: 0.0057 - val_metric: 0.0519
</code></pre>
<pre><code class="language-Python">wide_window.plot(lstm_model)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/lstmmodel_plot.png" alt="lstmmodel_plot" />
</p>
<h3 id="预测表现">预测表现</h3>
<p>在本教程的数据集上，这些模型的表现应该一个比一个强：</p>
<pre><code class="language-Python">def plot_performance(val_performance, test_performance, ylabel):
    x = np.arange(len(val_performance))
    width = 0.3
    metric_index = 1
    val_mae = [v[metric_index] for v in val_performance.values()]
    test_mae = [v[metric_index] for v in test_performance.values()]

    plt.ylabel(ylabel)
    plt.bar(x - 0.17, val_mae, width, label='Validation')
    plt.bar(x + 0.17, test_mae, width, label='Test')
    plt.xticks(ticks=x, labels=val_performance.keys(), rotation=45)
    plt.legend()

plot_performance(
    val_performance, test_performance,
    ylabel='mean_absolute_error [T (degC), normalized]'
)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/single_step_performance.png" alt="single_step_performance" />
</p>
<pre><code class="language-Python">for name, value in test_performance.items():
    print(f'{name:18s}: {value[1]:0.4f}')
</code></pre>
<pre><code>Baseline          : 0.0853
Linear            : 0.0668
Dense             : 0.0625
Multi step dense  : 0.0578
Conv              : 0.0592
LSTM              : 0.0534
</code></pre>
<blockquote>
<p>译注：卷积神经网络一节说 <code>Conv</code> 的性能应该和 <code>Multi step dense</code> 持平，但图里不是明显更差么……</p>
</blockquote>
<h3 id="多变量输出模型">多变量输出模型</h3>
<p>目前为止介绍的模型都是在单个时间步上预测单个特征 <code>T (degC)</code>。</p>
<p>要让这些模型输出多个特征其实非常简单，只需要修改输出层的特征数，让 <code>WindowGenerator</code> 产生的标签包含所有特征即可：</p>
<pre><code class="language-Python">single_step_window = WindowGenerator(input_width=1, label_width=1, shift=1)
wide_window = WindowGenerator(input_width=24, label_width=24, shift=1)

example_inputs, example_labels = wide_window.example
print(f'Inputs shape (batch, time, features): {example_inputs.shape}')
print(f'Labels shape (batch, time, features): {example_labels.shape}')
</code></pre>
<pre><code>Inputs shape (batch, time, features): torch.Size([32, 24, 19])
Labels shape (batch, time, features): torch.Size([32, 24, 19])
</code></pre>
<p>现在 <code>example_labels</code> 中 <code>features</code> 一维的长度和 <code>example_inputs</code> 相同了（以前是 <code>1</code>，现在是 <code>19</code>）。</p>
<h4 id="基准模型-1">基准模型</h4>
<p>缺省 <code>label_index</code> 参数即可重复所有输入特征：</p>
<pre><code class="language-Python">baseline = Baseline()
baseline.compile(loss_fn=nn.MSELoss(), metric_fn=nn.L1Loss())
</code></pre>
<pre><code class="language-Python">val_performance = {}
test_performance = {}
loss, metric = baseline.evaluate(single_step_window.val)
print(f'loss: {loss:.4f} - metric: {metric:.4f}')

val_performance['Baseline'] = baseline.evaluate(single_step_window.val)
test_performance['Baseline'] = baseline.evaluate(single_step_window.test)
</code></pre>
<pre><code>loss: 0.0892 - metric: 0.1593
</code></pre>
<h4 id="密集层模型-1">密集层模型</h4>
<pre><code class="language-Python">dense = Dense(num_features, num_features)
compile_and_fit(dense, single_step_window)
val_performance['Dense'] = dense.evaluate(single_step_window.val)
test_performance['Dense'] = dense.evaluate(single_step_window.test)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.1069 - metric: 0.1845 - val_loss: 0.0726 - val_metric: 0.1448
[Epoch 2/20] - loss: 0.0724 - metric: 0.1430 - val_loss: 0.0717 - val_metric: 0.1405
[Epoch 3/20] - loss: 0.0711 - metric: 0.1394 - val_loss: 0.0705 - val_metric: 0.1382
[Epoch 4/20] - loss: 0.0703 - metric: 0.1370 - val_loss: 0.0710 - val_metric: 0.1383
[Epoch 5/20] - loss: 0.0699 - metric: 0.1355 - val_loss: 0.0698 - val_metric: 0.1342
[Epoch 6/20] - loss: 0.0692 - metric: 0.1335 - val_loss: 0.0690 - val_metric: 0.1328
[Epoch 7/20] - loss: 0.0689 - metric: 0.1326 - val_loss: 0.0681 - val_metric: 0.1307
[Epoch 8/20] - loss: 0.0686 - metric: 0.1315 - val_loss: 0.0687 - val_metric: 0.1314
[Epoch 9/20] - loss: 0.0684 - metric: 0.1308 - val_loss: 0.0683 - val_metric: 0.1316
</code></pre>
<h4 id="rnn-模型">RNN 模型</h4>
<pre><code class="language-Python">%%time
lstm_model = LstmModel(num_features, num_features)
compile_and_fit(lstm_model, wide_window)
val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)
test_performance['LSTM'] = lstm_model.evaluate(wide_window.test)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.1299 - metric: 0.2077 - val_loss: 0.0688 - val_metric: 0.1384
[Epoch 2/20] - loss: 0.0662 - metric: 0.1318 - val_loss: 0.0643 - val_metric: 0.1277
[Epoch 3/20] - loss: 0.0638 - metric: 0.1262 - val_loss: 0.0633 - val_metric: 0.1253
[Epoch 4/20] - loss: 0.0628 - metric: 0.1240 - val_loss: 0.0627 - val_metric: 0.1234
[Epoch 5/20] - loss: 0.0622 - metric: 0.1227 - val_loss: 0.0624 - val_metric: 0.1227
[Epoch 6/20] - loss: 0.0618 - metric: 0.1218 - val_loss: 0.0623 - val_metric: 0.1225
[Epoch 7/20] - loss: 0.0615 - metric: 0.1213 - val_loss: 0.0624 - val_metric: 0.1218
[Epoch 8/20] - loss: 0.0613 - metric: 0.1209 - val_loss: 0.0625 - val_metric: 0.1217
CPU times: total: 12.7 s
Wall time: 2min 23s
</code></pre>
<h4 id="高级方法残差连接">高级方法：残差连接</h4>
<p>前面的基准模型利用了数据集这样一个特性：序列中相邻两步间的差异并不是很大。而其它需要训练的模型首先会随机初始化权重参数，然后才在训练中逐渐学到输出值相比前一步只变化了一点的事实。虽然你可以通过微调初始化方法来解决这一问题，但更简单的做法是把这种关系纳入模型结构中。</p>
<p>不直接预测下一步的数值而预测下一步的改变量，在时间序列建模中是很常见的策略。类似的，深度学习中的 <a href="https://arxiv.org/abs/1512.03385" target="_blank">残差神经网络</a>（ResNets）就是一种将每层输出加到模型累积结果里的结构。</p>
<p>改变很小这一特性，就是这样被利用起来的。</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/residual.png" alt="residual" />
</p>
<p>本质上讲，该结构相当于把模型初始化到跟 <code>Baseline</code> 一样的状态。对于当下的预测任务，该结构能使模型收敛更快，稍稍提高模型的性能。除此之外，该结构还能结合本教程提到的其它模型使用。</p>
<p>这里结合 LSTM 进行演示，注意用到了 <code>torch.nn.init.zeros_</code> 将 LSTM 最后一层的权重置零，以确保训练刚开始时预测出的改变量足够小，并且不会盖过残差连接的效果。因为置零仅对最后一层进行，所以不必担心梯度会出现 symmetry-breaking 的问题。</p>
<pre><code class="language-Python">class ResidualWrapper(Model):
    def __init__(self, model):
        super(ResidualWrapper, self).__init__()
        self.model = model
    
    def forward(self, x):
        dx = self.model(x)
        return x + dx
</code></pre>
<pre><code class="language-Python">%%time
lstm_model = LstmModel(num_features, num_features)
nn.init.zeros_(lstm_model.linear.weight) # 直接修改参数的话需要用no_grad包裹.
residual_lstm = ResidualWrapper(lstm_model)

compile_and_fit(residual_lstm, wide_window)
val_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val)
test_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.0658 - metric: 0.1238 - val_loss: 0.0631 - val_metric: 0.1193
[Epoch 2/20] - loss: 0.0620 - metric: 0.1181 - val_loss: 0.0624 - val_metric: 0.1182
[Epoch 3/20] - loss: 0.0609 - metric: 0.1169 - val_loss: 0.0622 - val_metric: 0.1178
[Epoch 4/20] - loss: 0.0603 - metric: 0.1163 - val_loss: 0.0619 - val_metric: 0.1174
[Epoch 5/20] - loss: 0.0598 - metric: 0.1159 - val_loss: 0.0619 - val_metric: 0.1173
[Epoch 6/20] - loss: 0.0593 - metric: 0.1155 - val_loss: 0.0623 - val_metric: 0.1175
CPU times: total: 15.4 s
Wall time: 1min 55s
</code></pre>
<h4 id="预测表现-1">预测表现</h4>
<p>下面是这些多变量输出模型的总体表现：</p>
<pre><code class="language-Python">plot_performance(
    val_performance, test_performance,
    ylabel='MAE (average over all outputs)'
)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multi_output_performance.png" alt="multi_output_performance" />
</p>
<pre><code class="language-Python">for name, value in test_performance.items():
    print(f'{name:15s}: {value[1]:0.4f}')
</code></pre>
<pre><code>Baseline       : 0.1633
Dense          : 0.1332
LSTM           : 0.1237
Residual LSTM  : 0.1188
</code></pre>
<p>以上评分取模型所有输出的平均值。</p>
<h2 id="多步模型">多步模型</h2>
<p>前面几节的单变量输出和多变量输出模型都只能预测一个时间步，即一小时后。本节将介绍如何将这些模型扩展成能预测多个时间步的版本。</p>
<p>在做多步预测时，模型需要学习预测未来一段时间范围的值。因此多步模型不像单步模型那样只能给出未来一个时刻的预测，而是会输出一段预测序列。为此大致有两种方法：</p>
<ul>
<li>单发预测（single-shot），即一次性预测整段时间序列。</li>
<li>自回归预测（autoregressive），即模型做单步预测后，再把这步结果作为输入喂给模型，得到下下步的预测，以此类推。</li>
</ul>
<p>本节的模型将在输出时间步上预测<strong>所有特征</strong>（即多变量输出）。</p>
<p>对于多步模型，训练数据同样由每小时的样本组成。不同的是模型将学习用过去 24 小时的输入预测未来 24 小时的特征。</p>
<p>下面的 <code>Window</code> 对象会从数据集中生成我们需要的切片：</p>
<pre><code class="language-Python">OUT_STEPS = 24
multi_window = WindowGenerator(
    input_width=24,
    label_width=OUT_STEPS,
    shift=OUT_STEPS
)
multi_window.plot()
multi_window
</code></pre>
<pre><code>Total window size: 48
Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
Label indices: [24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47]
Label column names(s): ['p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'Wx', 'Wy', 'max Wx', 'max Wy', 'Day sin', 'Day cos', 'Year sin', 'Year cos']
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multi_window_plot.png" alt="multi_window_plot" />
</p>
<h3 id="基准模型-2">基准模型</h3>
<p>对该任务来说一个简单的基准模型就是把输入最后一步的数值重复 <code>OUT_STEPS</code> 次：</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multistep_last.png" alt="multistep_last" />
</p>
<pre><code class="language-Python">class MultiStepLastBaseline(Model):
    def __init__(self):
        super(MultiStepLastBaseline, self).__init__()
    
    def forward(self, x):
        return x[:, -1:, :].tile(1, OUT_STEPS, 1)

last_baseline = MultiStepLastBaseline()
last_baseline.compile(loss_fn=nn.MSELoss(), metric_fn=nn.L1Loss())

multi_val_performance = {}
multi_test_performance = {}

multi_val_performance['Last'] = last_baseline.evaluate(multi_window.val)
multi_test_performance['Last'] = last_baseline.evaluate(multi_window.test)
multi_window.plot(last_baseline)
</code></pre>
<pre><code>loss: 0.6286 - metric: 0.5007
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multistepbaseline_plot.png" alt="multistepbaseline_plot" />
</p>
<p>因为预测任务是用过去 24 小时预测未来 24 小时，两段时间正好都是一天的长度，所以另一个简单的方法是假设明天和今天的时间序列差不多，直接重复今天的序列作为预测：</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multistep_repeat.png" alt="multistep_repeat" />
</p>
<pre><code class="language-Python">class RepeatBaseline(Model):
    def __init__(self):
        super(RepeatBaseline, self).__init__()
    
    def forward(self, x):
        return x

repeat_baseline = RepeatBaseline()
repeat_baseline.compile(loss_fn=nn.MSELoss(), metric_fn=nn.L1Loss())
loss, metric = repeat_baseline.evaluate(multi_window.val)
print(f'loss: {loss:.4f} - metric: {metric:.4f}')

multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)
multi_test_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test)
multi_window.plot(repeat_baseline)
</code></pre>
<pre><code>loss: 0.4271 - metric: 0.3959
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/repeatbaseline_plot.png" alt="repeatbaseline_plot" />
</p>
<h3 id="单发模型">单发模型</h3>
<p>一个更高级点的方法是使用“单发”模型，即模型会一次性预测未来的整段序列。</p>
<p>将 <code>torch.nn.Linear</code> 的输出特征数设为 <code>OUT_STEPS * features</code> 即可轻松实现这种模型，只是模型的最后一层需要将输出变形成 <code>(batch, OUT_STEPS, features)</code> 的形状。</p>
<h4 id="线性模型-1">线性模型</h4>
<p>一个用输入序列最后一步做预测的简单的线性模型表现会比前面两种基准模型要好，但好也好不到哪里去。该模型会把单步输入线性投影成 <code>OUT_STEPS</code> 步的预测结果，因此只能捕捉到序列行为里低维的部分，很可能主要靠时间在一天或一年中的位置来做预测。</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multistep_dense.png" alt="multistep_dense" />
</p>
<pre><code class="language-Python">class MultiLinearModel(Model):
    def __init__(self, in_features, out_features):
        super(MultiLinearModel, self).__init__()
        self.layers = nn.Sequential(
            # Shape =&gt; [batch, 1, OUT_STEPS * features]
            nn.Linear(in_features, OUT_STEPS * out_features),
            # Shape =&gt; [batch, OUT_STEPS, features]
            nnReshape(-1, OUT_STEPS, out_features)
        )
        nn.init.zeros_(self.layers[0].weight)
    
    def forward(self, x):
        # Shape [batch, time, features] =&gt; [batch, 1, features]
        return self.layers(x[:, -1:, :])

multi_linear_model = MultiLinearModel(num_features, num_features)
compile_and_fit(multi_linear_model, multi_window)

multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)
multi_test_performance['Linear'] = multi_linear_model.evaluate(multi_window.test)
multi_window.plot(multi_linear_model)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.3296 - metric: 0.3990 - val_loss: 0.2585 - val_metric: 0.3232
[Epoch 2/20] - loss: 0.2570 - metric: 0.3127 - val_loss: 0.2558 - val_metric: 0.3059
[Epoch 3/20] - loss: 0.2562 - metric: 0.3074 - val_loss: 0.2559 - val_metric: 0.3055
[Epoch 4/20] - loss: 0.2561 - metric: 0.3072 - val_loss: 0.2550 - val_metric: 0.3049
[Epoch 5/20] - loss: 0.2560 - metric: 0.3071 - val_loss: 0.2558 - val_metric: 0.3052
[Epoch 6/20] - loss: 0.2560 - metric: 0.3071 - val_loss: 0.2555 - val_metric: 0.3050
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multilinearmodel_plot.png" alt="multilinearmodel_plot" />
</p>
<h4 id="密集层模型-2">密集层模型</h4>
<p>在线性模型的输入输出之间再加一层 <code>torch.nn.Linear</code> 能增强其表现，不过说到底还是只用了输入最后一步的信息。</p>
<pre><code class="language-Python">class MultiDenseModel(Model):
    def __init__(self, in_features, out_features):
        super(MultiDenseModel, self).__init__()
        self.dense = nn.Sequential(
            # Shape =&gt; [batch, 1, 512]
            nn.Linear(in_features, 512),
            nn.ReLU(),
            # Shape =&gt; [batch, 1, OUT_STEPS * features]
            nn.Linear(512, OUT_STEPS * out_features),
            # Shape =&gt; [batch, OUT_STEPS, features]
            nnReshape(-1, OUT_STEPS, out_features)
        )
        nn.init.zeros_(self.dense[2].weight)
    
    def forward(self, x):
        # Shape [batch, time, features] =&gt; [batch, 1, features]
        return self.dense(x[:, -1:, :])

multi_dense_model = MultiDenseModel(num_features, num_features)
compile_and_fit(multi_dense_model, multi_window)

multi_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)
multi_test_performance['Dense'] = multi_dense_model.evaluate(multi_window.test)
multi_window.plot(multi_dense_model)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.2346 - metric: 0.2957 - val_loss: 0.2249 - val_metric: 0.2854
[Epoch 2/20] - loss: 0.2204 - metric: 0.2826 - val_loss: 0.2217 - val_metric: 0.2845
[Epoch 3/20] - loss: 0.2169 - metric: 0.2798 - val_loss: 0.2208 - val_metric: 0.2835
[Epoch 4/20] - loss: 0.2146 - metric: 0.2781 - val_loss: 0.2179 - val_metric: 0.2806
[Epoch 5/20] - loss: 0.2130 - metric: 0.2771 - val_loss: 0.2194 - val_metric: 0.2824
[Epoch 6/20] - loss: 0.2118 - metric: 0.2763 - val_loss: 0.2194 - val_metric: 0.2809
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multidensemodel_plot.png" alt="multidensemodel_plot" />
</p>
<h4 id="cnn">CNN</h4>
<p>卷积模型会用固定宽度的历史输入做预测，因为能看到输入是如何随时间变化的，所以表现可能比密集层模型要好点：</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multistep_conv.png" alt="multistep_conv" />
</p>
<pre><code class="language-Python">CONV_WIDTH = 3
class MultiConvModel(Model):
    def __init__(self, in_features, out_features):
        super(MultiConvModel, self).__init__()
        self.layers = nn.Sequential(
            # Shape =&gt; [batch, 1, 256]
            nnTranspose(1, 2),
            nn.Conv1d(in_features, 256, CONV_WIDTH),
            nnTranspose(1, 2),
            nn.ReLU(),
            # Shape =&gt; [batch, 1, OUTSTEPS * features]
            nn.Linear(256, OUT_STEPS * out_features),
            # Shape =&gt; [batch, OUTSTEPS, features]
            nnReshape(-1, OUT_STEPS, out_features)
        )
        nn.init.zeros_(self.layers[4].weight)

    def forward(self, x):
        # Shape [batch, time, features] =&gt; [batch, CONV_WIDTH, features]
        return self.layers(x[:, -CONV_WIDTH:, :])

multi_conv_model = MultiConvModel(num_features, num_features)
compile_and_fit(multi_conv_model, multi_window)

multi_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)
multi_test_performance['Conv'] = multi_conv_model.evaluate(multi_window.test)
multi_window.plot(multi_conv_model)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.2360 - metric: 0.3019 - val_loss: 0.2223 - val_metric: 0.2874
[Epoch 2/20] - loss: 0.2188 - metric: 0.2853 - val_loss: 0.2193 - val_metric: 0.2854
[Epoch 3/20] - loss: 0.2149 - metric: 0.2819 - val_loss: 0.2174 - val_metric: 0.2838
[Epoch 4/20] - loss: 0.2122 - metric: 0.2797 - val_loss: 0.2176 - val_metric: 0.2846
[Epoch 5/20] - loss: 0.2101 - metric: 0.2781 - val_loss: 0.2169 - val_metric: 0.2822
[Epoch 6/20] - loss: 0.2083 - metric: 0.2765 - val_loss: 0.2164 - val_metric: 0.2816
[Epoch 7/20] - loss: 0.2069 - metric: 0.2755 - val_loss: 0.2124 - val_metric: 0.2782
[Epoch 8/20] - loss: 0.2058 - metric: 0.2744 - val_loss: 0.2161 - val_metric: 0.2825
[Epoch 9/20] - loss: 0.2049 - metric: 0.2738 - val_loss: 0.2154 - val_metric: 0.2806
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multiconvmodel_plot.png" alt="multiconvmodel_plot" />
</p>
<h4 id="rnn">RNN</h4>
<p>RNN 模型能够学习用长期的历史数据做预测。下面这个模型会用内部状态积累 24 小时历史数据里的信息，然后一次性预报接下来 24 小时。</p>
<p>我们在这种单发预测里只需要 LSTM 输出的最后一个时间步：</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multistep_lstm.png" alt="multistep_lstm" />
</p>
<pre><code class="language-Python">class MultiLstmModel(Model):
    def __init__(self, in_features, out_features):
        super(MultiLstmModel, self).__init__()
        self.lstm = nn.LSTM(in_features, 32, batch_first=True)
        self.linear = nn.Sequential(
            # Shape =&gt; [batch, 1, OUT_STEPS * features]
            nn.Linear(32, OUT_STEPS * out_features),
            # Shape =&gt; [batch, OUT_STEPS, features]
            nnReshape(-1, OUT_STEPS, out_features)
        )
        nn.init.zeros_(self.linear[0].weight)
    
    def forward(self, x):
        # Shape [batch, time, features] =&gt; [batch, time, 32]
        output, _ = self.lstm(x)
        # Shape =&gt; [batch, 1, 32]
        return self.linear(output[:, -1:, :])

multi_lstm_model = MultiLstmModel(num_features, num_features)
compile_and_fit(multi_lstm_model, multi_window)

multi_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)
multi_test_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test)
multi_window.plot(multi_lstm_model)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.2907 - metric: 0.3612 - val_loss: 0.2290 - val_metric: 0.3052
[Epoch 2/20] - loss: 0.2167 - metric: 0.2937 - val_loss: 0.2203 - val_metric: 0.2947
[Epoch 3/20] - loss: 0.2095 - metric: 0.2853 - val_loss: 0.2178 - val_metric: 0.2913
[Epoch 4/20] - loss: 0.2054 - metric: 0.2809 - val_loss: 0.2188 - val_metric: 0.2896
[Epoch 5/20] - loss: 0.2027 - metric: 0.2780 - val_loss: 0.2133 - val_metric: 0.2853
[Epoch 6/20] - loss: 0.2006 - metric: 0.2761 - val_loss: 0.2124 - val_metric: 0.2832
[Epoch 7/20] - loss: 0.1989 - metric: 0.2744 - val_loss: 0.2142 - val_metric: 0.2844
[Epoch 8/20] - loss: 0.1973 - metric: 0.2730 - val_loss: 0.2133 - val_metric: 0.2823
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multilstmmodel_plot.png" alt="multilstmmodel_plot" />
</p>
<h3 id="高级方法自回归模型">高级方法：自回归模型</h3>
<p>上面几个模型都是一次性预报一整段序列，但有些时候让模型把预测分解到每一步上会更有帮助。然后，模型在每一步上的输出都会回馈给模型自身，下一步的预测可以基于前一步的预测来做，就像经典的 <a href="https://arxiv.org/abs/1308.0850" target="_blank">用循环神经网络生成序列</a> 一样。</p>
<p>这类模型最显著的优点是，能够产生任意时间长度的预测。</p>
<p>你当然可以用教程前面训练好的单步模型做自回归的循环（输出单步再输回去），但这里我们想搭一个直接把自回归纳入训练过程的模型。</p>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multistep_autoregressive.png" alt="multistep_autoregressive" />
</p>
<h4 id="rnn-1">RNN</h4>
<p>本教程只演示自回归的 RNN 模型，但其套路可以沿用于其它单步输出的模型。该模型的基础和之前的单步 LSTM 模型一样：<code>torch.nn.LSTM</code> 输出序列的最后一步被 <code>torch.nn.Linear</code> 变换成了预测值。</p>
<pre><code class="language-Python">class FeedBack(Model):
    def __init__(self, num_features, out_steps):
        super(FeedBack, self).__init__()
        self.out_steps = out_steps
        self.lstm = nn.LSTM(num_features, 32, batch_first=True)
        self.linear = nn.Linear(32, num_features)
    
    def next(self, x, hc=None):
        # Shape [batch, time, features] =&gt; [batch, 1, features]
        output, hc = self.lstm(x, hc)
        prediction = self.linear(output[:, -1:, :])
        return prediction, hc
    
    def forward(self, x):
        predictions = []
        prediction, hc = self.next(x)
        predictions.append(prediction)
        
        for i in range(1, self.out_steps):
            prediction, hc = self.next(prediction, hc)
            predictions.append(prediction)
        # Shape =&gt; [batch, out_steps, features]
        predictions = torch.cat(predictions, dim=1)
        
        return predictions
</code></pre>
<p>最开始我们将形如 <code>(batch, time, features)</code> 的输入序列传入 <code>next</code> 方法中，得到形如 <code>(batch, 1, features)</code> 的单步预测值 <code>prediction</code> 和 LSTM 网络中最后一个 cell 的状态 <code>hc</code>。本来应该是把输入序列和 <code>prediction</code> 连起来作为新的输入序列传入 <code>next</code> 方法，得到下下步的预测。但得益于 <code>hc</code> 记录了之前序列里的信息，我们可以只把 <code>prediction</code> 和 <code>hc</code> 传入 <code>next</code> 方法，得到下下步的预测。如此循环操作 <code>out_steps</code> 步，再将这些结果用 <code>torch.cat</code> 连起来，便得到了形如 <code>(batch, out_steps, features)</code> 的输出序列。</p>
<pre><code class="language-Python">feedback = FeedBack(num_features, OUT_STEPS)
compile_and_fit(feedback, multi_window)

multi_val_performance['AR LSTM'] = feedback.evaluate(multi_window.val)
multi_test_performance['AR LSTM'] = feedback.evaluate(multi_window.test)
multi_window.plot(feedback)
</code></pre>
<pre><code>[Epoch 1/20] - loss: 0.3593 - metric: 0.4166 - val_loss: 0.2748 - val_metric: 0.3589
[Epoch 2/20] - loss: 0.2522 - metric: 0.3367 - val_loss: 0.2539 - val_metric: 0.3354
[Epoch 3/20] - loss: 0.2369 - metric: 0.3208 - val_loss: 0.2415 - val_metric: 0.3198
[Epoch 4/20] - loss: 0.2264 - metric: 0.3079 - val_loss: 0.2348 - val_metric: 0.3141
[Epoch 5/20] - loss: 0.2207 - metric: 0.3013 - val_loss: 0.2339 - val_metric: 0.3080
[Epoch 6/20] - loss: 0.2172 - metric: 0.2975 - val_loss: 0.2287 - val_metric: 0.3036
[Epoch 7/20] - loss: 0.2136 - metric: 0.2941 - val_loss: 0.2268 - val_metric: 0.3012
[Epoch 8/20] - loss: 0.2114 - metric: 0.2917 - val_loss: 0.2270 - val_metric: 0.3006
[Epoch 9/20] - loss: 0.2086 - metric: 0.2889 - val_loss: 0.2231 - val_metric: 0.2963
[Epoch 10/20] - loss: 0.2065 - metric: 0.2873 - val_loss: 0.2260 - val_metric: 0.3001
[Epoch 11/20] - loss: 0.2040 - metric: 0.2850 - val_loss: 0.2286 - val_metric: 0.3018
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/feedback_plot.png" alt="feedback_plot" />
</p>
<h3 id="预测表现-2">预测表现</h3>
<p>在多步预测的问题上，显然收益随模型复杂度的升高而递减：</p>
<pre><code class="language-Python">plot_performance(
    multi_val_performance, multi_test_performance,
    ylabel='MAE (average over all times and outputs)'
)
</code></pre>
<p><img class="img-zoomable" src="/pytorch_time_series_tutorial/multi_step_performance.png" alt="multi_step_performance" />
</p>
<p>本教程多变量输出模型一节的评分图是对所有输出特征做平均后画出来的，这里的评分也类似，不过进一步在输出的时间步上也做了平均。</p>
<pre><code class="language-Python">for name, value in multi_test_performance.items():
    print(f'{name:8s}: {value[1]:0.4f}')
</code></pre>
<pre><code>Last    : 0.5156
Repeat  : 0.3774
Linear  : 0.2983
Dense   : 0.2756
Conv    : 0.2744
LSTM    : 0.2730
AR LSTM : 0.2922
</code></pre>
<p>将密集层模型升级成卷积模型和循环模型后获得的收益仅有微小的几个百分点，自回归模型甚至表现变得更差了。因此，这些复杂的模型可能并不适用于<strong>该问题</strong>，但实际应用中模型是好是坏只有动手试一试才会知道，说不定这些模型有助于解决<strong>你的问题</strong>呢。</p>
<h2 id="下一步">下一步</h2>
<p>本教程简单介绍了如何使用 PyTorch 预测时间序列。更多相关教程请参阅：</p>
<ul>
<li><a href="https://book.douban.com/subject/35218199/" target="_blank">《机器学习实战：基于 Scikit-Learn、Keras 和 TensorFlow》</a> 第二版的 15 章。</li>
<li><a href="https://book.douban.com/subject/30293801/" target="_blank">《Python 深度学习》</a> 的第 6 章。</li>
<li><a href="https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187" target="_blank">Udacity 的 Intro to TensorFlow for Deep Learning</a> 的第 8 课，包含 <a href="https://github.com/tensorflow/examples/tree/master/courses/udacity_intro_to_tensorflow_for_deep_learning" target="_blank">练习题</a>。</li>
</ul>
<p>此外，虽然本教程只关注 PyTorch 内置的功能，但你也可以用来实现任何 <a href="https://otexts.com/fpp3/" target="_blank">经典的时间序列模型</a>。</p>
    </div>
</article>



<div class="post-comment" data-comment="utterances">
    <span class="post-comment-notloaded">
        <i class="iconfont icon-chatbox-ellipses-sharp"></i>&nbsp;Load comments
    </span>
    <script>
        function loadComment() {
            var commentArea = document.querySelector('.post-comment');
            var utterancesTheme = document.body.getAttribute('data-theme');
            if (utterancesTheme === 'auto') {
                utterancesTheme = window.matchMedia('(prefers-color-scheme: dark)').matches ? 'photon-dark' :
                    'github-light';
            } else {
                utterancesTheme = utterancesTheme === 'dark' ? 'photon-dark' : 'github-light';
            }
            var s = document.createElement('script');
            s.src = 'https://utteranc.es/client.js';
            s.setAttribute('repo', 'ZhaJiMan\/ZhaJiMan.github.io');
            s.setAttribute('issue-term', 'pathname');
            s.setAttribute('theme', utterancesTheme);
            s.setAttribute('crossorigin', 'anonymous');
            s.setAttribute('async', '');
            document.querySelector('.post-comment').appendChild(s);
            document.querySelector('span.post-comment-notloaded').setAttribute('style', 'display: none;');
        }
    </script>
</div>


            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/ZhaJiMan" target="_blank"><span>GitHub</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/cartopy/">cartopy</a>
            </span>
            
            <span>
                <a href="/tags/github/">github</a>
            </span>
            
            <span>
                <a href="/tags/hugo/">hugo</a>
            </span>
            
            <span>
                <a href="/tags/matplotlib/">matplotlib</a>
            </span>
            
            <span>
                <a href="/tags/net/">net</a>
            </span>
            
            <span>
                <a href="/tags/nonsense/">nonsense</a>
            </span>
            
            <span>
                <a href="/tags/numpy/">numpy</a>
            </span>
            
            <span>
                <a href="/tags/pandas/">pandas</a>
            </span>
            
            <span>
                <a href="/tags/python/">python</a>
            </span>
            
            <span>
                <a href="/tags/pytorch/">pytorch</a>
            </span>
            
            <span>
                <a href="/tags/shapely/">shapely</a>
            </span>
            
            <span>
                <a href="/tags/vim/">vim</a>
            </span>
            
            <span>
                <a href="/tags/%E5%8D%AB%E6%98%9F/">卫星</a>
            </span>
            
            <span>
                <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a>
            </span>
            
            <span>
                <a href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/">时间序列</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B5%8B%E5%9C%B0%E5%AD%A6/">测地学</a>
            </span>
            
            <span>
                <a href="/tags/%E7%BF%BB%E8%AF%91/">翻译</a>
            </span>
            
            <span>
                <a href="/tags/%E8%89%B2%E5%BD%A9/">色彩</a>
            </span>
            
            <span>
                <a href="/tags/%E8%B5%84%E6%BA%90/">资源</a>
            </span>
            
            <span>
                <a href="/tags/%E8%BE%90%E5%B0%84/">辐射</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#基本设置">基本设置</a></li>
    <li><a href="#天气数据集">天气数据集</a>
      <ul>
        <li><a href="#查看并清理数据">查看并清理数据</a></li>
        <li><a href="#特征工程">特征工程</a></li>
        <li><a href="#数据划分">数据划分</a></li>
        <li><a href="#标准化数据">标准化数据</a></li>
      </ul>
    </li>
    <li><a href="#数据分窗">数据分窗</a>
      <ul>
        <li><a href="#1-下标索引和偏移">1. 下标索引和偏移</a></li>
        <li><a href="#2-划分">2. 划分</a></li>
        <li><a href="#3-生成-dataloader">3. 生成 DataLoader</a></li>
        <li><a href="#4-画图">4. 画图</a></li>
        <li><a href="#总结">总结</a></li>
      </ul>
    </li>
    <li><a href="#单步模型">单步模型</a>
      <ul>
        <li><a href="#模型类">模型类</a></li>
        <li><a href="#基准模型">基准模型</a></li>
        <li><a href="#线性模型">线性模型</a></li>
        <li><a href="#密集层模型">密集层模型</a></li>
        <li><a href="#多步密集层模型">多步密集层模型</a></li>
        <li><a href="#卷积神经网络">卷积神经网络</a></li>
        <li><a href="#循环神经网络">循环神经网络</a></li>
        <li><a href="#预测表现">预测表现</a></li>
        <li><a href="#多变量输出模型">多变量输出模型</a></li>
      </ul>
    </li>
    <li><a href="#多步模型">多步模型</a>
      <ul>
        <li><a href="#基准模型-2">基准模型</a></li>
        <li><a href="#单发模型">单发模型</a></li>
        <li><a href="#高级方法自回归模型">高级方法：自回归模型</a></li>
        <li><a href="#预测表现-2">预测表现</a></li>
      </ul>
    </li>
    <li><a href="#下一步">下一步</a></li>
  </ul>
</nav></div>
</aside>

        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>Pages</h3>
        <ul>
            
            <li>
                <a href="/">Home</a>
            </li>
            
            <li>
                <a href="/archives/">Archives</a>
            </li>
            
            <li>
                <a href="/about/">About</a>
            </li>
            
            <li>
                <a href="/search/">Search</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>Links</h3>
        <ul>
            
            <li>
                <a href="https://github.com/ZhaJiMan" target="_blank"><span>GitHub</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>Tags</h3>
        <div>
            
            <span>
                <a href="/tags/cartopy/">cartopy</a>
            </span>
            
            <span>
                <a href="/tags/github/">github</a>
            </span>
            
            <span>
                <a href="/tags/hugo/">hugo</a>
            </span>
            
            <span>
                <a href="/tags/matplotlib/">matplotlib</a>
            </span>
            
            <span>
                <a href="/tags/net/">net</a>
            </span>
            
            <span>
                <a href="/tags/nonsense/">nonsense</a>
            </span>
            
            <span>
                <a href="/tags/numpy/">numpy</a>
            </span>
            
            <span>
                <a href="/tags/pandas/">pandas</a>
            </span>
            
            <span>
                <a href="/tags/python/">python</a>
            </span>
            
            <span>
                <a href="/tags/pytorch/">pytorch</a>
            </span>
            
            <span>
                <a href="/tags/shapely/">shapely</a>
            </span>
            
            <span>
                <a href="/tags/vim/">vim</a>
            </span>
            
            <span>
                <a href="/tags/%E5%8D%AB%E6%98%9F/">卫星</a>
            </span>
            
            <span>
                <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a>
            </span>
            
            <span>
                <a href="/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/">时间序列</a>
            </span>
            
            <span>
                <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B5%8B%E5%9C%B0%E5%AD%A6/">测地学</a>
            </span>
            
            <span>
                <a href="/tags/%E7%BF%BB%E8%AF%91/">翻译</a>
            </span>
            
            <span>
                <a href="/tags/%E8%89%B2%E5%BD%A9/">色彩</a>
            </span>
            
            <span>
                <a href="/tags/%E8%B5%84%E6%BA%90/">资源</a>
            </span>
            
            <span>
                <a href="/tags/%E8%BE%90%E5%B0%84/">辐射</a>
            </span>
            
        </div>
    </div>
    
    
    
    <div class="sidebar-item sidebar-toc">
        <h3>Table of Contents</h3>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#基本设置">基本设置</a></li>
    <li><a href="#天气数据集">天气数据集</a>
      <ul>
        <li><a href="#查看并清理数据">查看并清理数据</a></li>
        <li><a href="#特征工程">特征工程</a></li>
        <li><a href="#数据划分">数据划分</a></li>
        <li><a href="#标准化数据">标准化数据</a></li>
      </ul>
    </li>
    <li><a href="#数据分窗">数据分窗</a>
      <ul>
        <li><a href="#1-下标索引和偏移">1. 下标索引和偏移</a></li>
        <li><a href="#2-划分">2. 划分</a></li>
        <li><a href="#3-生成-dataloader">3. 生成 DataLoader</a></li>
        <li><a href="#4-画图">4. 画图</a></li>
        <li><a href="#总结">总结</a></li>
      </ul>
    </li>
    <li><a href="#单步模型">单步模型</a>
      <ul>
        <li><a href="#模型类">模型类</a></li>
        <li><a href="#基准模型">基准模型</a></li>
        <li><a href="#线性模型">线性模型</a></li>
        <li><a href="#密集层模型">密集层模型</a></li>
        <li><a href="#多步密集层模型">多步密集层模型</a></li>
        <li><a href="#卷积神经网络">卷积神经网络</a></li>
        <li><a href="#循环神经网络">循环神经网络</a></li>
        <li><a href="#预测表现">预测表现</a></li>
        <li><a href="#多变量输出模型">多变量输出模型</a></li>
      </ul>
    </li>
    <li><a href="#多步模型">多步模型</a>
      <ul>
        <li><a href="#基准模型-2">基准模型</a></li>
        <li><a href="#单发模型">单发模型</a></li>
        <li><a href="#高级方法自回归模型">高级方法：自回归模型</a></li>
        <li><a href="#预测表现-2">预测表现</a></li>
      </ul>
    </li>
    <li><a href="#下一步">下一步</a></li>
  </ul>
</nav>
    </div>
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <span>&copy; 2020-2023
                <a href="https://zhajiman.github.io/">炸鸡人</a>
                 | <a href="https://github.com/ZhaJiMan/ZhaJiMan.github.io">Source code</a> 
                | Powered by <a href="https://github.com/dsrkafuu/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 
                
                | Visits: <span id="busuanzi_value_site_pv"></span>
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.staticfile.org/medium-zoom/1.0.6/medium-zoom.min.js"></script>
<script defer src="https://cdn.staticfile.org/lazysizes/5.3.2/lazysizes.min.js"></script>
<script defer src="https://cdn.staticfile.org/prism/1.28.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.staticfile.org/prism/1.28.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>

<link rel="stylesheet" href="https://cdn.staticfile.org/KaTeX/0.15.6/katex.min.css" integrity="sha384-ZPe7yZ91iWxYumsBEOn7ieg8q/o+qh/hQpSaPow8T6BwALcXSCS6C6fSRPIAnTQs" crossorigin="anonymous">
<script defer src="https://cdn.staticfile.org/KaTeX/0.15.6/katex.min.js" integrity="sha384-ljao5I1l+8KYFXG7LNEA7DyaFvuvSCmedUf6Y6JI7LJqiu8q5dEivP2nDdFH31V4" crossorigin="anonymous"></script>
<script defer src="https://cdn.staticfile.org/KaTeX/0.15.6/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          throwOnError : false
        });
    });
</script>




</body>

</html>
